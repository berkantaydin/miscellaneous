Yeah, I have to say that I'm not a huge fan of "bool". It has some odd
properties, especially in memory (ie as a structure member).

For this kind of function return value it actually tends to work very
well, and in fact often generates slightly better code than "int". So
I don't _hate_ bool, and we've certainly had a lot more use creep in
lately, but I also don't really see "bool" as much of an upside.

What?

There's another *huge* difference, namely locking. Your change has
huge detrimental effects wrt d_lock - not only do you drop it for the
local dentry (to then re-take it in check_submounts_and_drop()), but
the whole check_submounts_and_drop thing walks the parent chain and
locks each parent with the renamelock held for writing.

So NAK on this patch, if for no other reason that you are not talking
about the *huge* locking changes at all, and apparently completely
ignored them.

It's possible that you can argue that performance doesn't matter, and
that all the extra huge locking overhead is immaterial because this is
not commonly called, but no way in hell can I accept a patch with a
commit message that basically lies by omission about what it is doing.

This kills the whole series for me. I'm not going to bother trying to
review the rest of the patches when the second patch already makes me
go "Eric is trying to hide things". Because the locking changes aren't
obvious in the patch without knowing what else is going on, and they
certainly aren't mentioned in the commit message.Oops, my bad about the write lock, brainfart due to grepping and
reading the wrong context. 

check_submounts_and_drop() doesn't do the parent walk with the rename
lock held for writing, it just holds it for reading.

But it does do that very complex "walk parents and check all siblings"
and locks them, so the rest of the commentary was correct.Hmm. It only does that for directories that have sub-entries, though.

I think you may care just about directories (because that's what your
series is about), but d_invalidate() is used for other cases too,
notably d_revalidate() (ie things like stale NFS lookups of normal
files).

That said, I'll have to think about this more. If d_subdir is empty, I
guess d_walk() will be fairly cheap. It's very different, but maybe
not as disastrous as I thought.At least one worry: people are very used to 'rmdir()' not removing
empty directories, and I've written code myself that just does an
'rmdir()' without worrying about it. I think git has code like "remove
file, then try to remove directory file is in, and recurse until it
fails or you hit the top of tree". And it all depends on knowing that
rmdir() is harmless, and returns the appropriate error when the
directory isn't empty.

And you're now changing that. If it's a mount-point, the rmdir just
succeeds, afaik.

Does anybody care? I dunno. But it looks like a _big_ semantic change.

and the mount just goes away. But I do think that for sanity sake, it
should have something like "if one of the mounts is in the current
namespace, return -EBUSY". IOW, the patch-series would make the VFS
layer _able_ to remove mount-points, but a normal rmdir() when
something is mounted in that namespace would fail in order to give
legacy behavior.

Hmm?

What drugs are you on?

Your example is moronic, and against all _documented_ uses of chroot.
After you do a chroot(), you need to chdir *into* the root. The reason
chroot() itself doesn't do that is simple: you may still be doing
various setup stuff.

But your example is just stupid. Yes, chroot'ed environments can
generally be escaped, but your example escape is simply because you
didn't use chroot() correctly.

So learn this pattern: every time you use chroot, add a simple

  chdir("/");

immediately after the chroot call.

Then, if you decide that you want to do some setup in between the two
(like the interface allows), that's fine, but always start off with
that "chroot+chdir" pattern.

(Similarly, if it turns out that you want to chdir somewhere else,
like "/home/user" after the chroot, then you can obviously remove the
now superfluous chdir("/"), but you always conceptually start off with
that chroot/chdir pair).  btw, also make sure that you close all non-essential file
descriptors. Having any open directory file descriptors pointing to
outside the chroot is also a classic escape.

Even then, escaping chroot is usually fairly easy. Making a
escape-proof chroot is really quite hard. Basically impossible if you
allow root.

Yeah, chroot() really doesn't cut it if you allow root access - and
thus internal chroot() calls - as you noticed (I didn't realize that
your example was meant to be run _inside_ a chroot).

There are generally other ways to escape chroot too if you're root.
It's just too hard to plug. That doesn't make chroot() useless - it
just means that the uses are elsewhere (it's useful for various
non-security issues like development environments, but it can also be
useful as one small _part_ of some bigger model, like a VM etc).

pivot_root() does end up being a "better chroot than chroot" if you're
looking for containment. It may not be a pretty system call, but it
does avoid at least the most obvious gotchas with chroot(). 
Btw, what is the definition of "observable" for the atomics?

Because I'm hoping that it's not the same as for volatiles, where
"observable" is about the virtual machine itself, and as such volatile
accesses cannot be combined or optimized at all.

Now, I claim that atomic accesses cannot be done speculatively for
writes, and not re-done for reads (because the value could change),
but *combining* them would be possible and good.

For example, we often have multiple independent atomic accesses that
could certainly be combined: testing the individual bits of an atomic
value with helper functions, causing things like "load atomic, test
bit, load same atomic, test another bit". The two atomic loads could
be done as a single load without possibly changing semantics on a real
machine, but if "visibility" is defined in the same way it is for
"volatile", that wouldn't be a valid transformation. Right now we use
"volatile" semantics for these kinds of things, and they really can
hurt.

Same goes for multiple writes (possibly due to setting bits):
combining multiple accesses into a single one is generally fine, it's
*adding* write accesses speculatively that is broken by design. 

At the same time, you can't combine atomic loads or stores infinitely
- "visibility" on a real machine definitely is about timeliness.
Removing all but the last write when there are multiple consecutive
writes is generally fine, even if you unroll a loop to generate those
writes. But if what remains is a loop, it might be a busy-loop
basically waiting for something, so it would be wrong ("untimely") to
hoist a store in a loop entirely past the end of the loop, or hoist a
load in a loop to before the loop.

Does the standard allow for that kind of behavior?

I really disagree with the "will need to use volatile".

We should never need to use volatile (outside of whatever MMIO we do
using C) if C11 defines atomics correctly.

Allowing load/store merging is *fine*. All sane CPU's do that anyway -
it's called a cache - and there's no actual reason to think that
"ACCESS_ONCE()" has to mean our current "volatile".

Now, it's possible that the C standards simply get atomics _wrong_, so
that they create visible semantics that are different from what a CPU
cache already does, but that's a plain bug in the standard if so.

But merging loads and stores is fine. And I *guarantee* it is fine,
exactly because CPU's already do it, so claiming that the compiler
couldn't do it is just insanity.

Now, there are things that are *not* fine, like speculative stores
that could be visible to other threads. Those are *bugs* (either in
the compiler or in the standard), and anybody who claims otherwise is
not worth discussing with.

But I really really disagree with the "we might have to use
'volatile'". Because if we *ever* have to use 'volatile' with the
standard C atomic types, then we're just better off ignoring the
atomic types entirely, because they are obviously broken shit - and
we're better off doing it ourselves the way we have forever.

Seriously. This is not even hyperbole. It really is as simple as that.
Quite frankly, I think it's stupid, and the "documentation" is not a
benefit, it's just wrong.

How would you figure out whether your added "documentation" holds true
for particular branches but not others?

How could you *ever* trust a compiler that makes the dependency meaningful?

Again, let's keep this simple and sane:

 - if a compiler ever generates code where an atomic store movement is
"visible" in any way, then that compiler is broken shit.

I don't understand why you even argue this. Seriously, Paul, you seem
to *want* to think that "broken shit" is acceptable, and that we
should then add magic markers to say "now you need to *not* be broken
shit".

Here's a magic marker for you: DON'T USE THAT BROKEN COMPILER.

And if a compiler can *prove* that whatever code movement it does
cannot make a difference, then let it do so. No amount of
"documentation" should matter.

Seriously, this whole discussion has been completely moronic. I don't
understand why you even bring shit like this up:I mean, really? Anybody who writes code like that, or any compiler
where that "control_dependency()" marker makes any difference
what-so-ever for code generation should just be retroactively aborted.

There is absolutely *zero* reason for that "control_dependency()"
crap. If you ever find a reason for it, it is either because the
compiler is buggy, or because the standard is so shit that we should
never *ever* use the atomics.

Seriously. This thread has devolved into some kind of "just what kind
of idiotic compiler cesspool crap could we accept". Get away from that
f*cking mindset. We don't accept *any* crap.

Why are we still discussing this idiocy? It's irrelevant. If the
standard really allows random store speculation, the standard doesn't
matter, and sane people shouldn't waste their time arguing about it.
Btw, the other part of this coin is that our manual types (using
volatile and various architecture-specific stuff) and our manual
barriers and inline asm accesses are generally *fine*.

The C11 stuff doesn't buy us anything. The argument that "new
architectures might want to use it" is prue and utter bollocks, since
unless the standard gets the thing *right*, nobody sane would ever use
it for some new architecture, when the sane thing to do is to just
fill in the normal barriers and inline asms.

So I'm very very serious: either the compiler and the standard gets
things right, or we don't use it. There is no middle ground where "we
might use it for one or two architectures and add random hints".
That's just stupid.

The only "middle ground" is about which compiler version we end up
trusting _if_ it turns out that the compiler and standard do get
things right. From Torvald's explanations (once I don't mis-read them
;), my take-away so far has actually been that the standard *does* get
things right, but I do know from over-long personal experience that
compiler people sometimes want to be legalistic and twist the
documentation to the breaking point, at which point we just go "we'd
be crazy do use that".

See our use of "-fno-strict-aliasing", for example. The C standard
aliasing rules are a mistake, stupid, and wrong, and gcc uses those
stupid type-based alias rules even when statically *proving* the
aliasing gives the opposite result. End result: we turn the shit off.

Exact same deal wrt atomics. We are *not* going to add crazy "this
here is a control dependency" crap. There's a test, the compiler
*sees* the control dependency for chrissake, and it still generates
crap, we turn that broken "optimization" off. It really is that
simple.

Clearly it does *not*. This whole discussion is proof of that. It's
not at all clear, and the standard apparently is at least debatably
allowing things that shouldn't be allowed. It's also a whole lot more
complicated than "volatile", so the likelihood of a compiler writer
actually getting it right - even if the standard does - is lower.
They've gotten "volatile" wrong too, after all (particularly in C++).

           Well, Paul seems to still think that the standard possibly allows
speculative writes or possibly value speculation in ways that break
the hardware-guaranteed orderings.

And personally, I can't read standards paperwork. It is invariably
written in some basically impossible-to-understand lawyeristic mode,
and then it is read by people (compiler writers) that intentionally
try to mis-use the words and do language-lawyering ("that depends on
what the meaning of 'is' is"). The whole "lvalue vs rvalue expression
vs 'what is a volatile access'" thing for C++ was/is a great example
of that.

So quite frankly, as a result I refuse to have anything to do with the
process directly.
Heh. See the example I used in my reply to Alec Teal. It basically
broke the same dependency the same way.

Yes, value speculation of reads is simply wrong, the same way
speculative writes are simply wrong. The dependency chain matters, and
is meaningful, and breaking it is actively bad.

As far as I can tell, the intent is that you can't do value
speculation (except perhaps for the "relaxed", which quite frankly
sounds largely useless). But then I do get very very nervous when
people talk about "proving" certain values.
Hmm. The language I see for "consume" is not obvious:

  "Consume operation: no reads in the current thread dependent on the
value currently loaded can be reordered before this load"

and it could make a compiler writer say that value speculation is
still valid, if you do it like this (with "ptr" being the atomic
variable):

  value = ptr->val;

into

  tmp = ptr;
  value = speculated.value;
  if (unlikely(tmp != &speculated))
    value = tmp->value;

which is still bogus. The load of "ptr" does happen before the load of
"value = speculated->value" in the instruction stream, but it would
still result in the CPU possibly moving the value read before the
pointer read at least on ARM and power.

So if you're a compiler person, you think you followed the letter of
the spec - as far as *you* were concerned, no load dependent on the
value of the atomic load moved to before the atomic load. You go home,
happy, knowing you've done your job. Never mind that you generated
code that doesn't actually work.

I dread having to explain to the compiler person that he may be right
in some theoretical virtual machine, but the code is subtly broken and
nobody will ever understand why (and likely not be able to create a
test-case showing the breakage).

But maybe the full standard makes it clear that "reordered before this
load" actually means on the real hardware, not just in the generated
instruction stream. Reading it with understanding of the *intent* and
understanding all the different memory models that requirement should
be obvious (on alpha, you need an "rmb" instruction after the load),
but . 

That's just for googling for explanations. I do have some old standard
draft, but that doesn't have any concise definitions anywhere that I
could find.
Yes.

Ok, good.

Can you point to it? Because I can find a draft standard, and it sure
as hell does *not* contain any clarity of the model. It has a *lot* of
verbiage, but it's pretty much impossible to actually understand, even
for somebody who really understands memory ordering.
Ahh, this is different from what others pointed at. Same people,
similar name, but not the same paper.

I will read this version too, but from reading the other one and the
standard in parallel and trying to make sense of it, it seems that I
may have originally misunderstood part of the whole control dependency
chain.

The fact that the left side of "? :", "&&" and "||" breaks data
dependencies made me originally think that the standard tried very
hard to break any control dependencies. Which I felt was insane, when
then some of the examples literally were about the testing of the
value of an atomic read. The data dependency matters quite a bit. The
fact that the other "Mathematical" paper then very much talked about
consume only in the sense of following a pointer made me think so even
more.

But reading it some more, I now think that the whole "data dependency"
logic (which is where the special left-hand side rule of the ternary
and logical operators come in) are basically an exception to the rule
that sequence points end up being also meaningful for ordering (ok, so
C11 seems to have renamed "sequence points" to "sequenced before").

So while an expression like

    atomic_read(p, consume) ? a : b;

doesn't have a data dependency from the atomic read that forces
serialization, writing

   if (atomic_read(p, consume))
      a;
   else
      b;

the standard *does* imply that the atomic read is "happens-before" wrt
"a", and I'm hoping that there is no question that the control
dependency still acts as an ordering point.

THAT was one of my big confusions, the discussion about control
dependencies and the fact that the logical ops broke the data
dependency made me believe that the standard tried to actively avoid
the whole issue with "control dependencies can break ordering
dependencies on some CPU's due to branch prediction and memory
re-ordering by the CPU".

But after all the reading, I'm starting to think that that was never
actually the implication at all, and the "logical ops breaks the data
dependency rule" is simply an exception to the sequence point rule.
All other sequence points still do exist, and do imply an ordering
that matters for "consume"

Am I now reading it right?

So the clarification is basically to the statement that the "if
(consume(p)) a" version *would* have an ordering guarantee between the
read of "p" and "a", but the "consume(p) ? a : b" would *not* have
such an ordering guarantee. Yes?

Stores I don't worry about so much because

 (a) you can't sanely move stores up in a compiler anyway
 (b) no sane CPU or moves stores up, since they aren't on the critical path

so a read->cmp->store is actually really hard to make anything sane
re-order. I'm sure it can be done, and I'm sure it's stupid as hell.

But that "it's hard to screw up" is *not* true for a load->cmp->load.

So lets make this really simple: if you have a consume->cmp->read, is
the ordering of the two reads guaranteed?So let's make it *really* specific, and make it real code doing a real
operation, that is actually realistic and reasonable in a threaded
environment, and may even be in some critical code.

The issue is the read-side ordering guarantee for 'a' and 'b', for this case:

and quite frankly, if there is no ordering guarantee between the read
of "a" and the read of "b" in the consumer thread, then the C atomics
standard is broken.

Put another way: I claim that if "thread 1" ever sees a return value
other than -1 or 42, then the whole definition of atomics is broken.

Question 2: and what changes if the atomic_read() is turned into an
acquire, and why? Does it start working?So exactly what part of the standard allows the loads to be
re-ordered, and why? Quite frankly, I'd think that any sane person
will agree that the above code snippet is realistic, and that my
requirement that thread 1 sees either -1 or 42 is valid.

And if the C standards body has said that control dependencies break
the read ordering, then I really think that the C standards committee
has screwed up.

If the consumer of an atomic load isn't a pointer chasing operation,
then the consume should be defined to be the same as acquire. None of
this "conditionals break consumers". No, conditionals on the
dependency path should turn consumers into acquire, because otherwise
the "consume" load is dangerous as hell.

And if the definition of acquire doesn't include the control
dependency either, then the C atomic memory model is just completely
and utterly broken, since the above *trivial* and clearly useful
example is broken.

I really think the above example is pretty damn black-and-white.
Either it works, or the standard isn't worth wiping your ass with.Ok, quite frankly, I think that means that "consume" is misdesigned.

Correct. And I think that is too subtle. It's dangerous, it makes code
that *looks* correct work incorrectly, and it actually happens to work
on x86 since x86 doesn't have crap-for-brains memory ordering
semantics.

No. We're not playing games here. I'm fed up with complex examples
that make no sense.

Nobody sane writes code that does that pointer comparison, and it is
entirely immaterial what the compiler can do behind our backs. The C
standard semantics need to make sense to the *user* (ie programmer),
not to a CPU and not to a compiler. The CPU and compiler are "tools".
They don't matter. Their only job is to make the code *work*, dammit.

So no idiotic made-up examples that involve code that nobody will ever
write and that have subtle issues.

So the starting point is that (same example as before, but with even
clearer naming):

Initialization state:
  initialized = 0;
  value = 0;

Consumer:

    return atomic_read(&initialized, consume) ? value : -1;

Writer:
    value = 42;
    atomic_write(&initialized, 1, release);

and because the C memory ordering standard is written in such a way
that this is subtly buggy (and can return 0, which is *not* logically
a valid value), then I think the C memory ordering standard is broken.

That "consumer" memory ordering is dangerous as hell, and it is
dangerous FOR NO GOOD REASON.

The trivial "fix" to the standard would be to get rid of all the
"carries a dependency" crap, and just say that *anything* that depends
on it is ordered wrt it.

That just means that on alpha, "consume" implies an unconditional read
barrier (well, unless the value is never used and is loaded just
because it is also volatile), on x86, "consume" is the same as
"acquire" which is just a plain load with ordering guarantees, and on
ARM or power you can still avoid the extra synchronization *if* the
value is used just for computation and for following pointers, but if
the value is used for a comparison, there needs to be a
synchronization barrier.

Notice? Getting rid of the stupid "carries-dependency" crap from the
standard actually
 (a) simplifies the standard
 (b) means that the above obvious example *works*
 (c) does not in *any* way make for any less efficient code generation
for the cases that "consume" works correctly for in the current
mis-designed standard.
 (d) is actually a hell of a lot easier to explain to a compiler
writer, and I can guarantee that it is simpler to implement too.

Why do I claim (d) "it is simpler to implement" - because on ARM/power
you can implement it *exactly* as a special "acquire", with just a
trivial peep-hole special case that follows the use chain of the
acquire op to the consume, and then just drop the acquire bit if the
only use is that compute-to-load chain.

In fact, realistically, the *only* thing you need to actually care
about for the intended use case of "consume" is the question "is the
consuming load immediately consumed as an address (with offset) of a
memory operation. So you don't even need to follow any complicated
computation chain in a compiler - the only case that matters for the
barrier removal optimization is the "oh, I can see that it is only
used as an address to a dereference".

Seriously. The current standard is broken. It's broken because it
mis-compiles code that on the face of it looks logical and works, it's
broken because it's overly complex, and it's broken because the
complexity doesn't even *buy* you anything. All this complexity for no
reason. When much simpler wording and implementation actually WORKS
BETTER.

So I think we now all agree that that is what the standard is saying.

And I'm saying that that is wrong, that the standard is badly written,
and should be fixed.

Because before the standard is fixed, I claim that "consume" is
unusable. We cannot trust it. End of story.

The fact that apparently gcc is currently buggy because it got the
dependency calculations *wrong* just reinforces my point.

The gcc bug Torvald pointed at is exactly because the current C
standard is illogical unreadable CRAP. I can guarantee that what
happened is:

 - the compiler saw that the result of the read was used as the left
hand expression of the ternary "? :" operator

 - as a result, the compiler decided that there's no dependency

 - the compiler didn't think about the dependency that comes from the
result of the load *also* being used as the middle part of the ternary
expression, because it had optimized it away, despite the standard not
talking about that at all.

 - so the compiler never saw the dependency that the standard talks about

BECAUSE THE STANDARD LANGUAGE IS PURE AND UTTER SHIT.

My suggested language never had any of these problems, because *my*
suggested semantics are clear, logical, and don't have these kinds of
idiotic pit-falls.

Solution: Fix the f*cking C standard. No excuses, no explanations.
Just get it fixed.No it does not, for two reasons, first the legalistic (and bad) reason:

As I actually described it, the "consume" becomes an "acquire" by
default. If it's not used as an address to the dependent load, then
it's an acquire. The use "going away" in no way makes the acquire go
away in my simplistic model.

So the compiler would actually translate that to a load-with-acquire,
not be able to remove the acquire, and we have end of story. The
actual code generation would be that "ld + sync + ld" on powerpc, or
"ld.acq" on ARM.

Now, the reason I claim that reason was "legalistic and bad" is that
it's actually a cop-out, and if you had made the example be something
like this:

   p = atomic_load(&ptr, memory_order_consume);
   x = array[0 + p - p];
   y = p->val;

then yes, I actually think that the order of loads of 'x' and 'p' are
not enforced by the "consume". The only case that is clear is the
order of 'y' and 'p', because that is the only one that really *USES*
the value.

The "use" of "+p-p" is syntactic bullshit. It's very obvious to even a
slightly developmentally challenged hedgehog that "+p-p" doesn't have
any actual *semantic* meaning, it's purely syntactic.

And the syntactic meaning is meaningless and doesn't matter. Because I
would just get rid of the whole "dependency chain" language
ALTOGETHER.

So in fact, in my world, I would consider your example to be a
non-issue. In my world, there _is_ no "dependency chain" at a
syntactic level. In my SANE world, none of that insane crap language
exists. That language is made-up and tied to syntax exactly because it
*cannot* be tied to semantics.

In my sane world, "consume" has a much simpler meaning, and has no
legalistic syntactic meaning: only real use matters. If the value can
be optimized away, the so can the barrier, and so can the whole load.
The value isn't "consumed", so it has no meaning.

So if you write

   i = atomic_load(idx, memory_order_consume);
   x = array[0+i-i];

then in my world that "+i-i" is meaningless. It's semantic fluff, and
while my naive explanation would have left it as an acquire (because
it cannot be peep-holed away), I actually do believe that the compiler
should be obviously allowed to optimize the load away entirely since
it's meaningless, and if no use of 'i' remains, then it has no
consumer, and so there is no dependency.

Put another way: "consume" is not about getting a lock, it's about
getting a *value*. Only the dependency on the *value* matters, and if
the value is optimized away, there is no dependency.

And the value itself does not have any semantics. There's nothing
"volatile" about the use of the value that would mean that the
compiler cannot re-order it or remove it entirely. There's no barrier
"carried around" by the value per se. The barrier is between the load
and use. That's the *point* of "consume" after all.

The whole "chain of dependency" language is pointless. It's wrong.
It's complicated, it is illogical, and it causes subtle problems
exactly because it got tied to the language *syntax* rather than to
any logical use.

Don't try to re-introduce the whole issue. It was a mistake for the C
standard to talk about dependencies in the first place, exactly
because it results in these idiotic legalistic practices.

You do realize that that whole "*(q+flag-flag)" example in the
bugzilla comes from the fact that the programmer tried to *fight* the
fact that the C standard got the control dependency wrong?

In other words, the *deepest* reason for that bugzilla is that the
programmer tried to force the logical dependency by rewriting it as a
(fake, and easily optimizable) data dependency.

In *my* world, the stupid data-vs-control dependency thing goes away,
the test of the value itself is a use of it, and "*p ? *q :0" just
does the right thing, there's no reason to do that "q+flag-flag" thing
in the first place, and if you do, the compiler *should* just ignore
your little games.

Heh. I have heard people wax poetic about the pleasures of standards
committee meetings.

Enough that I haven't really ever had the slightest urge to participate ;)
Well.  I'd really hope we would never do that. That said, we have
certainly used disgusting things that we knew would disable certain
optimizations in the compiler before, so I wouldn't put it *entirely*
past us to do things like that, but I'd argue that we'd do so in order
to confuse the compiler to do what we want, not in order to argue that
it's a good thing.

I mean, right now we have at least *one* active ugly work-around for a
compiler bug (the magic empty inline asm that works around the "asm
goto" bug in gcc). So it's not like I would claim that we don't do
disgusting things when we need to.

But I'm pretty sure that any compiler guy must *hate* that current odd
dependency-generation part, and if I was a gcc person, seeing that
bugzilla entry Torvald pointed at, I would personally want to
dismember somebody with a rusty spoon. 

So I suspect there are a number of people who would be *more* than
happy with a change to those odd dependency rules.I understand.

And I disagree. I think the standard is wrong, and what I *should* be
doing is point out the fact very loudly, and just tell people to NEVER
EVER use "consume" as long as it's not reliable and has insane
semantics.

So what I "should do" is to not accept any C11 atomics use in the
kernel. Because with the "acquire", it generates worse code than what
we already have, and with the "consume" it's shit.

See?

I do agree. They are two independent things.

I think the standard is wrong, because it's overly complex, hard to
understand, and nigh unimplementable. As shown by the bugzilla
example, "carries a dependency" encompasses things that are *not* just
synchronizing things just through a pointer, and as a result it's
actually very complicated, since they could have been optimized away,
or done in non-local code that wasn't even aware of the dependency
carrying.

That said, I'm reconsidering my suggested stricter semantics, because
for RCU we actually do want to test the resulting pointer against NULL
_without_ any implied serialization.

So I still feel that the standard as written is fragile and confusing
(and the bugzilla entry pretty much proves that it is also practically
unimplementable as written), but strengthening the serialization may
be the wrong thing.

Within the kernel, the RCU use for this is literally purely about
loading a pointer, and doing either:

 - testing its value against NULL (without any implied synchronization at all)

 - using it as a pointer to an object, and expecting that any accesses
to that object are ordered wrt the consuming load.

So I actually have a suggested *very* different model that people
might find more acceptable.

How about saying that the result of a "atomic_read(&a, mo_consume)" is
required to be a _restricted_ pointer type, and that the consume
ordering guarantees the ordering between that atomic read and the
accesses to the object that the pointer points to.

No "carries a dependency", no nothing.

Now, there's two things to note in there:

 - the "restricted pointer" part means that the compiler does not need
to worry about serialization to that object through other possible
pointers - we have basically promised that the *only* pointer to that
object comes from the mo_consume. So that part makes it clear that the
"consume" ordering really only is valid wrt that particular pointer
load.

 - the "to the object that the pointer points to" makes it clear that
you can't use the pointer to generate arbitrary other values and claim
to serialize that way.

IOW, with those alternate semantics, that gcc bugzilla example is
utterly bogus, and a compiler can ignore it, because while it tries to
synchronize through the "dependency chain" created with that "p-i+i"
expression, that is completely irrelevant when you use the above rules
instead.

In the bugzilla example, the object that "*(p-i+i)" accesses isn't
actually the object pointed to by the pointer, so no serialization is
implied. And if it actually *were* to be the same object, because "p"
happens to have the same value as "i", then the "restrict" part of the
rule pops up and the compiler can again say that there is no ordering
guarantee, since the programmer lied to it and used a restricted
pointer that aliased with another one.

So the above suggestion basically tightens the semantics of "consume"
in a totally different way - it doesn't make it serialize more, in
fact it weakens the serialization guarantees a lot, but it weakens
them in a way that makes the semantics a lot simpler and clearer.
Note that this is very much outside the scope of the C standard,
regardless of 'consume' or not.

We'll always do things outside the standard, so I wouldn't worry.
Note that this doesn't actually affect the restrict/ordering rule in
theory: "?:" isn't special according to those rules. The rules are
fairly simple: we guarantee ordering only to the object that the
pointer points to, and even that guarantee goes out the window if
there is some *other* way to reach the object.

?: is not really relevant, except in the sense that *any* expression
that ends up pointing to outside the object will lose the ordering
guarantee. ?: can be one such expression, but so can "p-p" or anything
like that.

And in *practice*, the only thing that needs to be sure to generate
special code is alpha, and there you'd just add the "rmb" after the
load. That is sufficient to fulfill the guarantees.

On ARM and powerpc, the compiler obviously has to guarantee that it
doesn't do value-speculation on the result, but again, that never
really had anything to do with the whole "carries a dependency", it is
really all about the fact that in order to guarantee the ordering, the
compiler mustn't generate that magical aliased pointer value. But if
the aliased pointer value comes from the *source* code, all bets are
off.

Now, even on alpha, the compiler can obviously move that "rmb" around.
For example, if there is a conditional after the
"atomic_read(mo_consume)", and the compiler can tell that the pointer
that got read by mo_consume is dead along one branch, then the
compiler can move the "rmb" to only exist in the other branch. Why?
Because we inherently guarantee only the order to any accesses to the
object the pointer pointed to, and that the pointer that got loaded is
the *only* way to get to that object (in this context), so if the
value is dead, then so is the ordering.

In fact, even if the value is *not* dead, but it is NULL, the compiler
can validly say "the NULL pointer cannot point to any object, so I
don't have to guarantee any serialization". So code like this (writing
alpha assembly, since in practice only alpha will ever care):

can validly be translated to:

because the compiler knows that a NULL pointer cannot be dereferenced,
so it can decide to put the rmb in the non-NULL path - even though the
pointer value is still *live* in the other branch (well, the liveness
of a constant value is somewhat debatable, but you get the idea), and
may be used by the caller (but since it is NULL, the "use" can not
include accessing any object, only really testing)

So note how this is actually very different from the "carries
dependency" rule. It's simpler, and it allows much more natural
optimizations.
Addition and subtraction is fine, as long as they stay within the same
object/array.

And realistically, people violate the whole C pointer "same object"
rule all the time. Any time you implement a raw memory allocator,
you'll violate the C standard and you *will* basically be depending on
architecture-specific behavior. So everybody knows that the C "pointer
arithmetic has to stay within the object" is really a fairly made-up
but convenient shorthand for "sure, we know you'll do tricks on
pointer values, but they won't be portable and you may have to take
particular machine representations into account".

Well, in the C sense, or in the actual "integer index" sense? Because
technically, a[b] is nothing but *(a+b), so "inside" the "[]" is
strictly speaking meaningless. Inside and outside are just syntactic
sugar.

That said, I agree that the way I phrased things really limits things
to *just* pointers, and if you want to do RCU on integer values (that
get turned into pointers some other way), that would be outside the
spec.

But in practice, the code generation will *work* for non-pointers too
(the exception might be if the compiler actually does the above "NULL
is special, so I know I don't need to order wrt it". Exactly the way
that people do arithmetic operations on pointers that aren't really
covered by the standard (ie arithmetic 'and' to align them etc), the
code still *works*. It may be outside the standard, but everybody does
it, and one of the reasons C is so powerful is exactly that you *can*
do things like that. They won't be portable, and you have to know what
you are doing, but they don't stop working just because they aren't
covered by the standard.

I don't see that you would need to protect anything but the first
read, so I think you need rcu_dereference() only on the initial
pointer access.

On architectures where following a pointer is sufficient ordering,
we're fine. On alpha (and, if anybody ever makes that horrid mistake
again), the 'rmb' after the first access will guarantee that all the
later accesses will see the new list.

So basically, 'consume' will end up inserting a barrier (if necessary)
before following the pointer for the first time, but once that barrier
has been inserted, we are now fully ordered wrt the releasing store,
so we're done. No need to order *all* the accesses (even off the same
base pointer), only the first one.Oh yes. It's in the same object. If it wasn't, then "p[5]" itself
would be meaningless and outside the scope of the standard to begin
with.Yes. By definition. And again, for the very same reason. You would
violate *other* parts of the C standard before you violated the
suggested rule.

Also, remember: a lot of this is about "legalistic guarantees". In
practice, just the fact that a data chain *exists* is guarantee
enough, so from a code generation standpoint, NONE OF THIS MATTERS.

The exception is alpha, where a "mo_consume" load basically needs to
be generated with a "rmb" following it (see above how you can relax
that a _bit_, but not much). Trivial.

So code generation is actually *trivial*. Ordering is maintained
*automatically*, with no real effort on the side of the compiler.

The only issues really are:

 - the current C standard language implies *too much* ordering. The
whole "carries dependency" is broken. The practical example - even
without using "?:" or any conditionals or any function calls - is just
that

  and there really isn't any sane ordering there. But the current
standard clearly says there is. The current standard is wrong.

 - my trick of saying that it is only ordered wrt accesses to the
object the pointer points to gets rid of that whole bogus false
dependency.

 - the "restricted pointer" thing is just legalistic crap, and has no
actual meaning for code generation, it is _literally_ just that if the
programmer does value speculation by hand:

    then in this situation the compiler does *not* need to worry about
the ordering to the "object.val" access, because it was gotten through
an alias.

So that "restrict" thing is really just a way to say "the ordering
only exists through that single pointer". That's not really what
restrict was _designed_ for, but to quote the standard:

 "An object that is accessed through a restrict-qualiﬁed pointer has a
special association
  with that pointer. This association, deﬁned in 6.7.3.1 below,
requires that all accesses to
  that object use, directly or indirectly, the value of that particular pointer"

that's not the *definition* of "restrict", and quite frankly, the
actual language that talks about "restrict" really talks about being
able to know that nobody *modifies* the value through another pointer,
so I'm clearly stretching things a bit. But anybody reading the above
quote from the standard hopefully agrees that I'm not stretching it
all that much.Ack. And that's normal "access to an object" behavior anyway.I suspect that in practice, all the same normal rules apply - assuming
that there aren't any "aliasing" integers, and that the compiler
doesn't do any value prediction, the end result *should* be exactly
the same as for the pointer arithmetic. So even if the standard were
to talk about just pointers, I suspect that in practice, there really
isn't anything that can go wrong.Actually, pointer subtraction is a pretty useful operation, even
without the "gotcha" case of "p-p" just to force a fake dependency.
Getting an index, or indeed just getting an offset within a structure,
is valid and common, and people will and should do it. It doesn't
really matter for my suggested language: it should be perfectly fine
to do something like

   .  pass off 'index' to some function, which then re-generates the
ptr using it . 

and the compiler will have no trouble generating code, and the
suggested "ordered wrt that object" guarantee is that the eventual
ordering is between the pointer load and the use in the function,
regardless of how it got there (ie turning it into an index and back
is perfectly fine).

So both from a legalistic language wording standpoint, and from a
compiler code generation standpoint, this is a non-issue.

Now, if you actually lose information (ie some chain there drops
enough data from the pointer that it cannot be recovered, partially of
fully), and then "regenerate" the object value by faking it, and still
end up accessing the right data, but without actually going through
any of the the pointer that you loaded, that falls under the
"restricted" heading, and you must clearly at least partially have
used other information. In which case the standard wording wouldn't
guarantee anything at all.

So here, the standard requires that the store with release is an
ordering to all preceding writes. So *all* writes to bar and foo are
ordered, despite the fact that the pointer just points to foo.

So the theory is that a compiler *could* do some value speculation now
on "q", and with value speculation move the actual load of "q->p" up
to before "foop" was even loaded.

So in practice, I think we agree that this doesn't really affect
compiler writers (because they'd have to do a whole lot extra code to
break it intentionally, considering that they can't do
value-prediction on 'p'), and we just need to make sure to close the
hole in the language to make this safe, right?

Let me think about it some more, but my gut feel is that just tweaking
the definition of what "ordered" means is sufficient.

So to go back to the suggested ordering rules (ignoring the "restrict"
part, which is just to clarify that ordering through other means to
get to the object doesn't matter), I suggested:

 "the consume ordering guarantees the ordering between that
  atomic read and the accesses to the object that the pointer
  points to"

and I think the solution is to just say that this ordering acts as a
fence. It doesn't say exactly *where* the fence is, but it says that
there is *some* fence between the load of the pointer and any/all
accesses to the object through that pointer.

So with that definition, the memory accesses that are dependent on 'q'
will obviously be ordered. Now, they will *not* be ordered wrt the
load of q itself, but they will be ordered wrt the load from 'foop' -
because we've made it clear that there is a fence *somewhere* between
that atomic_load and the load of 'q'.

So that handles the ordering guarantee you worry about.

Now, the other worry is that this "fence" language would impose a
*stricter* ordering, and that by saying there is a fence, we'd now
constrain code generation on architectures like ARM and power, agreed?
And we do not want to do that, other than make it clear that the whole
"break the dependency chain through value speculation" cannot break it
past the load from 'foop'. Are we in agreement?

So we do *not* want that fence to limit re-ordering of independent
memory operations. All agreed?

But let's look at any independent chain, especially around that
consuming load from '&foop'. Say we have some other memory access
(adding them for visualization into your example):

and we are looking to make sure that those memory accesses can still
move around freely.

I'm claiming that they can, even with the "fence" language, because

 (a) we've said 'q' is restricted, so there is no aliasing between q
and the pointers b/c. So the compiler is free to move those accesses
around the "q = p->next" access.

 (b) once you've moved them to *before* the "q = p->next" access, the
fence no longer constrains you, because the guarantee is that there is
a fence *somewhere* between the consuming load and the accesses to
that object, but it could be *at* the access, so you're done.

So I think the "we guarantee there is an ordering *somewhere* between
the consuming load and the first ordered access" model actually gives
us the semantics we need.

(Note that you cannot necessarily move the accesses through the
pointers b/c past the consuming load, since the only non-aliasing
guarantee is about the pointer 'p', not about 'foop'. But you may use
other arguments to move them past that too)

But it's possible that the above argument doesn't really work. It is a
very off-the-cuff "my intuition says this should work" model.

I'm wrong. That doesn't work. At all. There is no ordering except
through the pointer chain.

So I think saying just that, and nothing else (no magic fences, no
nothing) is the right thing:

 "the consume ordering guarantees the ordering between that
  atomic read and the accesses to the object that the pointer
  points to directly or indirectly through a chain of pointers"

The thing is, anything but a chain of pointers (and maybe relaxing it
to "indexes in tables" in addition to pointers) doesn't really work.

The current standard tries to break it at "obvious" points that can
lose the data dependency (either by turning it into a control
dependency, or by just dropping the value, like the left-hand side of
a comma-expression), but the fact is, it's broken.

It's broken not just because the value can be lost other ways (ie the
"p-p" example), it's broken because the value can be turned into a
control dependency so many other ways too.

Compilers regularly turn arithmetic ops with logical comparisons into
branches. So an expression like "a = !!ptr" carries a dependency in
the current C standard, but it's entirely possible that a compiler
ends up turning it into a compare-and-branch rather than a
compare-and-set-conditional, depending on just exactly how "a" ends up
being used. That's true even on an architecture like ARM that has a
lot of conditional instructions (there are way less if you compile for
Thumb, for example, but compilers also do things like "if there are
more than N predicated instructions I'll just turn it into a
branch-over instead").

So I think the C standard needs to just explicitly say that you can
walk a chain of pointers (with that possible "indexes in arrays"
extension), and nothing more.

No, it's not about type at all, and the "chain of pointers" can be
much more complex than that, since the "int *" can point to within an
object that contains other things than just that "int" (the "int" can
be part of a structure that then has pointers to other structures
etc).

So in your example,

    ptr = atomic_read(p, CONSUME);

would indeed order against the subsequent access of the chain through
*that* pointer (the whole "restrict" thing that I left out as a
separate thing, which was probably a mistake), but certainly not
against any integer pointer, and certainly not against any aliasing
pointer chains.

So yes, the atomic_read() would be ordered wrt '*ptr' (getting 'q')
_and_ '**ptr' (getting 'i'), but nothing else - including just the
aliasing access of dereferencing 'i' directly.It's supposed to be ordered wrt the first load (the consuming one), yes.

No problem.

The thing is, the ordering is actually handled by the CPU in all
relevant cases.  So the compiler doesn't actually need to *do*
anything. All this legalistic stuff is just to describe the semantics
and the guarantees.

The problem is two cases:

 (a) alpha (which doesn't really order any accesses at all, not even
dependent loads), but for a compiler alpha is actually trivial: just
add a "rmb" instruction after the load, and you can't really do
anything else (there's a few optimizations you can do wrt the rmb, but
they are very specific and simple).

So (a) is a "problem", but the solution is actually really simple, and
gives very *strong* guarantees: on alpha, a "consume" ends up being
basically the same as a read barrier after the load, with only very
minimal room for optimization.

 (b) ARM and powerpc and similar architectures, that guarantee the
data dependency as long as it is an *actual* data dependency, and
never becomes a control dependency.

On ARM and powerpc, control dependencies do *not* order accesses (the
reasons boil down to essentially: branch prediction breaks the
dependency, and instructions that come after the branch can be happily
executed before the branch). But it's almost impossible to describe
that in the standard, since compilers can (and very much do) turn a
control dependency into a data dependency and vice versa.

So the current standard tries to describe that "control vs data"
dependency, and tries to limit it to a data dependency. It fails. It
fails for multiple reasons - it doesn't allow for trivial
optimizations that just remove the data dependency, and it also
doesn't allow for various trivial cases where the compiler *does* turn
the data dependency into a control dependency.

So I really really think that the current C standard language is
broken. Unfixably broken.

I'm trying to change the "syntactic data dependency" that the current
standard uses into something that is clearer and correct.

The "chain of pointers" thing is still obviously a data dependency,
but by limiting it to pointers, it simplifies the language, clarifies
the meaning, avoids all syntactic tricks (ie "p-p" is clearly a
syntactic dependency on "p", but does *not* involve in any way
following the pointer) and makes it basically impossible for the
compiler to break the dependency without doing value prediction, and
since value prediction has to be disallowed anyway, that's a feature,
not a bug.
Btw, what CPU architects and memory ordering guys tend to do in
documentation is give a number of "litmus test" pseudo-code sequences
to show the effects and intent of the language.

I think giving those kinds of litmus tests for both "this is ordered"
and "this is not ordered" cases like the above is would be a great
clarification. Partly because the language is going to be somewhat
legalistic and thus hard to wrap your mind around, and partly to
really hit home the *intent* of the language, which I think is
actually fairly clear to both compiler writers and to programmers.
No. You cannot define it this way. Taking the value of a pointer and
doing a bitwise operation that throws away all the bits (or even
*most* of the bits) results in the compiler easily being able to turn
the "chain" into a non-chain.

The obvious example being "val & 0", but things like "val & 1" are in
practice also something that compilers easily turn into control
dependencies instead of data dependencies.

So you can talk about things like "aligning the pointer value to
object boundaries" etc, but it really cannot and *must* not be about
the syntactic operations.

The same goes for "adding and subtracting an integer". The *syntax*
doesn't matter. It's about remaining information. Doing "p-(int)p" or
"p+(-(int)p)" doesn't leave any information despite being "subtracting
and adding an integer" at a syntactic level.

Syntax is meaningless. Really.
Parenthesis? I'm assuming that you mean calling through the chained pointer.

Also, I think all of /, * and % are perfectly fine, and might be used
for that "aligning the pointer" operation that is fine.Quite frankly, I think all of this language that is about the actual
operations is irrelevant and wrong.

It's not going to help compiler writers, and it sure isn't going to
help users that read this.

Why not just talk about "value chains" and that any operations that
restrict the value range severely end up breaking the chain. There is
no point in listing the operations individually, because every single
operation *can* restrict things. Listing individual operations and
depdendencies is just fundamentally wrong.

For example, let's look at this obvious case:

and there are literally *zero* operations that modify the value
change, so obviously the two operations are ordered, right?

Wrong.

What if the "nothing modifies 'p'" part looks like this:

and now any sane compiler will happily optimize "q = *p" into "q =
myvariable", and we're all done - nothing invalid was ever

So my earlier suggestion tried to avoid this by having the restrict
thing, so the above wouldn't work.

But your (and the current C standards) attempt to define this with
some kind of syntactic dependency carrying chain will _inevitably_ get
this wrong, and/or be too horribly complex to actually be useful.

Seriously, don't do it. I claim that all your attempts to do this
crazy syntactic "these operations maintain the chained pointers" is
broken. The fact that you have changed "carries a dependency" to
"chained pointer" changes NOTHING.

So just give it up. It's a fundamentally broken model. It's *wrong*,
but even more importantly, it's not even *useful*, since it ends up
being too complicated for a compiler writer or a programmer to
understand.

I really really really think you need to do this at a higher
conceptual level, get away from all these idiotic "these operations
maintain the chain" crap. Because there *is* no such list.

Quite frankly, any standards text that has that
"[[carries_dependency]]" or "[[kill_dependency]]" or whatever
attribute is broken. It's broken because the whole concept is TOTALLY
ALIEN to the compiler writer or the programmer. It makes no sense.
It's purely legalistic language that has zero reason to exist. It's
non-intuitive for everybody.

And *any* language that talks about the individual operations only
encourages people to play legalistic games that actually defeat the
whole purpose (namely that all relevant CPU's are going to implement
that consume ordering guarantee natively, with no extra code
generation rules AT ALL). So any time you talk about some random
detail of some operation, somebody is going to come up with a "trick"
that defeats things. So don't do it. There is absolutely ZERO
difference between any of the arithmetic operations, be they bitwise,
additive, multiplicative, shifts, whatever.

The *only* thing that matters for all of them is whether they are
"value-preserving", or whether they drop so much information that the
compiler might decide to use a control dependency instead. That's true
for every single one of them.

Similarly, actual true control dependencies that limit the problem
space sufficiently that the actual pointer value no longer has
significant information in it (see the above example) are also things
that remove information to the point that only a control dependency
remains. Even when the value itself is not modified in any way at all.

But that's *BS*. You didn't actually listen to the main issue.

Paul, why do you insist on this carries-a-dependency crap?

It's broken. If you don't believe me, then believe the compiler person
who already piped up and told you so.

The "carries a dependency" model is broken. Get over it.

No sane compiler will ever distinguish two different registers that
have the same value from each other. No sane compiler will ever say
"ok, register r1 has the exact same value as register r2, but r2
carries the dependency, so I need to make sure to pass r2 to that
function or use it as a base pointer".

And nobody sane should *expect* a compiler to distinguish two
registers with the same value that way.

So the whole model is broken.

I gave an alternate model (the "restrict"), and you didn't seem to
understand the really fundamental difference. It's not a language
difference, it's a conceptual difference.

In the broken "carries a dependency" model, you have fight all those
aliases that can have the same value, and it is not a fight you can
win. We've had the "p-p" examples, we've had the "p&0" examples, but
the fact is, that "p==&myvariable" example IS EXACTLY THE SAME THING.

All three of those things: "p-p", "p&0", and "p==&myvariable" mean
that any compiler worth its salt now know that "p" carries no
information, and will optimize it away.

So please stop arguing against that. Whenever you argue against that
simple fact, you are arguing against sane compilers.

So *accept* the fact that some operations (and I guarantee that there
are more of those than you can think of, and you can create them with
various tricks using pretty much *any* feature in the C language)
essentially take the data information away. And just accept the fact
that then the ordering goes away too.

So give up on "carries a dependency". Because there will be cases
where that dependency *isn't* carried.

The language of the standard needs to get *away* from the broken
model, because otherwise the standard is broken.

I suggest we instead talk about "litmus tests" and why certain code
sequences are ordered, and others are not.

So the code sequence I already mentioned is *not* ordered:

   is *NOT* ordered, because the compiler can trivially turn this into
"return variable.val", and break the data dependency.

   This is true *regardless* of any "carries a dependency" language,
because that language is insane, and doesn't work when the different
pieces here may be in different compilation units.

  *IS* ordered, because while it looks syntactically a lot like
"Litmus test 1", there is no sane way a compiler can use the knowledge
that "p is not a pointer to a particular location" to break the data
dependency.

There is no way in hell that any sane "carries a dependency" model can
get the simple tests above right.

So give up on it already. "Carries a dependency" cannot work. It's a
bad model. You're trying to describe the problem using the wrong
tools.

Note that my "restrict+pointer to object" language actually got this
*right*. The "restrict" part made Litmus test 1 not ordered, because
that "p == &variable" success case means that the pointer wasn't
restricted, so the pre-requisite for ordering didn't exist.

See? The "carries a dependency" is a broken model for this, but there
are _other_ models that can work.

You tried to rewrite my model into "carries a dependency". That
*CANNOT* work. It's like trying to rewrite quantum physics into the
Greek model of the four elements. They are not compatible models, and
one of them can be shown to not work.

Btw, don't get me wrong. I don't _like_ it not being ordered, and I
actually did spend some time thinking about my earlier proposal on
strengthening the 'consume' ordering.

I have for the last several years been 100% convinced that the Intel
memory ordering is the right thing, and that people who like weak
memory ordering are wrong and should try to avoid reproducing if at
all possible. But given that we have memory orderings like power and
ARM, I don't actually see a sane way to get a good strong ordering.
You can teach compilers about cases like the above when they actually
see all the code and they could poison the value chain etc. But it
would be fairly painful, and once you cross object files (or even just
functions in the same compilation unit, for that matter), it goes from
painful to just "ridiculously not worth it".

So I think the C semantics should mirror what the hardware gives us -
and do so even in the face of reasonable optimizations - not try to do
something else that requires compilers to treat "consume" very
differently.

If people made me king of the world, I'd outlaw weak memory ordering.
You can re-order as much as you want in hardware with speculation etc,
but you should always *check* your speculation and make it *look* like
you did everything in order. Which is pretty much the intel memory
ordering (ignoring the write buffering).No, actually, it's not so much Linux-specific at all.

I'm actually thinking about what I'd do as a compiler writer, and as a
defender the "C is a high-level assembler" concept.

I love C. I'm a huge fan. I think it's a great language, and I think
it's a great language not because of some theoretical issues, but
because it is the only language around that actually maps fairly well
to what machines really do.

And it's a *simple* language. Sure, it's not quite as simple as it
used to be, but look at how thin the "K&R book" is. Which pretty much
describes it - still.

That's the real strength of C, and why it's the only language serious
people use for system programming.  Ignore C++ for a while (Jesus
Xavier Christ, I've had to do C++ programming for subsurface), and
just think about what makes _C_ a good language.

I can look at C code, and I can understand what the code generation
is, and what it will really *do*. And I think that's important.
Abstractions that hide what the compiler will actually generate are
bad abstractions.

And ok, so this is obviously Linux-specific in that it's generally
only Linux where I really care about the code generation, but I do
think it's a bigger issue too.

So I want C features to *map* to the hardware features they implement.
The abstractions should match each other, not fight each other.

But if I can give two clear examples that are basically identical from
a syntactic standpoint, and one clearly can be trivially optimized to
the point where the ordering guarantee goes away, and the other
cannot, and you cannot describe the difference, then I think your
description is seriously lacking.

And I do *not* think the C language should be defined by how it can be
described. Leave that to things like Haskell or LISP, where the goal
is some kind of completeness of the language that is about the
language, not about the machines it will run on.
Yes. Note also that that is what existing compilers would actually do.

And they'd do it "by mistake": they'd load the address of the variable
into a register, and then compare the two registers, and then end up
using _one_ of the registers as the base pointer for the "p->val"
access, but I can almost *guarantee* that there are going to be
sequences where some compiler will choose one register over the other
based on some random detail.

So my model isn't just a "model", it also happens to descibe reality.So I think carries_dependency is problematic because:

 - it's not actually in C11 afaik

 - it requires the programmer to solve the problem of the standard not
matching the hardware.

 - I think it's just insanely ugly, *especially* if it's actually
meant to work so that the current carries-a-dependency works even for
insane expressions like "a-a".

in practice, it's one of those things where I guess nobody actually
would ever use it.
I actually agree.

If you write insane code to "trick" the compiler into generating
optimizations that break the dependency, then you get what you
deserve.

Now, realistically, I doubt a compiler will notice, but if it does,
I'd go "well, that's your own fault for writing code that makes no
sense".

Basically, the above uses a pointer as a boolean flag.  The compiler
noticed it was really a boolean flag, and "consume" doesn't work on
boolean flags. Tough.
No, I 100% agree with that. "q" is *not* restricted. But "p" is, since
it came from that consuming load.

But "q = p->next" is ordered by how something can alias "p->next", not by 'q'!

There is no need to restrict anything but 'p' for all of this to work.

Btw, it's also worth pointing out that I do *not* in any way expect
people to actually write the "restrict" keyword anywhere. So no need
to change source code.

What you have is a situation where the pointer coming out of the
memory_order_consume is restricted. But if you assign it to a
non-restricted pointer, that's *fine*. That's perfectly normal C
behavior. The "restrict" concept is not something that the programmer
needs to worry about or ever even notice, it's basically just a
promise to the compiler that "if somebody has another pointer lying
around, accesses though that other pointer do not require ordering".

So it sounds like you believe that the programmer would mark things
"restrict", and I did not mean that at all.

So 'p' is what comes from that consuming load that returns a
'restrict' pointer. That doesn't affect 'q' in any way.

But the act of initializing 'q' by dereferencing p (in "p->next") is -
by virtue of the restrict - something that the compiler can see cannot
alias with anything else, so the compiler could re-order other memory
accesses freely around it, if you see what I mean.

Modulo all the *other* ordering guarantees, of course. So other
atomics and volatiles etc may have their own rules, quite apart from
any aliasing issues.Yes, and to me, it's really just a legalistic trick to make it clear
that any *other* pointer that  happens to point to the same object
cannot be dereferenced within scope of the result of the
atomic_read(mo_consume), at least not if you expect to get the memory
ordering semantics. You can do it, but then you violate the guarantee
of the restrict, and you get what you get - a potential non-ordering.

So if somebody just immediately assigns the value to a normal
(non-restrict) pointer nothing *really* changes. It's just there to
describe the guarantees.

No, the obvious alternative is to do what we do already, and just do
it by hand. Using acquire is *worse* than what we have now.

Maybe for some other users, the thing falls out differently.

No. But if you have something that is mis-designed, easy to get wrong,
and untestable, any sane programmer will go "that's bad".
No.

Read my suggestion. Knowing where to stop is *trivial*.

Either the dependency is immediate and obvious, or you treat it like an acquire.

Seriously. Any compiler that doesn't turn the dependency chain into
SSA or something pretty much equivalent is pretty much a joke. Agreed?

So we can pretty much assume that the compiler will have some
intermediate representation as part of optimization that is basically
SSA.

So what you do is,

 - build the SSA by doing all the normal parsing and possible
tree-level optimizations you already do even before getting to the SSA
stage

 - do all the normal optimizations/simplifications/cse/etc that you do
normally on SSA

 - add *one* new rule to your SSA simplification that goes something like this:

   * when you see a load op that is marked with a "consume" barrier,
just follow the usage chain that comes from that.
   * if you hit a normal arithmetic op, just follow the result chain of that
   * if you hit a memory operation address use, stop and say "looks good"
   * it you hit anything else (including a copy/phi/whatever), abort
   * if nothing aborted as part of the walk, you can now just remove
the "consume" barrier.

You can fancy it up and try to follow more cases, but realistically
the only case that really matters is the "consume" being fed directly
into one or more loads, with possibly an offset calculation in
between. There are certainly more cases you could *try* to remove the
barrier, but the thing is, it's never incorrect to not remove it, so
any time you get bored or hit any complication at all, just do the
"abort" part.

I *guarantee* that if you describe this to a compiler writer, he will
tell you that my scheme is about a billion times simpler than the
current standard wording. Especially after you've pointed him to that
gcc bugzilla entry and explained to him about how the current standard
cares about those kinds of made-up syntactic chains that he likely
removed quite early, possibly even as he was generating the semantic
tree.

Try it. I dare you. So if you want to talk about "difficulties", the
current C standard loses.

Bah.

The C standard does that all over. It's called "as-is". The C standard
talks about how the compiler can do pretty much whatever it likes, as
long as the end result acts the same in the virtual C machine.

So claiming that "semantics" being meaningful is somehow complex is
bogus. People do that all the time. If you make it clear that the
dependency chain is through the *value*, not syntax, and that the
value can be optimized all the usual ways, it's quite clear what the
end result is. Any operation that actually meaningfully uses the value
is serialized with the load, and if there is no meaningful use that
would affect the end result in the virtual machine, then there is no
barrier.

Why would this be any different, especially since it's easy to
understand both for a human and a compiler?

That is all visible in the SSA form. Variable assignment has been
converted to some use of the SSA node that generated the value. The
use might be a phi node or a cast op, or maybe it's just a memory
store op, but the whole point of SSA is that there is one single node
that creates the data (in this case that would be the "load" op with
the associated consume barrier - that barrier might be part of the
load op itself, or it might be implemented as a separate SSA node that
consumes the result of the load that generates a new pseudo), and the
uses of the result are all visible from that.

And yes, there might be a lot of users. But any complex case you just
punt on - and the difference here is that since "punt" means "leave
the barrier in place", it's never a correctness issue.

So yeah, it could be somewhat expensive, although you can always bound
that expense by just punting. But the dependencies in SSA form are no
more complex than the dependencies the C standard talks about now, and
in SSA form they are at least really easy to follow.  So if they are
complex and expensive in SSA form, I'd expect them to be *worse* in
the current "depends-on" syntax form.Btw, the actual data path may actually be semantically meaningful even
at a processor level.

For example, let's look at that gcc bugzilla that got mentioned
earlier, and let's assume that gcc is fixed to follow the "arithmetic
is always meaningful, even if it is only syntactic" the letter.
So we have that gcc bugzilla use-case:

and let's say that the fixed compiler now generates the code with the
data dependency that is actually suggested in that bugzilla entry:

ie the CPU actually sees that address data dependency. Now everything
is fine, right?

Wrong.

It is actually quite possible that the CPU sees the "and with zero"
and *breaks the dependencies on the incoming value*.

Modern CPU's literally do things like that. Seriously. Maybe not that
particular one, but you'll sometimes find that the CPU - int he
instruction decoding phase (ie very early in the pipeline) notices
certain patterns that generate constants, and actually drop the data
dependency on the "incoming" registers.

On x86, generating zero using "xor" on the register with itself is one
such known sequence.

Can you guarantee that powerpc doesn't do the same for "and r,r,#0"?
Or what if the compiler generated the much more obvious

    sub w2,w2,w2

for that "+flag-flag"? Are you really 100% sure that the CPU won't
notice that that is just a way to generate a zero, and doesn't depend
on the incoming values?

Because I'm not. I know CPU designers that do exactly this.

So I would actually and seriously argue that the whole C standard
attempt to use a syntactic data dependency as a determination of
whether two things are serialized is wrong, and that you actually
*want* to have the compiler optimize away false data dependencies.

Because people playing tricks with "+flag-flag" and thinking that that
somehow generates a data dependency - that's *wrong*. It's not just
the compiler that decides "that's obviously nonsense, I'll optimize it
away". The CPU itself can do it.

So my "actual semantic dependency" model is seriously more likely to
be *correct*. Not just t a compiler level.

Btw, any tricks like that, I would also take a second look at the
assembler and the linker. Many assemblers do some trivial
optimizations too. Are you sure that "and     w2, w2, #0" really ends
up being encoded as an "and"? Maybe the assembler says "I can do that
as a "mov w2,#0" instead? Who knows? Even power and ARM have their
variable-sized encodings (there are some "compressed executable"
embedded power processors, and there is obviously Thumb2, and many
assemblers end up trying to use equivalent "small" instructions. 

So the whole "fake data dependency" thing is just dangerous on so many levels.

MUCH more dangerous than my "actual real dependency" model.

Oh, I agree about that part - I very much understand the reason for
"consume", and I can see how it is more relaxed than "acquire" under
many circumstances.

I just think that you actually *do* want to have "consume" even for
flag values, exactly *because* it is potentially cheaper than acquire.

In fact, I'd argue that making consume reliable in the face of control
dependencies is actually a *good* thing. It may not matter for
something like x86, where consume and acquire end up with the same
simple load, but even there it might relax instruction scheduling a
bit, since a 'consume' would have a barrier just to the *users* of the
value loaded, while 'acquire' would still have a scheduling barrier to
any subsequent operations.

So I claim that for a sequence like my example, where the reader
basically does something like

the "consume" version can actually generate better code than "acquire"
- if "consume" is specified the way *I* specified it.

The way the C standard specifies it, the above code is *buggy*.
Agreed? It's really really subtly buggy, and I think that bug is not
only a real danger, I think it is logically hard to understand why.
The bug only makes sense to people who understand how memory ordering
and branch prediction interacts.

The way *I* suggested "consume" be implemented, the above not only
works and is sensible, it actually generates possibly better code than
forcing the programmer to use the (illogical) "acquire" operation.

Why? Let me give you another - completely realistic, even if obviously
a bit made up - example:

and let's all agree that this is a somewhat realistic example, and we
can imagine why/how somebody would write code like this. It's
basically a very common lazy initialization pattern, you'll find this
in libraries, in kernels, in application code yadda yadda. No
argument?

Now, let's assume that it turns out that this value ends up being
really performance-critical, so the programmer makes the fast-path an
inline function, tells the compiler that "initialized" read is likely,
and generally wants the compiler to optimize it to hell and back.
Still sounds reasonable and realistic?

In other words, the *expected* code sequence for this is (on x86,
which doesn't need any barriers):

and on ARM/power you'd see a 'sync' instruction or whatever.

So far 'acquire' and 'consume' have exacly the same code generation on
power of x86, so your argument can be: "Ok, so let's just use the
inconvenient and hard-to-understand 'consume' semantics that the
current standard has, and tell the programmer that he should use
'acquire' and not worry his little head about the difference because
he will never understand it anyway".

Sure, that would be an inconvencience for programmers, but hey,
they're programming in C or C++, so they are *expected* to be manly
men or womanly women, and a little illogical inconvenience never hurt
anybody. After all, compared to the aliasing rules, that "use acquire,
not consume" rule is positively *simple*, no?

Are we all in agreement so far?

But no, the "consume" thing can actually generate better code. Trivial example:

    int my_threads_value;
    extern int magic_system_multiplier;

    my_thread_value = return_expensive_system_value();
    my_thread_value *= magic_system_multiplier;

and in the "acquire" model, the "acquire" itself means that the load
from magic_system_multiplier is now constrained by the acquire memory
ordering on "initialized".

While in my *sane* model, where you can consume things even if they
then result in control dependencies, there will still eventually be a
"sync" instruction on powerpc (because you really need one between the
load of 'initialized' and the load of 'calculated'), but the compiler
would be free to schedule the load of 'magic_system_multiplier'
earlier.

So as far as I can tell, we want the 'consume' memory ordering to
honor *all* dependencies, because
 - it's simpler
 - it's more logical
 - it's less error-prone
 - and it allows better code generation

Hmm?
Actually, "consume" is more interesting than that. Looking at the
bugzilla entry Torvald pointed at, it has the trick to always turn any
"consume" dependency into an address data dependency.

So another reason why you *want* to allow "consume" + "control
dependency" is that it opens up the window for many more interesting
and relevant optimizations than "acquire" does.

Again, let's take that "trivial" expression:

     return atomic_read(&initialized, consume) ? value : -1;

and the compiler can actually turn this into an interesting address
data dependency and optimize it to basically use address arithmetic
and turn it into something like

    return *(&value + (&((int)-1)-value)*!atomic_read(&initialized, consume));

Of course, the *programmer* could have done that himself, but the
above is actually a *pessimization* on x86 or other strongly ordered
machines, so doing it at a source code level is actually a bad idea
(not to mention that it's horribly unreadable).

I could easily have gotten the address generation trick above wrong
(see my comment about "horribly unreadable" and no sane person doing
this at a source level), but that "complex" expression is not
necessarily at all complex for a compiler. If "value" is a static
variable, the compiler could create another read-only static variable
that contains that "-1" value, and the difference in addresses would
be a link-time constant, so it would not necessarily be all that ugly
from a code generation standpoint.

There are other ways to turn it into an address dependency, so the
whole "consume as an input to conditionals" really does seem to have
several optimization advantages (over "acquire").

Again, the way I'd expect a compiler writer to actually *do* this is
to just default to "acOops, pressed send by mistake too early.

I was almost done:

I'd expect a compiler to just default to "acquire" semantics, but then
have a few "obvious peephole" optimizations for cases that it
encounters and where it is easy to replace the synchronization point
with just an address dependency.
Yeah, so you and Paul agree. And as I mentioned in the email that
crossed with yours, I think that means that the standard is overly
complex, hard to understand, fragile, and all *pointlessly* so.

Btw, there are many ways that "use a consume as an input to a
conditional" can happen. In particular, even if the result is actually
*used* like a pointer as far as the programmer is concerned, tricks
like pointer compression etc can well mean that the "pointer" is
actually at least partly implemented using conditionals, so that some
paths end up being only dependent through a comparison of the pointer
value.

So I very much did *not* want to complicate the "litmus test" code
snippet when Paul tried to make it more complex, but I do think that
there are cases where code that "looks" like pure pointer chasing
actually is not for some cases, and then can become basically that
litmus test for some path.

Just to give you an example: in simple list handling it is not at all
unusual to have a special node that is discovered by comparing the
address, not by just loading from the pointer and following the list
itself. Examples of that would be a HEAD node in a doubly linked list
(Linux uses this concept quite widely, it's our default list
implementation), or it could be a place-marker ("cursor" entry) in the
list for safe traversal in the presence of concurrent deletes etc.

And obviously there is the already much earlier mentioned
compiler-induced compare, due to value speculation, that can basically
create such sequences even wherethey did not originally exist in the
source code itself.

So even if you work with "pointer dereferences", and thus match that
particular consume pattern, I really don't see why anybody would think
that "hey, we can ignore any control dependencies" is a good idea.
It's a *TERRIBLE* idea.

And as mentioned, it's a terrible idea with no upsides. It doesn't
help compiler optimizations for the case it's *intended* to help,
since those optimizations can still be done without the horribly
broken semantics. It doesn't help compiler writers, it just confuses
them.

And it sure as hell doesn't help users.
Oh, I agree, patent language is worse.
I know what lvalues and rvalues are. I *understand* the thinking that
goes on behind the "let's not do the access, because it's not an
rvalue, so there is no 'access' to the object".

I understand it from a technical perspective.

I don't understand the compiler writer that uses a *technicality* to
argue against generating sane code that is obviously what the user
actually asked for.

See the difference?

The volatile problem is long fixed. The people who argued for the
"legalistically correct", but insane behavior lost (and as mentioned,
I think C++11 actually fixed the legalistic reading too).

I'm bringing it up because I've had too many cases where compiler
writers pointed to standard and said "that is ambiguous or undefined,
so we can do whatever the hell we want, regardless of whether that's
sensible, or regardless of whether there is a sensible way to get the
behavior you want or not".
Alec, I know compilers. I don't do code generation (quite frankly,
register allocation and instruction choice is when I give up), but I
did actually write my own for static analysis, including turning
things into SSA etc.

No, I'm not a "compiler person", but I actually do know enough that I
understand what goes on.

And exactly because I know enough, I would *really* like atomics to be
well-defined, and have very clear - and *local* - rules about how they
can be combined and optimized.

None of this "if you can prove that the read has value X" stuff. And
things like value speculation should simply not be allowed, because
that actually breaks the dependency chain that the CPU architects give
guarantees for. Instead, make the rules be very clear, and very
simple, like my suggestion. You can never remove a load because you
can "prove" it has some value, but you can combine two consecutive
atomic accesses/

For example, CPU people actually do tend to give guarantees for
certain things, like stores that are causally related being visible in
a particular order. If the compiler starts doing value speculation on
atomic accesses, you are quite possibly breaking things like that.
It's just not a good idea. Don't do it. Write the standard so that it
clearly is disallowed.

Because you may think that a C standard is machine-independent, but
that isn't really the case. The people who write code still write code
for a particular machine. Our code works (in the general case) on
different byte orderings, different register sizes, different memory
ordering models. But in each *instance* we still end up actually
coding for each machine.

So the rules for atomics should be simple and *specific* enough that
when you write code for a particular architecture, you can take the
architecture memory ordering *and* the C atomics orderings into
account, and do the right thing for that architecture.

And that very much means that doing things like value speculation MUST
NOT HAPPEN. See? Even if you can prove that your code is "equivalent",
it isn't.

So for example, let's say that you have a pointer, and you have some
reason to believe that the pointer has a particular value. So you
rewrite following the pointer from this:

  value = ptr->val;

into

  value = speculated->value;
  tmp = ptr;
  if (unlikely(tmp != speculated))
    value = tmp->value;

and maybe you can now make the critical code-path for the speculated
case go faster (since now there is no data dependency for the
speculated case, and the actual pointer chasing load is now no longer
in the critical path), and you made things faster because your
profiling showed that the speculated case was true 99% of the time.
Wonderful, right? And clearly, the code "provably" does the same
thing.

EXCEPT THAT IS NOT TRUE AT ALL.

It very much does not do the same thing at all, and by doing value
speculation and "proving" something was true, the only thing you did
was to make incorrect code run faster. Because now the causally
related load of value from the pointer isn't actually causally related
at all, and you broke the memory ordering.

This is why I don't like it when I see Torvald talk about "proving"
things. It's bullshit. You can "prove" pretty much anything, and in
the process lose sight of the bigger issue, namely that there is code
that depends on

When it comes to atomic accesses, you don't play those kinds of games,
exactly because the ordering of the accesses matter in ways that are
not really sanely describable at a source code level. The *only* kinds
of games you play are like the ones I described - combining accesses
under very strict local rules.

And the strict local rules really should be of the type "a store
followed by a load to the same location with the same memory ordering
constraints can be combined". Never *ever* of the kind "if you can
prove X".

I hope my example made it clear *why* I react so strongly when Torvald
starts talking about "if you can prove the value is 1".

Yes.

So I think that one of the big advantages of atomics over volatile is
that they *can* be optimized, and as such I'm not at all against
trying to generate much better code than for volatile accesses.

But at the same time, that can go too far. For example, one of the
things we'd want to use atomics for is page table accesses, where it
is very important that we don't generate multiple accesses to the
values, because parts of the values can be change *by*hardware* (ie
accessed and dirty bits).

So imagine that you have some clever global optimizer that sees that
the program never ever actually sets the dirty bit at all in any
thread, and then uses that kind of non-local knowledge to make
optimization decisions. THAT WOULD BE BAD.

Do you see what I'm aiming for? Any optimization that tries to prove
anything from more than local state is by definition broken, because
it assumes that everything is described by the program.

But *local* optimizations are fine, as long as they follow the obvious
rule of not actually making changes that are semantically visible.

(In practice, I'd be impressed as hell for that particular example,
and we actually do end up setting the dirty bit by hand in some
situations, so the example is slightly made up, but there are other
cases that might be more realistic in that sometimes we do things that
are hidden from the compiler - in assembly etc - and the compiler
might *think* it knows what is going on, but it doesn't actually see
all accesses).
What "model"?

That's the thing. I have tried to figure out whether the model is some
abstract C model, or a model based on the actual hardware that the
compiler is compiling for, and whether the model is one that assumes
the compiler has complete knowledge of the system (see the example
above).

And it seems to be a mixture of it all. The definitions of the various
orderings obviously very much imply that the compiler has to insert
the proper barriers or sequence markers for that architecture, but
then there is no apparent way to depend on any *other* architecture
ordering guarantees. Like our knowledge that all architectures (with
the exception of alpha, which really doesn't end up being something we
worry about any more) end up having the load dependency ordering
guarantee.
So this is a problem. It basically means that we cannot do the kinds
of things we do now, which very much involve knowing what the memory
ordering of a particular machine is, and combining that knowledge with
our knowledge of code generation.

Now, *most* of what we do is protected by locking and is all fine. But
we do have a few rather subtle places in RCU and in the scheduler
where we depend on the actual dependency chain.

In *practice*, I seriously doubt any reasonable compiler can actually
make a mess of it. The kinds of optimizations that would actually
defeat the dependency chain are simply not realistic. And I suspect
that will end up being what we rely on - there being no actual sane
sequence that a compiler would ever do, even if we wouldn't have
guarantees for some of it.

And I suspect  I can live with that. We _have_ lived with that for the
longest time, after all. We very much do things that aren't covered by
any existing C standard, and just basically use tricks to coax the
compiler into never generating code that doesn't work (with our inline
asm barriers etc being a prime example).

So personally, if I were to write the spec, I would have taken a
completely different approach from what the standard obviously does.

I'd have taken the approach of specifying the required semantics each
atomic op (ie the memory operations would end up having to be
annotated with their ordering constraints), and then said that the
compiler can generate any code that is equivalent to that
_on_the_target_machine_.

Why? Just to avoid the whole "ok, which set of rules applies now" problem.
See above. This is exactly why I detest the C "model" thing. Now you
need ways to describe those CPU guarantees, because if you can't
describe them, you can't express them in the model.

I would *much* have preferred the C standard to say that you have to
generate code that is guaranteed to act the same way - on that machine
- as the "naive direct unoptimized translation".

IOW, I would *revel* in the fact that different machines are
different, and basically just describe the "stupid" code generation.
You'd get the guaranteed semantic baseline, but you'd *also* be able
to know that whatever architecture guarantees you have would remain.
Without having to describe those architecture issues.

It would be *so* nice if the C standard had done that for pretty much
everything that is "implementation dependent". Not just atomics.

[ will look at the rest of your email later ]

No, it's fine to do things like value-range-analysis, it's just that
you have to do it on a local scope, and not think that you can see
every single write to some variable.

And as Paul points out, this is actually generally true even outside
of kernels. We may be pretty unique in having some things that are
literally done by hardware (eg the page table updates), but we're
certainly not unique in having loadable modules or code that the
compiler doesn't know about by virtue of being compiled separately
(assembly files, JITted, whatever).

And we are actually perfectly fine with compiler barriers. One of our
most common barriers is simply this:

   #define barrier() __asm__ __volatile__("": : :"memory")

which basically tells the compiler "something changes memory in ways
you don't understand, so you cannot assume anything about memory
contents".

Obviously, a compiler is still perfectly fine optimizing things like
local variables etc that haven't had their address taken across such a
barrier (regardless of whether those local variables might be spilled
to the stack frame etc), but equally obviously we'd require that this
kind of thing makes sure that any atomic writes have been finalized by
the time the barrier happens (it does *not* imply a memory ordering,
just a compiler memory access barrier - we have separate things for
ordering requirements, and those obviously often generate actual
barrier instructions)

And we're likely always going to have things like this, but if C11
atomics end up working well for us, we might have *fewer* of them.The "hardware can change things" case is indeed pretty rare.

But quite frankly, even when it isn't hardware, as far as the compiler
is concerned you have the exact same issue - you have TLB faults
happening on other CPU's that do the same thing asynchronously using
software TLB fault handlers. So *semantically*, it really doesn't make
any difference what-so-ever if it's a software TLB handler on another
CPU, a microcoded TLB fault, or an actual hardware path.

So if the answer for all of the above is "use volatile", then I think
that means that the C11 atomics are badly designed.

The whole *point* of atomic accesses is that stuff like above should
"JustWork(tm)"
Yes, that's basically what I would want. And it is what I would expect
an atomic to be. Right now we tend to use "ACCESS_ONCE()", which is a
bit of a misnomer, because technically we really generally want
"ACCESS_AT_MOST_ONCE()" (but "once" is what we get, because we use
volatile, and is a hell of a lot simpler to write ;^).

So we obviously use "volatile" for this currently, and generally the
semantics we really want are:

 - the load or store is done as a single access ("atomic")

 - the compiler must not try to re-materialize the value by reloading
it from memory (this is the "at most once" part)

and quite frankly, "volatile" is a big hammer for this. In practice it
tends to work pretty well, though, because in _most_ cases, there
really is just the single access, so there isn't anything that it
could be combined with, and the biggest issue is often just the
correctness of not re-materializing the value.

And I agree - memory ordering is a totally separate issue, and in fact
we largely tend to consider it entirely separate. For cases where we
have ordering constraints, we either handle those with special
accessors (ie "atomic-modify-and-test" helpers tend to have some
serialization guarantees built in), or we add explicit fencing.

But semantically, C11 atomic accessors *should* generally have the
correct behavior for our uses.

If we have to add "volatile", that makes atomics basically useless. We
already *have* the volatile semantics, if atomics need it, that just
means that atomics have zero upside for us.
Well, what about *real* threads that do this, but that aren't
analyzable by the C compiler because they are written in another
language entirely (inline asm, asm, perl, INTERCA:. microcode,
PAL-code, whatever?)

I really don't think that "hardware" is necessary for this to happen.
What is done by hardware on x86, for example, is done by PAL-code
(loaded at boot-time) on alpha, and done by hand-tuned assembler fault
handlers on Sparc. The *effect* is the same: it's not visible to the
compiler. There is no way in hell that the compiler can understand the
hand-tuned Sparc TLB fault handler, even if it parsed it.Actually, it probably wouldn't really hurt code generation.

The thing is, you really have three cases:

 - architectures that have weak memory ordering and fairly stupid
hardware (power, some day maybe ARM)

 - architectures that expose a fairly strong memory ordering, but
reorders aggressively in hardware by tracking cacheline lifetimes
until instruction retirement (x86)

 - slow hardware (current ARM, crap Atom hardware etc embedded devices
that don't really do either)

and let's just agree that anybody who does high-performance work
doesn't really care about the third case, ok?

The compiler really wouldn't be that constrained in ordering on a
weakly ordered machine: sure, you'll have to add "lwsync" on powerpc
pretty much every time you have an ordered atomic read, but then you
could optimize them away for certain cases (like "oh, it was a
'consume' read, and the only consumer is already data-dependent, so
now we can remove the lwsync"), so you'd end up with pretty much the
code you looked for anyway.

And the HPC people who use the "relaxed" model wouldn't see the extra
memory barriers anyway.

And on a non-weakly ordered machine like x86, there are going to be
hardly any barriers in the first place, and the hardware will do the
right thing wrt ordering, so it's not like the compiler has gotten
prevented from any big optimization wins.

So I really think a simpler model that tied to the hardware directly -
which you need to do *anyway* since the memory ordering constraints
really are hw-dependent - would have been preferable. The people who
don't want to take advantage of hardware-specific guarantees would
never know the difference (they'll rely purely on pairing
acquire/release and fences properly - the way they *already* have to
in the current C11 model), and the people who _do_ care about
particular hw guarantees would get them by definition.
Oh, I don't even care about architectures that don't have real hardware atomics.

So if there's a software protocol for atomics, all bets are off. The
compiler almost certainly has to do atomics with function calls
anyway, and we'll just plug in out own. And frankly, nobody will ever
care, because those architectures aren't relevant, and never will be.

Sure, there are some ancient Sparc platforms that only support a
single-byte "ldstub" and there are some embedded chips that don't
really do SMP, but have some pseudo-smp with special separate locking.
Really, nobody cares. The C standard has that crazy lock-free atomic
tests, and talks about address-free, but generally we require both
lock-free and address-free in the kernel, because otherwise it's just
too painful to do interrupt-safe locking, or do atomics in user-space
(for futexes).

So if your worry is just about software protocols for CPU's that
aren't actually designed for modern SMP, that's pretty much a complete
non-issue.Let's just say that *some* are that way, and those are the ones that I
end up butting heads with.

The sane ones I never have to argue with - point them at a bug, and
they just say "yup, bug". The insane ones say "we don't need to fix
that, because if you read this copy of the standards that have been
translated to chinese and back, it clearly says that this is
acceptable".

The argument was that an lvalue doesn't actually "access" the memory
(an rvalue does), so this:

   volatile int *p = . ;

   *p;

doesn't need to generate a load from memory, because "*p" is still an
lvalue (since you could assign things to it).

This isn't an issue in C, because in C, expression statements are
always rvalues, but C++ changed that. The people involved with the C++
standards have generally been totally clueless about their subtle
changes.

I may have misstated something, but basically some C++ people tried
very hard to make "volatile" useless.

We had other issues too. Like C compiler people who felt that the
type-based aliasing should always override anything else, even if the
variable accessed (through different types) was statically clearly
aliasing and used the exact same pointer. That made it impossible to
do a syntactically clean model of "this aliases", since the _only_
exception to the type-based aliasing rule was to generate a union for
every possible access pairing.

We turned off type-based aliasing (as I've mentioned before, I think
it's a fundamentally broken feature to begin with, and a horrible
horrible hack that adds no value for anybody but the HPC people).

Gcc eventually ended up having some sane syntax for overriding it, but
by then I was too disgusted with the people involved to even care.
That's not the biggest problem.

The biggest problem is that you have compiler writers that don't care
about sane *use* of the features they write a compiler for, they just
care about the standard.

So they don't care about C vs C++ compatibility. Even more
importantly, they don't care about the *user* that uses only C++ and
the fact that their reading of the standard results in *meaningless*
behavior. They point to the standard and say "that's what the standard
says, suck it", and silently generate code (or in this case, avoid
generating code) that makes no sense.

So it's not about C++ being incompatible with C, it's about C++ having
insane and bad semantics unless you just admit that "oh, ok, I need to
not just read the standard, I also need to use my brain, and admit
that a C++ statement expression needs to act as if it is an "access"
wrt volatile variables".

In other words, as a compiler person, you do need to read more than
the paper of standard. You need to also take into account what is
reasonable behavior even when the standard could possibly be read some
other way. And some compiler people don't.

The "volatile access in statement expression" did get resolved,
sanely, at least in gcc. I think gcc warns about some remaining cases.

Btw, afaik, C++11 actually clarifies the standard to require the
reads, because everybody *knew* that not requiring the read was insane
and meaningless behavior, and clearly against the intent of
"volatile".

But that didn't stop compiler writers from saying "hey, the standard
allows my insane and meaningless behavior, so I'll implement it and
not consider it a bug".
Yes. But I don't think it's "independent".

Exactly *because* some people will read standards without applying
"does the resulting code generation actually make sense for the
programmer that wrote the code", the standard has to be pretty clear.

The standard often *isn't* pretty clear. It wasn't clear enough when
it came to "volatile", and yet that was a *much* simpler concept than
atomic accesses and memory ordering.

And most of the time it's not a big deal. But because the C standard
generally tries to be very portable, and cover different machines,
there tends to be a mindset that anything inherently unportable is
"undefined" or "implementation defined", and then the compiler writer
is basically given free reign to do anything they want (with
"implementation defined" at least requiring that it is reliably the
same thing).

And when it comes to memory ordering, *everything* is basically
non-portable, because different CPU's very much have different rules.
I worry that that means that the standard then takes the stance that
"well, compiler re-ordering is no worse than CPU re-ordering, so we
let the compiler do anything". And then we have to either add
"volatile" to make sure the compiler doesn't do that, or use an overly
strict memory model at the compiler level that makes it all pointless.

So I really really hope that the standard doesn't give compiler
writers free hands to do anything that they can prove is "equivalent"
in the virtual C machine model. That's not how you get reliable
results.

Oh, I'm perfectly fine with that. But we're not desperate to buy, and
I actually think the C11 people are - or at least should be - *way*
more desperate to sell their model to us than we are to take it.

Which means that as a buyer you should say "this is what we want, if
you don't give us this, we'll just walk away". Not try to see how much
we can pay for it. Because there is very little upside for us, and
_unless_ the C11 standard gets things right it's just extra complexity
for us, coupled with new compiler fragility and years of code
generation bugs.

Why would we want that extra complexity and inevitable compiler bugs?
If we then have to fight compiler writers that point to the standard
and say ". but look, the standard says we can do this", then at that
point it went from "extra complexity and compiler bugs" to a whole
'nother level of frustration and pain.

So just walk away unless the C11 standard gives us exactly what we
want. Not "something kind of like what we'd use". EXACTLY. Because I'm
not in the least interested in fighting compiler people that have a
crappy standard they can point to. Been there, done that, got the
T-shirt and learnt my lesson.

And the thing is, I suspect that the Linux kernel is the most complete
- and most serious - user of true atomics that the C11 people can sell
their solution to.

If we don't buy it, they have no serious user. Sure, they'll have lots
of random other one-off users for their atomics, where each user wants
one particular thing, but I suspect that we'll have the only really
unified portable code base that handles pretty much *all* the serious
odd cases that the C11 atomics can actually talk about to each other.

Oh, they'll push things through with or without us, and it will be a
collection of random stuff, where they tried to please everybody, with
particularly compiler/architecture people who have no f*cking clue
about how their stuff is used pushing to make it easy/efficient for
their particular compiler/architecture.

But we have real optimized uses of pretty much all relevant cases that
people actually care about.

We can walk away from them, and not really lose anything but a small
convenience (and it's a convenience *only* if the standard gets things
right).

And conversely, the C11 people can walk away from us too. But if they
can't make us happy (and by "make us happy", I really mean no stupid
games on our part) I personally think they'll have a stronger
standard, and a real use case, and real arguments. I'm assuming they
want that.

That's why I complain when you talk about things like marking control
dependencies explicitly. That's *us* bending over backwards. And as a
buyer, we have absolutely zero reason to do that.

Tell the C11 people: "no speculative writes". Full stop. End of story.
Because we're not buying anything else.

Similarly, if we need to mark atomics "volatile", then now the C11
atomics are no longer even a "small convenience", now they are just
extra complexity wrt what we already have. So just make it clear that
if the C11 standard needs to mark atomics volatile in order to get
non-speculative and non-reloading behavior, then the C11 atomics are
useless to us, and we're not buying.

Remember: a compiler can *always* do "as if" optimizations - if a
compiler writer can prove that the end result acts 100% the same using
an optimized sequence, then they can do whatever the hell they want.
That's not the issue. But if we can *ever* see semantic impact of
speculative writes, the compiler is buggy, and the compiler writers
need to be aware that it is buggy. No ifs, buts, maybes about it.

So I'm perfectly fine with you seeing yourself as a buyer. But I want
you to be a really *picky* and anal buyer - one that knows he has the
upper hand, and can walk away with no downside.I should have somebody who proof-reads my emails before I send them out.

I obviously meant "if they *can* make us happy" (not "can't").

            Ok, I'm fine with that.

No, please don't use this idiotic example. It is wrong.

The fact is, if a compiler generates anything but the obvious sequence
(read/cmp/branch/store - where branch/store might obviously be done
with some other machine conditional like a predicate), the compiler is
wrong.

Anybody who argues anything else is wrong, or confused, or confusing.

Instead, argue about *other* sequences where the compiler can do something.

For example, this sequence:

can validly be transformed to

and I think everybody agrees about that. In fact, that optimization
can be done even for mo_strict.

But even that "obvious" optimization has subtle cases. What if the
store is relaxed, but the load is strict? You can't do the
optimization without a lot of though, because dropping the strict load
would drop an ordering point. So even the "store followed by exact
same load" case has subtle issues.

With similar caveats, it is perfectly valid to merge two consecutive
loads, and to merge two consecutive stores.

Now that means that the sequence

can first be optimized to

and then you get the end result that you wanted in the first place
(including the ability to re-order the two stores due to the relaxed
ordering, assuming they can be proven to not alias - and please don't
use the idiotic type-based aliasing rules).

Bringing up your first example is pure and utter confusion. Don't do
it. Instead, show what are obvious and valid transformations, and then
you can bring up these kinds of combinations as "look, this is
obviously also correct".

Now, the reason I want to make it clear that the code example you
point to is a crap example is that because it doesn't talk about the
*real* issue, it also misses a lot of really important details.

For example, when you say "if the compiler can prove that the
conditional is always true" then YOU ARE TOTALLY WRONG.

So why do I say you are wrong, after I just gave you an example of how
it happens? Because my example went back to the *real* issue, and
there are actual real semantically meaningful details with doing
things like load merging.

To give an example, let's rewrite things a bit more to use an extra variable:

which looks exactly the same. If you now say "if you can prove the
conditional is always true, you can make the store unconditional", YOU
ARE WRONG.

Why?

This sequence:

    atomic_store(&x, 1, mo_relaxed);
    a = atomic_load(&x, mo_relaxed);
    atomic_store(&y, 3, mo_relaxed);

is actually - and very seriously - buggy.

Why? Because you have effectively split the atomic_load into two loads
- one for the value of 'a', and one for your 'proof' that the store is
unconditional.

Maybe you go "Nobody sane would do that", and you'd be right. But
compilers aren't "sane" (and compiler _writers_ I have some serious
doubts about), and how you describe the problem really does affect the
solution.

My description ("you can combine two subsequent atomic accesses, if
you are careful about still having the same ordering points") doesn't
have the confusion. It makes it clear that no, you can't speculate
writes, but yes, obviously you can combine certain things (think
"write buffers" and "memory caches"), and that may make you able to
remove the conditional. Which is very different from speculating
writes.

But my description also makes it clear that the transformation above
was buggy, but that rewriting it as

    a = 1;
    atomic_store(&y, 3, mo_relaxed);
    atomic_store(&x, a, mo_relaxed);

is fine (doing the re-ordering here just to make a point).

So I suspect we actually agree, I just think that the "example" that
has been floating around, and the explanation for it, has been
actively bad and misleading.It's not illegal code, but i you claim that you can make that store
unconditional, it's a pointless and wrong example.
The thing is, the first load DOES NOT RETURN 1. It returns whatever
that memory location contains. End of story.

Stop claiming it "can return 1".  It *never* returns 1 unless you do
the load and *verify* it, or unless the load itself can be made to go
away. And with the code sequence given, that just doesn't happen. END
OF STORY.

So your argument is *shit*. Why do you continue to argue it?

I told you how that load can go away, and you agreed. But IT CANNOT GO
AWAY any other way. You cannot claim "the compiler knows". The
compiler doesn't know. It's that simple.That is a new example. The important part is that it has left a
"trace" for the programmer: because 'a' contains the value, the
programmer can now look at the value later and say "oh, we know we did
a store iff a was 1"
Basically, if the compiler allows the condition of "I wrote 3 to the
y, but the programmer sees 'a' has another value than 1 later" then
the compiler is one buggy pile of shit. It fundamentally broke the
whole concept of atomic accesses. Basically the "atomic" access to 'x'
turned into two different accesses: the one that "proved" that x had
the value 1 (and caused the value 3 to be written), and the other load
that then write that other value into 'a'.

It's really not that complicated.

And this is why descriptions like this should ABSOLUTELY NOT BE
WRITTEN as "if the compiler can prove that 'x' had the value 1, it can
remove the branch". Because that IS NOT SUFFICIENT. That was not a
valid transformation of the atomic load.

The only valid transformation was the one I stated, namely to remove
the load entirely and replace it with the value written earlier in the
same execution context.

Really, why is so hard to understand?This is the very example I gave, where the real issue is not that "you
prove that load returns 1", you instead say "store followed by a load
can be combined".

I (in another email I just wrote) tried to show why the "prove
something is true" is a very dangerous model.  Seriously, it's pure
crap. It's broken.

If the C standard defines atomics in terms of "provable equivalence",
it's broken. Exactly because on a *virtual* machine you can prove
things that are not actually true in a *real* machine. I have the
example of value speculation changing the memory ordering model of the
actual machine.

See?
What BS is that? If you use an "atomic_store_explicit()", by
definition you're either

 (a) f*cking insane
 (b) not doing sequential non-synchronizing code

and a compiler that assumes that the programmer is insane may actually
be correct more often than not, but it's still a shit compiler.
Agreed?

So I don't see how any sane person can say that speculative writes are
ok. They are clearly not ok.

Speculative stores are a bad idea in general. They are completely
invalid for anything that says "atomic". This is not even worth
discussing.

In Paul's example, they were marked specially.

And you seemed to argue that Paul's example could possibly return
anything but 0/0.

If you really think that, I hope to God that you have nothing to do
with the C standard or any actual compiler I ever use.

Because such a standard or compiler would be shit. It's sadly not too uncommon.

Ahh, I am happy to have misunderstood. The "intuitively" threw me,
because I thought that was building up to a "but", and misread the
rest.

I then react stronly, because I've seen so much total crap (the
type-based C aliasing rules topping my list) etc coming out of
standards groups because it allows them to generate wrong code that
goes faster, that I just assume compiler people are out to do stupid
things in the name of ". but the standard allows it".I think that's a bad idea as a generic implementation, but it's quite
possibly worth doing on an architecture-by-architecture basis.

The thing is, gcc builtins sometimes suck. Sometimes it's because
certain architectures want particular gcc versions that don't do a
good job, sometimes it's because the architecture doesn't lend itself
to doing what we want done and we end up better off with some tweaked
thing, sometimes it's because the language feature is just badly
designed. I'm not convinced that the C11 people got things right.

But in specific cases, maybe the architecture wants to use the
builtin. And the gcc version checks are likely to be
architecture-specific for a longish while. So some particular
architectures might want to use them, but I really doubt it makes
sense to make it the default with arch overrides.Who uses these, and why?

The "_return()" versions of atomic ops are noticeably slower and more
complex on common architectures (ie x86), and apparently there is no
use of them since they didn't exist.

So why add them? Just to encourage people to do bad things?

                  

It's not the set/clear_mask ones I would worry about.

It's the "_return" variants. As far as I can tell, there are exactly
ZERO users of that stuff, and they are BAD BAD BAD.

On x86, those things would cause a cmpxchg loop, and the bad
read-for-shared-before-acquire cacheline pattern.

So why indirectly encourage people to add users for a bad operation?Ok, I think this is in a mergable form.

I wonder what the heck that insane tlb flush is in
kgdb_flush_swbreak_addr(), but it predates this, and while it makes no
sense to me, the patch makes it no worse. And I can't find it in
myself to care.

Does anybody see anything odd? If not, add an acked-by from me and put
it in -mm, or what?

                
Correct me if I'm wrong, but this can not possibly be correct.

vmacache_invalidate_all() walks over all the threads of the current
process, but "mm" here is the mm of the *new* process that is getting
created, and is unrelated in all ways to the threads of the old
process.

So it walks completely the wrong list of threads.

In fact, the sequence number of the old vm and the sequence number of
the new vm cannot in any way be related.

As far as I can tell, the only sane thing to do at fork/clone() time is to:

 - clear all the cache entries (of the new 'struct task_struct'! - so
not in dup_mmap, but make sure it's zeroed when allocating!)(

 - set vmcache_seqnum to 0 in dup_mmap (since any sequence number is
fine when it got invalidated, and 0 is best for "avoid overflow").

but I haven't thought deeply about this, but I pretty much guarantee
that the quoted sequence above is wrong as-is.

No, you're missing the much more fundamental issue: you are *not*
incrementing the current mm sequence number at all.

This code here:

    mm->vmacache_seqnum = oldmm->vmacache_seqnum + 1;

doesn't change the "oldmm" sequence number.

So invalidating the threads involved with the current sequence number
is totally bogus. It's a completely nonsensical operation. You didn't
do anything to their sequence numbers.

Your argument that "we still need to deal with the rest of the tasks"
is bogus. There is no "rest of the tasks". You're creating a new mm,
WHICH DOESN'T HAVE ANY TASKS YET!

(Well, there is the one half-formed task that is also in the process
of being created, but that you don't even have access to in
"dup_mmap()").

It's also not sensible to make the new vmacache_seqnum have anything
to do with the *old* vmacache seqnum. They are two totally unrelated
things, since they are separate mm's, and they are tied to independent
threads. They basically have nothing in common, so initializing the
new mm sequence number with a value that is related to the old
sequence number is not a sensible operation anyway.

So dup_mmap() should just clear the mm->seqnum, and not touch any
vmacache entries. There are no current vmacache entries associated
with that mm anyway.

And then you should clear the new vmacache entries in the thread
structure, and set vmacache_seqnum to 0 there too. You can do that in
"dup_mm()", which has access to the new task-struct.

And then you're done, as far as fork() is concerned.

Now, the *clone* case (CLONE_VM) is different. See "copy_mm()". For
that case, you should probably just copy the vma cache from the old
thread, and set the sequence number to be the same one. That's
correct, since you are re-using the mm. But in practice, you don't
actually need to do anything, since "dup_task_struct()" should have
done this all. So the CLONE_VM case doesn't actually require any
explicit code, because copying the cache and the sequence number is
already the right thing to do.

Btw, that all reminds me:

The one case you should make sure to double-check is the "exec" case,
which also creates a new mm. So that should clear the vmcache entries
and the seqnum.  Currently we clear mm->mmap_cache implicitly in
mm_alloc() because we do a memset(mm, 0) in it.

So in exec_mmap(), as you do the activate_mm(), you need to do that
same "clear vmacache and sequence number for *this* thread (and this
thread *only*)".Btw, as far as I can tell, that also makes the per-thread vmacache
automatically do the right thing for the non-MMU case, so that you
could just remove the difference between CONFIG_MMU and NOMMU.

Basically, dup_mmap() should no longer have anything to do with the
vmacache, since it is now per-thread, not per-mm.

So :

 - allocating a new "struct mm_struct" should clear the
vmacache_seqnum for that new mm, to try to minimize unnecessary future
overflow.

 - thread allocation should just zero the cache entries, and set
"tsk->vmacache_seqnum = mm->vmacache_seqnum" (after dup_mm()) to avoid
future unnecessary flushes.

and as far as I can tell, the logic would be exactly the same on NOMMU
(the dup_mm just doesn't happen, since all forks are basically sharing
mm).

And maybe you'd want to make VMACACHE_SIZE be 1 on NOMMU (and make
sure to change the "& 3" to "& (VMACACHE_SIZE-1)". Just to keep the
size down on small systems that really don't need it.

The major issue is user space support.

So what do others that support this do?  Looking at the gitweb for
ls.c in coreutils, we find:

http://git.savannah.gnu.org/gitweb/?p=coreutils.git;a=blob;f=src/ls.c

so that's presumably what we should use.So?

Remeber: whiteout entries do not exist "normally". No normal apps
should care or see them, since the whole and only point of them is
when they are part of a union mount (in which case they are not
visible).

So the "how do you see whiteouts" is really only about the raw
filesystem mount when *not* in the normal place.

IOW, it's not like these guys are going to show up in users home
directories etc. It's more like a special device node than a file - we
need to care about some basic system management interfaces, not about
"random apps". So "coreutils" is the primary user, although I guess a
few IT people would prefer for things like Nautilus etc random file
managers to be able to show them nicely too. But if they show up as an
icon with a question mark on them or whatever, that's really not a big
deal either.

Sure, maybe they'll look odd in some graphical file chooser *if*
somebody makes them show up, but I think creation of a whiteout - if
we allow it at all outside of the union mount itself - should be a
root-only thing (the same way mknod is) so quite frankly, it falls
under "filesystem corruption makes my directory listings look odd -
cry me a river".

(I do think we should allow creation - but for root only - for
management and testing purposes, but I really think it's a secondary
issue, and I do think we should literally use "mknod()" - either with
a new S_IFWHT or even just making use of existing S_IFCHR just so you
could use the user-space "mknod" to create it with some magic
major/minor combination.

Side note: I think 0/0 is the right choice, for a very specific
reason: it is already documented as being special. No other
combination has that.

We've had "major number 0" documented as being for unnamed devices,
and minor 0 is "reserved as null device number", which is just bad
documentation (it's *not* /dev/null, it doesn't exist). You cannot
register a character device with mijor/minor 0 in Linux, for example.

(The block layer similarly considers MKDEV(0,0) to be an unallocated device)
I think it had better work exactly like a special node (eg character
device etc). I don't know about creation (yes, we might even fake it
with mknod(), or just say that the only way to create them is as part
of the union-fs), but removal and renaming should absolutely *not* be
a new system call. That would be a disaster for any system admin,
having to use special tools to edit the filesystem.

Obviously when it is part of a union mount, whiteouts work differently
- they must *not* show up in getdents, and you can't rename/remove a
whiteout anywhere else. But that is obviously part of the union-fs,
nor the low-level filesystem itself.
I could care less for the paravirt case. As long as the native case
ends up being sane (even when CONFIG_PARAVIRT is set at compile time),
I'm fine.

When actually running in a paravirtualized environment, locks are
always going to have problems.

                  
Just use that 'paravirt_ticketlocks_enabled' static key. Xen enables it too.
Yup. Or rather,  I'd suggest implement just one version of
arch_spin_lock(), but at the top of it you do something like

which should basically generate almost-perfect code:  it's one extra
no-op for the native case if CONFIG_PARAVIRT_SPINLOCK is on, which
turns into a branch for the unfair version for paravirtualization.

Or something like that.

For non-paravirtualized environments, we should basically optimize for
the actual hardware, and assume that the full virtualization is good
enough that we don't care.

Obviously, sometimes we might take "this is particularly expensive to
virtualize" into account, and if it doesn't hurt hardware, try to help
the virtual environment along. So you might have cases where certain
hardware (typically interrupt controllers etc) should be accessed
certain particular ways, because those not only work on real hardware,
but behave well in virtualized environments too.

But for spinlocks, just make the raw hardware work well when not
*actively* paravirtualized. If some hypervisor deals badly with it,
it's a hypervisor or management problem.Ok, so I like this one much better than the previous version.

However, I do wonder if the per-mm vmacache is actually worth it.
Couldn't the per-thread one replace it entirely?

Also, the hash you use for the vmacache index is *particularly* odd.

        int idx =  (addr >> 10) & 3;

you're using the top two bits of the address *within* the page.
There's a lot of places that round addresses down to pages, and in
general it just looks really odd to use an offset within a page as an
index, since in some patterns (linear accesses, whatever), the page
faults will always be to the beginning of the page, so index 0 ends up
being special.

What am I missing?

Ok, I'd prefer the simpler model of just a single per-thread hashed
lookup, and then we could perhaps try something more complex if there
are particular loads that really matter. I suspect there is more
upside to playing with the hashing of the per-thread cache (making it
three bits, whatever) than with some global thing.
Hmm. Numbers talk, bullshit walks. So if you have the numbers that say
this is actually a good model. 

I guess that for any particular page, only the first access address
matters. And if it really is a "somewhat linear", and the first access
tends to hit in the first part of the page, and the cache index tends
to cluster towards idx=0. And for linear accesses, I guess *any*
clustering is actually a good thing, since spreading things out just
defeats the fact that linear accesses also tend to hit in the same
vma.

And if you have truly fairly random accesses, then presumably their
offsets within the page are fairly random too, and so hashing by
offset within page might work well to spread out the vma cache
lookups.

So I guess I can rationalize it. I just found it surprising, and I
worry a bit about us sometimes just masking the address, but I guess
this is all statistical *anyway*, so if there is some rare path that
masks the address, I guess we don't care, and the only thing that
matters is the hitrate.
So just

works fine?

That makes me think it all just wants to be maximally spread out to
approximate some NRU when adding an entry.

 Also, as far as I can tell, "vmacache_update()" should then become
just a simple unconditional

because your original code did

and that doesn't seem to make sense, since if "newvma" was already in
the cache, then we would have found it when looking up, and we
wouldn't be here updating it after doing the rb-walk? And with the
per-mm cache removed, all that should remain is that simple version,
no? You don't even need the "check the vmcache sequence number and
clear if bogus", because the rule should be that you have always done
a "vmcache_find()" first, which should have done that. 

Anyway, can you send the final cleaned-up and simplfied (and
re-tested) version? There's enough changes discussed here that I don't
want to track the end result mentally. 

I guess we should do something like

just to not have to worry about it.

And we can either use a "#ifndef CONFIG_64BIT" to disable it for the
64-bit case (because no, we really don't need to worry about overflow
in 64 bits ;), or just decide that a 32-bit sequence number actually
packs better in the structures, and make it be an "u32" even on 64-bit
architectures?

It looks like a 32-bit sequence number might pack nicely next to the

but I didn't actually go and look at the context there to see what
else is there. Hey, things are looking pretty normal, and rc4 is smaller than rc3, so
I'm happy.

The biggest patch in here (accounting for about a sixth of the total)
is just DaveJ re-indenting a reiserfs file. Ignoring that whitespace
cleanup, the rest is mostly the usual mix of drivers, networking and
some architecture updates.

Nothing big, and nothing that looks particularly scary.

So get to it, and test it all out.
stop printing the hex addresses entirely.

However, your kind of script actually makes that worse, in that it
uses the redundant hex addresses for 'addr2line', and that tool is
known to not work with symbolic addresses, only with actual numerical
ones.

So I would *really* want to do this kernel change (possibly
conditional on RANDOMIZE_BASE_ADDRESS or whatever the config variable
is called):

which would make the kernel stack traces much prettier.

But that would require that there be a "resolve symbolic address" (if
CONFIG_KALLSYMS isn't enabled, it would still be hexadecimal) for the
address inside the [<>] thing. 

I don't know of any sane tool that does that directly, but it
shouldn't be *that* hard. You can *almost* do it with

  echo "p backtrace_regression_test+0x38" | gdb vmlinux

but you see the problem if you try that ;)We could easily get rid of printk_[stack_]address() by just inlining
it into the callers, and make them use the [<%p[SB]>] thing directly.

That's a separate issue from trying to get rid of the hex parts of the
address, though.

Also, note that even if you merged printk_stack_address into its
callers, you'd still have the actual stack walking ->address() and
->stack() calls as separate calls, so it's not like you'd really avoid
any interleaving if there are concurrent walkers.So far so good. I really do want signed tags for hosting setups I
don't know, but oss.sgi.com is not some random public hosting site,
and as long as this gets sorted out by the next merge window it's all
good.

                
Looks more like it's just a few cross-merges, which then ends up
meaning that there are multiple merge-bases and not just one common
ancestor commit.

In that case, a simple "git diff" can't do a great job, and the actual
merge diff will usually end up different.When I made the rc2 announcement, I mentioned how nice and small it
was. I also mentioned that I mistrusted you guys, and that I suspected
that some people were giggling to themselves and holding back their
pull requests, evil little creatures like you are.

And I hate being right. rc2 was quite small, but rc3 makes up for it.
I quietly took all the pull requests because clearly this wasn't a
surprise, but I'm warning you - I'm going to start cursing at people
if it looks like this trend is going to continue. You've been warned.

Anyway, because the shortlog is pretty hefty, I'm doing my overview
"mergelog" here, and will post the shortlog separately as a reply to
this email for people who want more detail. The bulk is the networking
and random drivers (net, staging, usb, block, infiniband. ) but
there's some arch updates too (powerpc, arm, x86) and some
documentation updates.
(And as always for my mergelog - the people credited are the people
who send me the pull request, which is generally different from the
author of the code. See the shortlog for actual code authorship)

And here's the promised shortlog.

Of course, I hope people are using git, and these are all totally
redundant, but maybe some people are more interested in hearing about
the statistics than actually getting the updates.

You really should compile and test it, though. By -rc3, the scary
stuff should have all been fixed. Not that there really was that much
scary stuff this time around, I think.

Passing addresses in registers is usually a *horrible* thing for the
kernel, because most of the time they are actually offsets within some
structure, and the address is almost invariably of the type
"offset(%register)". Which means that if you have to pass it as a pure
register, you are now (a) wasting a register and (b) adding an extra
"lea" or similar instruction just to do so.

Sadly, there is no good constraint for something like that. There's
"o" for "offsetable", but that is actually perfectly fine with a
PC-relative thing.

My preferred solution would be to (a) just say that it cannot be done
for replacement instructions (but is ok for the original
non-replacement one) and (b) perhaps have some build-time check of
just the replacement tables.

So then we'd at least get a build-time failure, and any users could be
taught that they have to use some obfuscation macro - I know we have
them, I can't recall their name, we've used them for the percpu stuff
- to force the address to be in a register rather than to be
%rip-relative.

Does that sound doable at all?Yes, I do think we violate POSIX here because of how we handle f_pos
(the earlier thread from 2006 you point to talks about the "thread
safe" part, the point here about the actual wording of "atomic" is a
separate issue).

Long long ago we used to just pass in the pointer to f_pos directly,
and then the low-level write would update it all under the inode
semaphore, and all was good.

And then we ended up having tons of problems with non-regular files
and drivers accessing f_pos and having nasty races with it because
they did *not* have any locking (and very fundamentally didn't want
any), and we broke the serialization of f_pos. We still do the *IO*
atomically, but yes, the f_pos access itself is outside the lock.

Ho humm.  Al, any ideas of how to fix this?

So this is a great example, and in general I really like your page at:

and the reason I like your page is that it really talks about the
problem by pointing to the "unoptimized" code, and what hardware would
do.

As mentioned, I think that's actually the *correct* way to think about
the problem space, because it allows the programmer to take hardware
characteristics into account, without having to try to "describe" them
at a source level.

As to your example of

I really think that it is ok to combine that into

(by virtue of "exact same behavior on actual hardware"), and then the
only remaining question is whether the "ra?A:B" can be optimized to
remove the conditional if A==B as in your example where both are "42".
Agreed?

Now, I would argue that the "naive" translation of that is
unambiguous, and since "ra" is not volatile or magic in any way, then
"ra?42:42" can obviously be optimized into just 42 - by the exact same
rule that says "the compiler can do any transformation that is
equivalent in the hardware". The compiler can *locally* decide that
that is the right thing to do, and any programmer that complains about
that decision is just crazy.

So my "local machine behavior equivalency" rule means that that
function can be optimized into a single "store 42 atomically into rb".

Now, if it's *not* compiled locally, and is instead implemented as a
macro (or inline function), there are obviously situations where "ra ?
A : B" ends up having to do other things. In particular, X may be
volatile or an atomic read that has ordering semantics, and then that
expression doesn't become just "42", but that's a separate issue. It's
not all that dissimilar to "function calls are sequence points",
though, and obviously if the source of "ra" has semantic meaning, you
have to honor that semantic meaning.

Agreed?
So I agree, but I think that's a generic issue with non-local memory
ordering, and is not at all specific to the optimization wrt that
"x?42:42" expression.

If you have a value that you loaded with a non-relaxed load, and you
pass that value off to a non-local function that you don't know what
it does, in my opinion that implies that the compiler had better add
the necessary serialization to say "whatever that other function does,
we guarantee the semantics of the load".

So on ppc, if you do a load with "consume" or "acquire" and then call
another function without having had something in the caller that
serializes the load, you'd better add the lwsync or whatever before
the call. Exactly because the function call itself otherwise basically
breaks the visibility into ordering. You've basically turned a
load-with-ordering-guarantees into just an integer that you passed off
to something that doesn't know about the ordering guarantees - and you
need that "lwsync" in order to still guarantee the ordering.

Tough titties. That's what a CPU with weak memory ordering semantics
gets in order to have sufficient memory ordering.

And I don't think it's actually a problem in practice. If you are
doing loads with ordered semantics, you're not going to pass the
result off willy-nilly to random functions (or you really *do* require
the ordering, because the load that did the "acquire" was actually for
a lock!

So I really think that the "local optimization" is correct regardless.

Hmm. Yeah, clearly this isn't working, since the real workloads all
end up looking like
 becomes
and
becomes
and
becomes
ie it shows a clear reduction in faults, but the added costs clearly
eat up any wins and it all becomes (just _slightly_) slower.

Sad.

I do wonder if we really need to lock the pages we fault in. We lock
them in order to test for being up-to-date and still mapped. The
up-to-date check we don't really need to worry about: that we can test
without locking by just reading "page->flags" atomically and verifying
that it's uptodate and not locked.

The other reason to lock the page is:

 - for anonymous pages we need the lock for rmap, so the VM generally
always locks the page. But that's not an issue for file-backed pages:
the "rmap" for a filebacked page is just the page mapcount and the
cgroup statistics, and those don't need the page lock.

 - the whole truncation/unmapping thing

So the complex part is racing with truncate/unmapping the page. But
since we hold the page table lock, I *think* what we should be able to
do is:

 - increment the page _mapcount (iow, do "page_add_file_rmap()"
early). This guarantees that any *subsequent* unmap activity on this
page will walk the file mapping lists, and become serialized by the
page table lock we hold.

 - mb_after_atomic_inc() (this is generally free)

 - test that the page is still unlocked and uptodate, and the page
mapping still points to our page.

 - if that is true, we're all good, we can use the page, otherwise we
decrement the mapcount (page_remove_rmap()) and skip the page.

Hmm? Doing something like this means that we would never lock the
pages we prefault, and you can go back to your gang lookup rather than
that "one page at a time". And the race case is basically never going
to trigger.

Comments?

Hmm. I don't hate this. Looking through it, it's fairly simple
conceptually, and the code isn't that complex either. I can live with
this.

I think it's a bit odd how you pass both "max_pgoff" and "nr_pages" to
the fault-around function, though. In fact, I'd consider that a bug.
Passing in "FAULT_AROUND_PAGES" is just wrong, since the code cannot -
and in fact *must* not - actually fault in that many pages, since the
starting/ending address can be limited by other things.

So I think that part of the code is bogus. You need to remove
nr_pages, because any use of it is just incorrect. I don't think it
can actually matter, since the max_pgoff checks are more restrictive,
but if you think it can matter please explain how and why it wouldn't
be a major bug?

Apart from that, I'd really like to see numbers for different ranges
of FAULT_AROUND_ORDER, because I think 5 is pretty high, but on the
whole I don't find this horrible, and you still lock the page so it
doesn't involve any new rules. I'm not hugely happy with another raw
radix-tree user, but it's not horrible.

Btw, is the "radix_tree_deref_retry(page) -> goto restart" really
necessary? I'd be almost more inclined to just make it just do a
"break;" to break out of the loop and stop doing anything clever at
all.

IOW, from a quick look there's a couple of small details I don't like
that look odd, but . 

Yeah, but that should be trivial to do, and limit it to FAULT_AROUND_ORDER.So the reason I'd prefer to limit the whole thing to that is to not
generate too many extra cache misses. It would be lovely if we stayed
withing one or two cachelines of the page table entry that we have to
modify anyway.

But it would be really interesting to see the numbers for different
FAULT_AROUND_ORDER and perhaps different variations of this.

Ok. Maybe somebody else screams bloody murder, but considering that
you got 1%+ performance improvements (if I read your numbers right), I
think it looks quite promising, and not overly horrid.

Having some complexity and layering violation that is strictly all in
mm/filemap.c I don't see as horrid.

I would probably *not* like random drivers start to use that new
'fault_nonblock' thing, though.It's not "holepunches and truncate", it's "holepunches and page
mapping", and I do think we currently serialize the two - the whole
"check page->mapping still being non-NULL" before mapping it while
having the page locked does that.

Besides, that per-page locking should serialize against truncate too.
No, there is no "global" serialization, but there *is* exactly that
page-level serialization where both truncation and hole punching end
up making sure that the page no longer exists in the page cache and
isn't mapped.

I'm just claiming that *because* of the way rmap works for file
mappings (walking the i_mapped list and page tables), we should
actually be ok.  The anonymous rmap list is protected by the page
lock, but the file-backed rmap is protected by the pte lock (well, and
the "i_mmap_mutex" that in turn protects the i_mmap list etc).
Hmm. With truncate, we should be protected by i_size being changed
first (iirc - I didn't actually check), but I think you're right that
hole punching might race with a page being mapped at the same time.
Actually, Kirill's latest patch seems to solve the problem with
locking - by simply never locking more than one page at a time. So I
guess it's all moot at least wrt the page preload. 

Actually, you cannot get into that situation, since the definition of
"found" is that you have to follow the page tables (remember: this is
a *file* mapping, not an anonymous one, so you don't actually have an
rmap list, you have the inode mapping list).

And since we hold the page table lock, you cannot actually get to the
point where you see that it's not mapped yet. See?

That said:

The BUG_ON() itself could trigger, because it could race with us
optimistically trying to increment the page mapping code. And yes, we
might have to remove that.

But the actual "unmap page" logic should not be able to ever see any
difference.

See my argument?

I really would prefer for the loop to be much smaller than that, and
not contain indirect calls to helpers that pretty much guarantee that
you can't generate nice code.

Plus I'd rather not have the mm layer know too much about the radix
tree iterations anyway, and try to use the existing page array
functions we already have (ie "find_get_pages()").

So I'd really prefer if we can do this with tight loops over explicit
pages, rather than some loop over an iterator.
You should probably have used PFN_PHYS(), which does this correctly.
Your explicit u64 isn't exactly wrong, but phys_addr_t is really the
right type for the result.

That said, it's admittedly a disgusting name, and I wonder if we
should introduce a nicer-named "pfn_to_phys()" that matches the other
"xyz_to_abc()" functions we have (including "pfn_to_virt()")

Looking at it, the Xen people then do this disgusting thing:
"__va(PFN_PHYS(pfn))" which is both ugly and pointless (__va() isn't
going to work for a phys_addr_t anyway). And <linux/mm.h> has this
gem:

  __va(PFN_PHYS(page_to_pfn(page)));

Ugh. The ugly - it burns. that really should be
"pfn_to_virt(page_to_pfn())", I think.  Adding a few mailing lists in
the hope that some sucker^Whumanitarian person would want to take a
look.

Anyway, I pulled your change to scsi_lib.c, since it's certainly no
worse than what we used to have, but James and company cc'd too.Well, phys_addr_t had better be as big as dma_addr_t, because that's
what the resource management handles.

Yeah, that definitely sounds like it would be a good idea.
Yup.
Well, that code clearly cannot handle highmem anyway, but yes, it
really smells like xen should use page_address().

Adding Xen people who I didn't add the last time around.

Ok, I had actually just pulled your previous one, but not pushed it
out yet, so I had time to reset and fix it up.

Thanks,

           
Is this whole thread still just for the crazy and pointless
"max_sane_readahead()"?

Or is there some *real* reason we should care?

Because if it really is just for max_sane_readahead(), then for the
love of God, let us just do this

and bury this whole idiotic thread.

Seriously, if your IO subsystem needs more than 512kB of read-ahead to
get full performance, your IO subsystem is just bad, and has latencies
that are long enough that you should just replace it. There's no real
reason to bend over backwards for that, and the whole complexity and
fragility of the insane "let's try to figure out how much memory this
node has" is just not worth it. The read-ahead should be small enough
that we should never need to care, and large enough that you get
reasonable IO throughput. The above does that.

Goddammit, there's a reason the whole "Gordian knot" parable is
famous. We're making this all too damn complicated FOR NO GOOD REASON.

Just cut the rope, people. Our aim is not to generate some kind of job
security by making things as complicated as possible.
Ok, no problem. I just wanted to make sure that we're not going down
some fragile rats nest just for something silly that wasn't worth it.

                 
Good point. But it's probably overly complex to have two different
limits. The "512kB" thing was entirely random - the only real issue is
that it should be small enough that it won't be a problem on any
reasonable current machines, and big enough to get perfectly fine IO
patterns unless your IO subsystem sucks so bad that it's not even
worth worrying about.

If we just add a third requirement that it be "big enough that
reasonable uses of [fm]advice() will work well enough", then your
shared library example might well be grounds for saying "let's just do
2MB instead". That's still small enough that it won't really hurt any
modern machines.

And if it means that WILLNEED won't necessarily always read the whole
file for big files - well, we never guaranteed that to begin with.

path_openat() starts off with a get_empty_filp(), which allocates a
file pointer with GFP_KERNEL. So that should have triggered the
might_sleep warning if irq's were already disabled at that point.

So it's not before that - in particular, it's not in the signal
handling or do_coredump() paths.

Also, at least xfs_buf_lock() - which is much deeper in that chain -
does a down(&bp->b_sema). I'm disguested that that doesn't have a
might_sleep() in it.

Dave, mind adding a "might_sleep()" to the top of
"down[_interruptible]()". It's silly to not have coverage of semaphore
use in bad contexts.
That would also explain why it shows up for do_coredump(), even though
clearly interrupts are not disabled at that point. It's just because
do_coredump() opens a filename at a deeper point in the stack than the
more normal system call paths do.

It looks like just "do_signal()" has a stack frame that is about 230
bytes even under normal circumstancs (largely due to "struct ksignal"
- which in turn is largely due to the insane 128-byte padding in
siginfo_t). Add a few other frames in there, and I guess that if it
was close before, the coredump path just makes it go off.

And some of the debug options that I'm sure DaveJ has enabled tend to
make the stack usage worse (simply because they make a lot of data
structures bigger).[ Added linux-mm to the participants list ]

Ok, so DaveJ confirmed that DaveC's patch fixes his issue (damn,
people, your parents were some seriously boring people, were they not?
We've got too many Dave's around), but DaveC earlier pointed out that
pretty much any memory allocation path can end up using 3kB of stack
even without XFS being involved.

Which does bring up the question whether we should look (once more) at
the VM direct-reclaim path, and try to avoid GFP_FS/IO direct
reclaim. 

Direct reclaim historically used to be an important throttling
mechanism, and I used to not be a fan of trying to avoid direct
reclaim. But the stack depth issue really looks to be pretty bad, and
I think we've gotten better at throttling explicitly, so. 

I *think* we already limit filesystem writeback to just kswapd (in
shrink_page_list()), but DaveC posted a backtrace that goes through
do_try_to_free_pages() to shrink_slab(), and through there to the
filesystem and then IO. That looked like a disaster.

And that's because (if I read things right) shrink_page_list() limits
filesystem page writeback to kswapd, but not swap pages. Which I think
probably made more sense back in the days than it does now (I
certainly *hope* that swapping is less important today than it was,
say, ten years ago)

So I'm wondering whether we should remove that page_is_file_cache()
check from shrink_page_list()?

And then there is that whole shrink_slab() case. 

Well, a lot of it is just the callee-saved registers. The compiler
will tend to preferentially allocate registers in the callee-trashed
registers, but if the function isn't a leaf function, any registers
that are live around a function call will have to be saved somewhere -
either explicitly around the function call, or - more likely - in
callee-saved registers that then get saved in the prologue/epilogue of
the function. And this will happen even in leaf functions when there
is enough register pressure that the callee-trashed registers aren't
sufficient (which is pretty common).

So saving 5-6 registers on the stack (in addition to any actual stack
frame) is pretty much the norm for anything but the very simplest
cases.

But yeah, I'm sure some config options make it worse.
STACK_PROTECTOR_STRONG could easily be one of those.
We have better uses for random buffers in "struct task_struct", I'd
hate to put a siginfo_t there.

The thing is, siginfo_t has that idiotic 128-byte area, but it's all
"for future expansion". I think it's some damn glibc disease - we've
seen these kinds of insane paddings before.

The actual *useful* part of siginfo_t is on the order of 32 bytes. If that.

Sad.Sounds like the RightThing(tm) to do to me, and I don't see why it
wouldn't work.

We'd have to teach each user of "dequeue_signal()" to free the siginfo
thing. Which shouldn't be too bad - I think we've collected all of
that into generic code, and there isn't the mass or architecture code
that knows about these things any more. But there are a few odd
drivers etc and signalfd. I didn't look at what the lifetimes were.

Adding Oleg to the cc, since any time we touch any of that code, he
should be involved. Oleg - the issue is the biggish size of 'struct
ksignal' on stack, brought on by the silly "put a whole siginfo_t in
it".I don't think the users need to care. They'd just call
"sigqueue_free()" not knowing about our preallocations etc. That kind
of detail should be confined to inside signal.c.

But there really aren't that many users. There's a couple of
"dequeue_signal_lock()" users, but they don't actually *want* the
siginfo at all (they're kernel threads), so we can just make that
function free the siginfo immediately (and get rid of the totally
unnecessay kernel stack allocation). And outside of signal.c only
signalfd uses "dequeue_signal()" itself, and that would be the only
one that would need to be taught to use (in signalfd_copyinfo()) and
then free the sigqueue entry.

So it really looks like the right thing to do, and fairly
straightforward. But I'm leaving the coding proof to Al, since he
already offered ;)
Ugh, that's bad. A thousand bytes of stack space is much too close to
any limits.

Do you have the stack traces for these things so that we can look at
worst offenders?

If the new block-mq code is to blame, it needs to be fixed.
__virtblk_add_req() has a 300-byte stack frame, it seems. Looking
elsewhere, blkdev_issue_discard() has 350 bytes of stack frame, but is
hopefully not in any normal path - online discard is moronic, and I'm
assuming XFS doesn't do that.

There's a lot of 200+ byte stack frames in block/blk-core.s, and they
all seem to be of the type perf_trace_block_buffer() - things created
with DECLARE_EVENT_CLASS(), afaik. Why they all have 200+ bytes of
frame, I have no idea. That sounds like a potential disaster too,
although hopefully it's mostly leaf functions - but leaf functions
*deep* in the callchain. Tejun? Steven, why _do_ they end up with such
huge frames?

And if the stack use comes from the VFS layer, we can probably work on
that too. But I don't think that has really changed much lately. Adding Kay and Greg, since the original code is from commit
7ff9554bb578 ("printk: convert byte-buffer to variable-length record
buffer") and all the "prev" flag tweaks end up building on top of
that.

The whole "prev flags" is messed up, and LOG_CONT is done very confusingly.

Why are *those* particular two "prev = msg->flags" incorrect, when
every other case where we walk the messages they are required?

The code/logic makes no sense. You remove the "prev = msg->flags" at
line 1070, when the *identical* loop just above it has it. So now the
two loops count the number of characters differently. That makes no
sense.

So I don't think this fixes the fundamental problem. I'm more inclined
to believe that LOG_CONT is wrongly set somewhere, for example because
a continuation wasn't actually originally printed due to coming from
different users or something like that.

Or at the very least I want a coherent explanation why one loop would
do this and the other would not, and why counting up *different*
numbers could possibly make sense.

Because as it is, there clearly is some problem, but the patch does
not look sensible to me.

That still makes zero sense.

The size should damn well not change, because we *should* be calling
it with the exact same flags. If the size changes, something is wrong.
The logic is:

 - first traverse all log indexes to find out the total length

 - then traverse *again* all the indexes, until you have traversed
enough that the remainder fits in the given buffer size.

For this to make sense, that second pass absolutely *has* to use the
exact same lengths as the first pass did. Otherwise the whole logic is
totally broken.

Now, *once* you have found the right record (so that the rest should
fit), the problem is that the *third* loop (that traverses that "rest"
part) is now done with a "prev" value that does not match the original
"let's figure out the size".

So my suspicion is that the *real* bug is that

    prev = 0;

before that *third* loop, because it means that the first time through
that loop (when we did *not* reset "seq/idx" to the beginning, we use
the wrong "prev" value.

So my (totally and utterly untested, and obviously whitespace-damaged)
suggested patch would be something along the lines of this:

because at least this makes sense. It means that "prev" is only zero
when we start from "clear_idx", at all other times it's actually the
msg->flags of the previous message.

But maybe I'm missing something. That code really is messy. But
clearing "prev" there in the middle really looks wrong to me.

And try reading the source code. Because your statement is BS.

The third loop does *not* start again from the first line! It
*continues* from where the second loop ended. Which is exactly why
clearing "prev" is *wrong*. Because the *last* line that the second
loop processes is indeed the previous line before the *first* line
that the third loop starts processing.

No, I haven't tested my patch, and maybe it's broken for some subtle
reason I'm missing too. But neither of your patches really make sense,
although I can believe that your second patch happens to get the
buffer size right almost by accident. Why? It's by virtue of the
"prefix" for a line generally being the same length, so when you
discount the prefix of the last line that you *don't* print, you by
accident get as much room as the - extraneous - prefix of the *next*
line that then actually gets copied. See?

(Btw, just to clarify: kmsg_dump_get_buffer() has the exact same logic
and the exact same bug, so as with your patches, you should remove the
"prev = 0" from before the third loop from that function too. Because
exactly as with syslog_print_all(), the third loop *continues* where
the second loop stops, and thus clearing "prev" is actually wrong).

That extended patch attached, now without whitespace damage. But still
completely and utterly untested.

Well, it's not printing out the prefix, but it's also not printing out
the whole first part of the line, so quite frankly, I think that's
actually "more correct".

After all, it has already skipped the beginning of the line.
Prepending the prefix, then skipping part of the line, and then
printing the last part, that sounds truly insane, no?

This isn't even new behaviour - that's how it all worked back when it
was all a byte stream (except then it was *purely* about that byte
stream, so it could literally start anywhere in the line, including
partial prefixes etc).

Of course, an alternative approach - if we really want to always have
full lines - is to skip any partial lines at the beginning of the
output entirely.  So not just drop the prefix, but drop the LOG_CONT
parts too. But since (as mentioned), we've never really guaranteed
that the log starts at line boundaries, I'm not sure what the
advantage would really be. 
Hmm. What doesn't make sense is to have interrupts disabled in the
caller - since clearly math_state_restore() may actually let
interrupts in.

So while I definitely think your patch fixes a bug, I'm wondering if
we shouldn't look deeper at this issue. The comment above says

 * Must be called with kernel preemption disabled (eg with local
 * local interrupts as in the case of do_device_not_available).

which *kind* of makes sense, but at the same time only reinforces that
whole "if there are reasons why the irq has to be disabled, we can't
just randomly enable it for allocations".

So I really get the feeling that there are more bugs than just the "it
exits with irqs disabled" issue.
Ok, and this is interesting too.

kernel_fpu_{begin|end}() shouldn't mess around with tsk_used_math(), I
feel. If the task hasn't used math, no reason to allocate any
save-space, because kernel_fpu_end() will just throw it all away
anyway.

Plus we want to be able to do those things from interrupts, and:
 - we can't have blocking allocations
 - we should try to not mess with these kinds of process states from
interrupts anyway

So I think that math_state_restore() and the use_eager_fpu() changes
were actually seriously buggy here wrt that whole !tsk_used_math()
case.

I'm adding in some people here, because I think in the end this bug
was introduced by commit 304bceda6a18 ("x86, fpu: use non-lazy fpu
restore for processors supporting xsave") that introduced that
math_state_restore() in kernel_fpu_end(), but we have other commits
(like 5187b28ff08: "x86: Allow FPU to be used at interrupt time even
with eagerfpu") that seem tangential too and might be part of why it
actually *triggers* now.

Comments?Yes, I suspect that will help some, and probably fix this particular bug.

That said, regardless of the allocation issue, I do think that it's
stupid for kernel_fpu_{begin,end} to save the math state if
"used_math" was not set.  So I do think__kernel_fpu_end() as-s is
buggy and stupid. So I do think we should *either* say

 (a) "we don't want to restore at all, because once the kernel starts
using math, it might do so a lot, and saving/restoring is a bad idea":

 (b) make the use_eager_fpu() case check tsk_used_math() (in which
case we had better already have an allocation!)

Quite frankly, I'd almost lean towards (a). Comments? Does anybody
have any loads where the kernel does a lot of fpu stuff (ie network
encryption using the hw engines or something)? I'd really like to hear
if it makes a difference. Please do it, since clearly I wasn't aware enough about the whole
non-TS-checking FPU state details.

Also, since this issue doesn't seem to be a recent regression, I'm not
going to take this patch directly (even though I'm planning on doing
-rc1 in a few hours), and expect that I'll get it through the normal
channels (presumably together with the __kernel_fpu_end cleanups). Ok
with everybody?
Thinking about it some more, this patch is *almost* not needed at all.

I'm wondering if you should just change the first patch to just always
initialize the fpu when it is allocated, and at execve() time (ie in
flush_thread()).

If we do that, then this:

can be dropped entirely from math_state_restore(). And quite frankly,
at that point, I think all the changes to __kernel_fpu_end() can go
away, because at that point math_state_restore() really does the right
thing - all the allocations are gone, and all the async task state
games are gone, only the "restore state" remains.

Hmm? So the only thing needed would be to add that "init_fpu()" to the
initial bootmem allocation path and to change flush_thread() (it
currently does "drop_init_fpu()", let's just make it initialize the
FPU state using fpu_finit()), and then we could remove the whole
"used_math" bit entirely, and just say that the FPU is always
initialized.

What do you guys think?It definitely does not want an else, I think.

If tsk_used_math() is false, or if the FPU restore failed, we
*definitely* need that stts(). Otherwise we'd return to user mode with
random contents in the FP state, and let user mode muck around with
it.

No?
Uhhuh. Ok.

Why do we do that, btw? I think it would make much more sense to just
do what I *thought* we did, and just make it a context-switch-time
optimization ("let's always switch FP state"), not make it a huge
semantic difference.

                

Immaterial.

We *already* avoid twiddling TS if it's not needed.

It is true that we used to twiddle it at every context switch (and
then twiddle it *again* if we decided that we'd want to pre-load the
FPU state anyway, and avoid the extra fault).

But that was fixed, and if we switch from a task that had math state
to another task that has math state, we leave TS alone.

But Suresh apparently hits on the real issue:
which if so is sad but yes, makes CR0.TS no longer sufficient.Suresh broke it with his suggested version.

The inner if-statement is supposed to avoid the stts *if* we had used
math *and* the FPU restore worked.

But with the extra "else" that Suresh added, it now always avoids the
stts for the eager-fpu case, which breaks the whole logic for "hey, if
the process hadn't used math, we don't waste time restoring data that
doesn't exist". And, as you say, making the inner if clause pointless.

         Hmm. If there are features that (silently) depend on the FPU state
being on, then the plain "stts" doesn't work at all, because we'd be
returning to user space too (not just kernel space) with TS turned
off.

.  which *does* actually bring up something that might work, and might
be a good idea: remove the "restore math state or set TS" from the
normal kernel paths *entirely*, and move it to the "return to user
space" phase.

We could do that with the whole "task_work" thing (or perhaps just
do_notify_resume(), especially after merging the "don't necessarily
return with iret" patch I sent out earlier), with additionally making
sure that scheduling does the right thing wrt a "currently dirty math
state due to kernel use".

The advantage of that would be that we really could do a *lot* of FP
math very cheaply in the kernel, because we'd pay the overhead of
kernel_fpu_begin/end() just once (well, the "end" part would be just
setting the bit that we now have dirty state, the cost would be in the
return-to-user-space-and-restore-fp-state part).

Comments? That would be much more invasive than just changing
__kernel_fpu_end(), but would bring in possibly quite noticeable
advantages under loads that use the FP/vector resources in the kernel.
So either of my suggested changes to __kernel_fpu_end() _should_ fix
the bug, with the caveat that if we do take the "check tsk-used-math"
version, we had better verify that yes, we allocate the save storage
before we set that flag. One of the reasons I tended to prefer simpler
the "just do stts()" version is that it seemed safer. I am a bit
worried about the whole interaction with synchronous task state and
interrupts.

Suresh's notion of just always allocating the FP state save area at
process creation would fix it too.

So many ways to fix it, so little knowledge about the actual usage
patterns and performance issues. That's what the eager fpu code tries to do now (apart from process
initialization, I think). But the problem is that even disregarding
the process init issue, it saves and restores around every kernel FPU
use. What the extra flag (and task_work/do_resume_suspend) would do is
to just save/restore *once* per kernel entry.

Now, the *common* situation is certainly that the kernel doesn't do
any FPU work at all, but there are clearly exceptions to that. If
certain wireless encryption modes end up actually using hw
acceleration, we might have a *lot* of overhead from the save/restore
code. In fact, I think it triggers even for the idle task, which
doesn't really care.

Ok, I guess that means we should just make the quirk unconditional.

Ingo, do you want to do that or should I?

Not to my knowledge. But I'm sure Jakub&co would love to have a test-case.

Sadly, gcc has that really annoying habit of making small changes
create *huge* changes in label numbers etc, and that's definitely the
case with the extra empty asm - it's basically impossible to compare
the generated asm with and without the workaround, because all the
label numbers change.

I have no idea how gcc people debug things like this, when the output
is so unstable.

Jakub, any suggestions to how Steven might be able to pinpoint where
the code generation problem lies?Another week, another RC. And things look fine.

rc8 has the usual "two thirds drivers, one third random" mix, with
"random" having some arch updates (arm and parisc this time), but is
mostly network updates. A fair chunk of the driver changes are network
drivers too, for that matter.

But nothing particularly scary. The scariest part of this release is
how slow my network is, and the patch is still being uploaded, but it
should be done any minute now. Or maybe in half an hour. Whatever. But
the git trees have been updated for hours, it's just the tar-balls and
patches (and this release announcement) that were delayed by bad
networking.

Let's hope things keep calm, and that I can make the final 3.13 next
week when I get back home. But for that all to work well, you guys
need to test. Ok?

But O_TMPFILE without write access fails on modern kernels, so what
you test doesn't make sense.

In fact, that "O_TMPFILE fails with O_RDONLY" was done exactly so that
you could not have a program that works on kernels with O_TMPFILE
support _and_ opens a directory on old kernels without it. Basically
"O_TMPFILE | O_RDONLY" is an insane thing to do, and will never create
a temporary file.
Don't be silly. The size of all the object files increase *hugely*.

This is my fs/builtin.o in my normal config:

  -rw-rw-r--. 1 torvalds torvalds  2838613 Feb  3 14:32 fs/built-in.o

and with the *reduced* debug info, ie .config diff as follows:

  +CONFIG_DEBUG_INFO=y
  +CONFIG_DEBUG_INFO_REDUCED=y

I get this:

  -rw-rw-r--. 1 torvalds torvalds 11793317 Feb  3 14:33 fs/built-in.o

iow, that "reduced" debug info is still more than 4x bigger than a
non-debug-info kernel.

David, if you don't think that debug-info bloats the build, you're in
huge *huge* denial. CONFIG_DEBUG_INFO (even with the "reduced" thing)
is ABSOLUTELY HORRIBLE CRAP.

That >4x size increase doesn't match your idea of "huge bloat"?

The object files being much bigger really does screw you especially on
laptops that often have less memory and less capable IO subsystems.
The final links in particular tend to be much bigger and use much more
memory.

I suspect a lot of people are in denial about just how *horrible* the
overhead of debug builds are. And yeah, if you have oodles of memory
to cache things, it's not too bad. But you really want to have *lots*
of memory, because otherwise you won't be caching all those object
files in RAM, and your build easily becomes IO bound at link time. A
factor of four size difference will not be helping your poor slow
laptop disk. Thinking about it, we already have this. It's called "CONFIG_COMPILE_TEST".

It very much gets set by all{yes,mod}config, and it also says "we're
interested in testing the compile, not so much running the result".

So let's just make the DEBUG_INFO depend on !COMPILE_TEST.

Trivial oneliner patch (independent of the discussion about improving
the documentation), so I'll just apply it.

I can't imagine that this would be controversial:

Hm? EXPERT is useless, people set it randomly.

I'd suggest that for any statistics gathering, you just disable
COMPILE_TEST. You can do that by just editing the .config file after
doing the allyesconfig, or by pre-seeding the allyesconfig with
COMPILE_TEST disabled.

Your use-case is rather special, after all. I do *not* believe that we
should make one of the goals of "make all{yes,mod}config" be that kind
of odd specialized thing.Agreed, that should be disabled too by default. Although in that case
I think the COMPILE_TEST combination makes more sense, since it's a
coverage issue.

Suggestions for symbols?
I absolutely *detest* this patch.

Not so much because is_vmalloc_or_module_addr() should never be used,
but because the particular use in question is pure and utter garbage.

There are valid reasons to use "is_vmalloc_or_module_addr()", but
those are along the lines of drivers/char/mem.c, which says "return
-ENXIO for this crap".

And btw, that horrid crap called "kmap_to_page()" needs to die too.
When is it *ever* valid to use a kmap'ed page for IO? Here's a clue:
never.

I notice that we have a similar abortion in "get_kernel_page[s]()",
which probably has the same broken source.

We need to get *rid* of this crap, rather than add more of it. I
should have looked more closely, but it came in through Andrew, which
generally makes me go "ok, Andrew sends me a VM patch - apply". In
this case it was clearly a huge mistake.

So who the f*ck sends static module data as IO? Just stop doing that.
What's And that idiotic kmap_to_page() really needs to die too.

Those *disgusting* get_kernel_page[s]() functions came with a
commentary about "The initial user is expected to be NFS. " and that
is still the *only* user. The fact that *everybody* else has been able
to avoid that crap should tell us something.

Mel, Rik, this needs to die. I'm sorry I didn't notice how crap it was earlier.

And what's the backtrace that gets mentioned - but not quoted - for
the horrible 9p crap? So that we can see who the guilty party is that
thinks that it's somehow ok to pass module addresses off to other
kernel users. 

Please let's just fix the real problem, don't add more horridness on
top of it because somebody messed up earlier.

So copy_module_from_fd() does read into a vmalloc'ed buffer (which
isn't pretty, but at least it's not like using some statically
allocated module data), but that's a regular vmalloc() afaik.

So I don't see the need for "is_vmalloc_or_module_addr()". The regular
"is_vmalloc_addr()" should be fine, and *is* usable from modules (it's
inline, not exported, but it comes to the same thing wrt module use),
exactly because we have traditionally allowed vmalloc'ed memory to be
used.

So is there some reason why it's not just using that simpler function?

The kmap_to_page() thing is actually worse, but that's preexisting damage.So?

People shouldn't do IO to module data, so who cares if something is a
module address or not?

The thing is, even module *loading* doesn't do IO to the magic module
addresses - it loads the module data into regular vmalloc space, and
then copies it into the final location separately.

And no module should ever do any IO on random static data (and
certainly not on code).

So there is _zero_ reason for a driver or a filesystem to use
is_vmalloc_or_module_addr(). It's just not a valid question to ask.

If somebody uses module data/code addresses, we're *better* off with a
oops or other nasty behavior than to try to make it "work".

Yes, it is terrible. Don't do it. It shouldn't work.

Now, whether it is "obvious" or not is immaterial. It might be hard to
notice, but let's face it, we are kernel programmers. Our interfaces
aren't designed for convenience, they are for working well and
efficiently.  And if that happens to mean that you shouldn't do IO on
kmap'ed pages, or use random static data in your modules, that is what
it means.

We have lots of other rules too. People should deal with it, and
realize that in the kernel we absolutely *have* to try to minimize the
problem space as much as possible.

The usual computer sciencey approach of "make things as generic as
possible so that you can reuse the code" stuff is generally bullshit
even in other contexts (example: STL), but it's _particularly_ bad for
the kernel. It's much better to have strict rules about what is
acceptable.It's been pretty quiet, actually, which should make me happy. But I
have a suspicious nature, and I'm going to wait to see if the other
shoe drops, and people are just lulling me into a false sense of
security. Because I know kernel developers, and they are sneaky.  I
suspect Davem (to pick somebody not at random) is giggling to himself,
waiting for this release message, planning to send me some big-ass
pull request tomorrow.

Because that's the kind of people you guys are.

Anyway, what little there was looks normal: roughly two thirds drivers
(gpu, block, media, misc), with almost half the remaining patches
being architecture updates (x86, s390 and arm64). With the rest being
filesystems (vfs, nfs, ocfs, btrfs and some kernfs fixes), some mm
noise, and tooling (perf).

Shortlog appended, which doesn't always happen for rc2.

I suspect we could just drop the addresses entirely if we have a
symbolic version. It's not like the hex addresses are all that
interesting, and in commit messages I actually end up editing them out
just because they are worthless noise.

So we could just do "schedule+0x45/0x368" without the actual hex
address in brackets. 

Why the heck wouldn't you do that? Just do

   list schedule+0x45

instead.

Why would you have a symbol-less vmlinux? The only reason to strip
vmlinux is because you were crazy enough to build with
CONFIG_DEBUG_INFO and the damn debug info is so large that it won't
fit on your root partition. But dammit, if you build with debug_info
and then strip the end result, you're just insane. You made your build
take ten times longer, use ten times more diskspace, and then you
throw it all away. Crazy.

So I don't think the symbol-less version is worth even worrying about.
You do want to build with KALLSYMS (or whatever the config option is
called), so that the symbolic name is worth something, but once you
have the symbolc name, you're good unless you did something terminally
stupid.

Btw, we should make it harder to enable CONFIG_DEBUG_INFO. It's a
f*cking pain. It's particularly nasty when you do "make allmodconfig"
and it enables debug-info and makes the build take forever and waste
diskspace - but nobody sane actually *boots* the end result, so that
debug info is all pointless.
Ok, but so what?

As mentioned, nobody sane should build with DEBUG_INFO. But a normal
vmlinux file has the symbol information even without it.That seems to be just a gdb bug (or "UI feature"), in that gdb likes
to give misleading error messages and requires odd syntax for some
things.

The symbols are there (see the first line):

  Reading symbols from /home/mingo/tip/vmlinux. (no debugging symbols
found). done.

and that "no debugging symbols found" is just because you don't have
the extra *debug* info.

The "list" command requires debug info to work (since that's where the
line number information is), and will not work with hex symbols either
if you don't have that. So when it says "No symbol table is loaded" it
really means "no debug information is loaded".

But you can see that the symbol is perfectly fine:

So my point is that the hex address doesn't give you *anything* that
the symbolic address doesn't give you. Unless you do truly crazy
things like actively strip the kernel.

In the kaslr case, the hex values cannot possibly help, since they are
meaningless due to the random offset (the external tools will *not* be
able to use them without relocation information anyway, and if the
tools can take relocation into account, they might as well just take
the symbol name into account anyway).

I'd really suggest dropping the damn thing entirely. Even if you do
use "objdump" (and I admit to doing that quite often myself), the
symbol version isn't really all that hard to use. And the thing is,
once again, the symbol version works even for kaslr, while the raw hex
does not.

I do agree that having to do the arithmetic can be annoying, but it
wouldn't actually be all that hard to write a script that turns the
"objdump" format from hex address to "symbol+offset" format. It would
probably be three lines of perl:

 - parse the "hex: rest-of-line" thing
 - if "rest-of-line" is of the format "<symbol>:", then remember the
hex and the symbol name.
 - print out "symbol+offset: rest-of-line" where "offset" is "hex -
remembered_hex"

Even I could probably do it, and my perl knowledge is really limited
to "I can edit other peoples perl code if I'm lucky". Somebody who
actually does perl probably goes "Three lines? That's a oneliner".

Probably. And then we should make sure that allyesconfig/allmodconfig
don't pick it.

There *are* reasonable uses for DEBUG_INFO:

 - if you very actively and extensively use kgdb, DEBUG_INFO is very useful.

 - distros might want to build distro kernels with DEBUG_INFO for the
kernel debug package

but for most kernel developers, DEBUG_INFO really just bloats things
and makes the build much *much* slower, for very little gain. Sure,
you get line numbers, but let's face it, it's not that hard to just do
"x/20i <address>" and trying to match it up with the generated code
instead. And since "just match it up" model works with user-reported
oopses of random kernels, you had better be able and willing to do
that *anyway*.

If most of the oopses you decode are on your own machine with your own
kernel, you might want to try to learn to be more careful when writing
code. And I'm not even kidding.
I don't have a debug build, so maybe it's something specific to gdb
actually seeing type information and then being confused. 

Can you do "x/10i schedule+0x45" because that definitely works for me.
But I literally detest DBUG_INFO builds, because it's useless crap.
99% of everything I debug is from people reporting problems on their
kernels, so it's not like my local debug info would match that anyway.

Anyway, if it's a type information thing due to DEBUG_INFO that
confuses gdb and makes it think that the "+0x45" part doesn't make
sense, you may need to add a cast to get gdb to ignore the type
information. IOW, does

work for you?

I happen to have gdb-7.6.50 here that I tested with, but I've used
"symbol+offset" forever afaik, so it's definitely not something new.
Ok, so it's just list that is braindamaged. Maybe it wants a
*(schedule+0x45) or something. I dunno. You can obviously get the hex
number from just doing "p schedule+0x45" and then do that.

Whatever. I still claim that the hex numbers in the oopses are
worthless noise. The fact that gdb has "UI issues" is an unrelated
issue and perhaps not surprising.

                
So I pulled this, but one question:

How fundamental is that "!HIBERNATION" issue? Right now that
anti-dependency on hibernation support will mean that no distro kernel
will actually use the kernel address space randomization. Which
long-term is a problem.

I'm not sure HIBERNATION is really getting all that much use, but I
suspect distros would still want to support it.

Is it just a temporary "I wasn't able to make it work, need to get
some PM people involved", or is it something really fundamental?
.  oh, and since I decided to test it, and was looking for problems:
enabling kaslr breaks "perf". The *profile* looks fine, but the
disassembly doesn't work.

I'm not entirely surprised. I decided I wanted to test it for a
reason, after all. So it's not unexpected, but perhaps people hadn't
thought about it, and clearly hadn't tested it.

Kernel modules disassemble fine, so clearly perf knows about code that
moves around, but apparently it gets surprised when the core vmlinux
file disassembly doesn't match addresses.
Grr. You missed the branch name.

I can see from the SHA1 (and historical pull requests) that you meant
the usual 'v4l_for_linus' branch, but please be more careful.

             
The x86-64 (and 32-bit, for that matter) system call slowpaths are all
in C, but the *selection* of which slow-path to take is a mixture of
complicated assembler ("sysret_check -> sysret_careful ->
sysret_signal ->sysret_audit -> int_check_syscall_exit_work" etc), and
oddly named and placed C code ("schedule_user" vs
"__audit_syscall_exit" vs "do_notify_resume").

This attached patch tries to take the "do_notify_resume()" approach,
and renaming it to something sane ("syscall_exit_slowpath") and call
out to *all* the different slow cases from that one place, instead of
having some cases hardcoded in asm, and some in C. And instead of
hardcoding which cases result in a "iretq" and which cases result in a
faster sysret case, it's now simply a return value from that
syscall_exit_slowpath() function, so it's very natural and easy to say
"taking a signal will force us to do the slow iretq case, but we can
do the task exit work and still do the sysret".

I've marked this as an RFC, because I didn't bother trying to clean up
the 32-bit code similarly (no test-cases, and trust me, if you get
this wrong, it will fail spectacularly but in very subtle and
hard-to-debug ways), and I also didn't bother with the slow cases in
the "iretq" path, so that path still has the odd asm cases and calls
the old (now legacy) do_notify_resume() path.

But this is actually tested, and seems to work (including limited
testing with strace, gdb etc), and while it adds a few more lines than
it removes, the removed lines are mostly asm, and added lines are C
(and part of them are the temporary still-extant do_notify_resume()
wrapper). In particular, it should be fairly straightforward to take
this as a starting point, removing the extant
do_notify_resume/schedule/etc cases one by one, and get rid of more
asm code and finally the wrapper.

Comments? This was obviously brought on by my frustration with the
currently nasty do_notify_resume() always returning to iret for the
task_work case, and PeterZ's patch that fixed that, but made the asm
mess even *worse*.

Actually, I should have taken a closer look.

Yes, do_notify_resume() is a real issue, and my stupid open/close
test-case showed that part of the profile.

But the "iretq" that dominates on the kernel build is actually the
page fault one.

I noticed this when I compared "-e cycles:pp" with "-e cycles:p". The
single-p version shows largely the same profile for the kernel, except
that instead of showing "iretq" as the big cost, it shows the first
instruction in "page_fault".

In fact, even when *not* zoomed into the kernel DSO, "page_fault"
actually takes 5% of CPU time according to pref report. That's really
quite impressive.

I suspect the Haswell architecture has made everything else cheaper,
and the exception overhead hasn't kept up. I'm wondering if there is
anything we could do to speed this up - like doing gang lookup in the
page cache and pre-populating the page tables opportunistically.

We're using an interrupt gate for the page fault handling, and I don't
think we can avoid that. For all I know, a trap gate might be slightly
faster (but likely not really noticeable - the microcode is surely
expensive, but the pipeline unwinding is probably the biggest cost of
the page fault), but we have the issue of interrupts causing page
faults for vmalloc pages.  And obviously we can't avoid the iretq for
the return path.

So as far as I can see, there's no sane way to make the page fault
itself cheaper. Looking at opportunistically prepopulating page tables
when it's cheap and easy might be the best we can do. THP isn't really realistic for any general-purpose loads. 2MB pages
are just too big of a granularity.

On a machine with just a few gigs (ie anything but big servers), you
count the number of large pages in a few thousands. That makes
fragmentation a big issue. Also, ELF sections aren't actually
2MB-aligned, nor do you really want to throw away 9 bits of virtual
address space randomization. Plus common binaries aren't even big
enough. Look at "bash", which is a pig - it's still less than a
megabyte as just the binary, with the code segment being 900kB or so
for me. For something like "perl", it's even smaller, with a number of
shared object files that get loaded dynamically etc.

IOW, THP is completely useless for any kind of scripting. It can be
good for individual large binaries that have long lifetimes (DB
processes, HPC, stuff like that), but no, it is *not* the answer to
complex loads. And almost certainly never will be.

It's possible that some special case (glibc for example) could be
reasonably hacked to use a THP code segment, but that sounds unlikely.
And the virtual address randomization really gets hurt.

No, I was thinking "try to optimistically map 8 adjacent aligned pages
at a time" - that would be the same cacheline in the page tables, so
it would be fairly cheap if we couple it with a gang-lookup of the
pages in the page cache (or, for anonymous pages, by just
optimistically trying to do an order-3 page allocation, and if that
works, just map the 32kB allocation you got as eight individual
pages).

I know it's been discussed at some point, and I even have a dim memory
of having seen some really ugly patches.
Doing the gang-lookup is hard, since it's all abstracted away, but the
attached patch kind of tries to do what I described.

This patch probably doesn't work, but something *like* this might be
worth playing with.

Except I suspect the page-backed faults are actually the minority,
judging by how high clear_page_c_e is in the profile it's probably
mostly anonymous memory. I have no idea why I started with the (more
complex) case of file-backed prefaulting. Oh well.

Interesting. Here are some pte fault statistics with and without the patch.

I added a few new count_vm_event() counters: PTEFAULT, PTEFILE,
PTEANON, PTEWP, PTESPECULATIVE for the handle_pte_fault,
do_linear_fault, do_anonymous_page, do_wp_page and the "let's
speculatively fill the page tables" case.

This is what the statistics look like for me doing a "make -j" of a
fully built almodconfig build:

where obviously the ptespeculative count is zero, and I was wrong
about anon faults being most common - the file mapping faults really
are the most common for this load (it's fairly fork/exec heavy, I
guess).

This is what happens with that patch I posted:

about 200k page faults went away, and the numbers make sense (ie they
got removed from the ptefile column - the other number changes are
just noise).

Now, we filled 600k extra page table entries to do that (that
ptespeculative number), so the "hitrate" for the speculative filling
was basically about 33%. Which doesn't sound crazy - the code
basically populates the 8 aligned pages around the faulting page.

Now, because I didn't make this easily dynamically configurable I have
no good way to really test timing, but the numbers says at least the
concept works.

Whether the reduced number of page faults and presumably better
locality for the speculative prefilling makes up for the fact that 66%
of the prefilled entries never get used is very debatable. But I think
it's a somewhat interesting experiment, and the patch was certainly
not hugely complicated.

I should add a switch to turn this on/off and then do many builds in
sequence to get some kind of idea of whether it actually changes
performance. But if 5% of the overall time was literally spent on the
*exception* part of the page fault (ie not counting all the work we do
in the kernel), I think it's worth looking at this.
That's what my patch largely does. Except I screwed up and didn't use
FAULT_FLAG_ALLOW_RETRY in fault_around().

Anyway, my patch kind of works, but I'm starting to hate it. I think I
want to try to extend the "->fault()" interface to allow
filemap_fault() to just fill in multiple pages.

We alread have that "vmf->page" thing, we could make it a small array
easily. That would allow proper gang lookup, and much more efficient
"fill in multiple entries in one go" in mm/memory.c.

It does nothing, afaik.
Probably.

.  and on the other hand, we should actually be able to use 'sysret'
for signal handling on x86-64, because while sysret destroys %rcx and
doesn't allow for returning to odd modes, for calling a signal handler
I don't think we really care. 

Yes, the 32-bit code I didn't want to touch, partly because I no
longer have a test-case. And it does end up having some more
interesting cases.
For signal *delivery*, CS will always be __USER_CS, and %rcx can be
crap, so sysret should be fine. We could easily check that %rip is
valid in the whole slow-path instead of saying "return 1 if we did
do_signal()".

Now, it's a different thing wrt signal handler *return*, because at
that point we really cannot return with some random value in %rcx. We
absolutely do need to use 'iretq' in that whole [rt_]sigreturn() path,
but on x86-64 that is all handled by the system call itself (see the
stub_*_sigreturn stuff in entry_64.S) and it very much uses iret
explicitly (the 32-bit case also does that, by forcing the sigreturn
to be done with an "int 0x80" instruction - we could change that to
use syscall+iret, but frankly, I'm not all that inclined to care,
although it might be worth trying to do just to unify the models a
bit).Fair enough. But it would still be really easy, and make the common
case signal delivery a bit faster.

Now, sadly, most signal delivery is then followed by sigreturn (the
exceptions being dying or doing a longjmp), so we'd still get the
iretq then. But it would cut the iretq's related to signals in half.

We *could* try to do sigreturn with sysret and a small trampoline too,
of course. But I'm not sure how far I'd want to take it.I have no idea why you keep re-sending this.

It got merged a week ago. See commit f1499382f114.
It is my merge (Christoph's ACL changes came in today through the VFS
tree from Al).

I was doing the merges today on my laptop (I had jury duty yesterday
and today), and so I didn't do the allmodconfig build I would normally
do on my (much faster) desktop. Well, actually I did do the full fs
builds for the earlier pulls that actually had some conflicts, but not
for the ceph pull. The conflict was hidden by the fact that the whole
cifs ACL support is new, so there was no data conflict, just a silent
semantic conflict between the new smarter ACL helpers and the new ACL
use in CIFS.

I'm back home now (yay, all the afternoon cases got settled), and I
see the problem now. I should have done an allmodconfig build
immediately after coming home, but I never even thought of it.

Anyway, here's an *untested* conversion to the new posix acl helper
infrastructure. I do wonder if ceph_init_acl() could be converted to
posix_acl_create(), right now that part is a "non-conversion" - it's
just made to use __posix_acl_create() that implements the old
interface.

Al, Christoph, can you please check my conversion for sanity from a
generic posix-acl standpoint?

Sage, Guangliang, Li, can you check the actual cifs usage/sanity of
the attached patch?

Sorry about the messed-up merge. Although I also blame Al, because
he's horrible about having his changes in linux-next, so nobody was
ever really aware of this semantic conflict.

Al. Bad, bad boy. Consider yourself hit with a rolled-up newspaper.

Ok, I already committed my untested (and broken) patch yesterday,
because even a broken tree is better than a non-*compiling* tree for
testing.

I created a diff from my current tree to this, and will happily apply
it, but the sign-off chain is now broken (Ilya didn't sign off on his
changes to Sage's patch. Not that it really matters for what is really
a one-liner change, but I thought I'd mention it, since another issue
came up with this patch. 

Ceph now does

in its ceph_set_acl() function, and that's because the VFS layer
doesn't pass down the dentry to the acl code any more.

Christoph - that sounds like a misfeature in the new interfaces. I
realize that for traditional unix filesystems, the path is irrelevant
for an inode operation, but the thing is, from a Linux VFS standpoint
and a conceptual standpoint, the dentry really is the more important
and unique one, and some filesystems want the dentry because they are
*correctly* designed to be about pathnames.

Network filesystems that are pass file descriptors around (ie NFS etc)
are not the "RightWay(tm)" of doing things, so I think that it is
quite reasonable for ceph to want the *path* to the inode and not just
the inode.

So attached is the incremental diff of the patch by Sage and Ilya, and
I'll apply it (delayed a bit to see if I can get the sign-off from
Ilya), but I also think we should fix the (non-cached) ACL functions
that call down to the filesystem layer to also get the dentry.

We already do that for the xattr functions, just not for
get_acl()/set_acl()/acl_chmod().

Christoph, Al? Yes, most filesystems will ignore the dentry pointer, but still. 

Yeah, that's pretty annoying, largely because that path is also
RCU-walk aware, which does *not* need this all (because it will never
call down into the filesystem - if the acl isn't found in the cached
acl's, we just abort).

And we're going through that very common "generic_permission()" thing
that in turn is also often called from the low-level filesystens, and
it's all fairly tightly integrated with __inode_permission() etc.

In the end, all the original call-sites should have a dentry, and none
of this is "fundamental". But you're right, it looks like an absolute
nightmare to add the dentry pointer through the whole chain. Damn.

So I'm not thrilled about it, but maybe that "d_find_alias(inode)" to
find the dentry is good enough in practice. It feels very much
incorrect (it could find a dentry with a path that you cannot actually
access on the server, and result in user-visible errors), but I
definitely see your argument. It may just not be worth the pain for
this odd ceph case.

That said, if the ceph people decide to try to bite the bullet and do
the required conversions to pass the dentry to the permissions
functions, I think I'd take it unless it ends up being *too* horribly

Hmm. I spent some time actually seeing how painful it would be, and I
think we can do it.

The attached patch seems to work for me - it does *not* make any
actual semantic changes, but it pushes the dentry pointer into the
filesystems for the permission checking, and the
vfs_{create,mkdir,mknot,symlink,link,rmdir,unlink,rename} helper
functions.

NOTE! It does *not* actually push them into the ACL functions, and in
fact it doesn't even push it down to "generic_permission()" yet. But
it would be a prerequisite for doing so.

And interestingly, replacing the inode pointer with a dentry pointer
ended up actually simplifying several of the call-sites, so while
doing this patch I got the feeling that this was the better interface
anyway, and we should have done this long ago.

Now, to be honest, pushing it down one more level (to
generic_permission()) will actually start causing some trouble. In
particular, gfs2_permission() fundamentally does not have a dentry for
several of the callers.

But aside from being pretty large, this patch looks pretty good to me.
And as I mentioned, I suspect it would actually be the right thing to
do even if we don't go any futher. And modulo gfs2, it would make it
trivial to push down the dentry pointer to generic_permission and then
to the acl lookup code.

What do you think? I guess this patch could be split up into two: one
that does the "vfs_xyz()" helper functions, and another that does the
inode_permission() change. I tied them together mainly because I
started with the inode_permission() change, and that required the
vfs_xyz() change.

No, Al. It's *not* just a function of the inode alone. That's the point.

Some network filesystems pass the *path* to the server. Any operation
that needs to check something on the server needs the *dentry*, not
the inode.

This whole "the inode describes the file" mentality comes from old
broken UNIX semantics. It's fundamentally true for local Unix
filesystems, but that's it. It's not true in general.

Sure, many network filesystems then emulate the local Unix filesystem
behavior, so in practice, you get the unix semantics quite often. But
it really is wrong.

If the protocol is path-based (and it happens, and it's actually the
*correct* thing to do for a network filesystem, rather than the
idiotic "file handle" crap that tries to emulate the unix inode
semantics in the protocol), then the inode is simply not sufficient.

And no, d_find_alias() is not correct or sufficient either. It can
work in practice (and probably does perfectly fine 99.9% of the time),
but it can equally well give the *wrong* dentry: yes, the dentry it
returns would have been a valid dentry for somebody at some time, but
it might be a stale dentry *now*, and it might be the wrong dentry for
the current user (because the current user may not have permissions to
that particular path, even if the user has permissions through his
*own* path).

So I really think you're *fundamentally* incorrect when you say
"result *is* a function of inode alone".No. You're wrong.

EVEN ON A UNIX FILESYSTEM THE PATH IS MEANINGFUL.

Do this: create a hardlink in two different directories. Make the
*directory* permissions for one of the directories be something you
cannot traverse. Now try to check the permissions of the *same* inode
through those two paths. Notice how you get *different* results.

Really.

Now, imagine that you are doing the same thing over a network. On the
server, there may be a single inode for the file, but when the client
gives the server a pathname, the two pathnames to that single inode
ARE NOT EQUIVALENT.

And the fact is, filesystems with hardlinks and path-name-based
operations do exist. cifs with the unix extensions is one of them.

Al, face it, you're wrong this time.

Hmm? I'm pretty sure cifs can actually have hardlinks.

             Heh. It's no less redundant than it used to be.

But if you want to clean up the vfs_xyzzy() ones further, I'm
perfectly fine with that.
Minimal patch.  I really didn't want to check what the heck lustre
does with the insane ll_vfs thing.

Note that *not* passing in inode would make the patch much bigger,
because now every filesystem would have to add the

at the top.

Also, I'm not actually convinced it is redundant at all. Remember the
RCU lookup case? dentry->d_inode is not safe.

The RCU case actually does

and here the difference between nd->inode and dentry->d_inode are
relevant, I think.

Not true. Look closer.

Look at gfs2_lookupi() in particular, and check how it is called.

I did hate that part of the patch, and I did mention the kinds of
problems this will cause if the next phase passes in dentry to
"generic_permission()".
"generic_permission()" is just a helper that implements the default
UNIX permissions, and won't necessarily even be called. A filesystem
could decide not to call it at all, and in fact there are cases that
don't (eg coda or the bad_inode case).

The inode_permission() class of helpers, in contrast, is what gets
called by the VFS layer itself. So if you want to catch all permission
checks (and that would be security_inode_permission()) then you need
to catch it there.
I agree.

The good news, though, is that in the RCU lookup case, we have that
MAY_NOT_BLOCK thing, and most filesystems will have errored out for
any complex operations.

RCU lookup is special, and complicated, and sadly, the permissions
checking is very much part of that. But for the really complex cases,
at least we can punt.
I don't know. And I suspect that for things like the journal index
file lookup (which is actually worse - see gfs2_jindex_hold()) we
don't really about the permissions, since this is just done at
init_journal() time.

So I think all of this is quite solvable for gfs2, it just wasn't the
obvious kind of "we already have the dentry" case that every single
other case I looked at was.

Ok, this is the split-up. I haven't completed the "make allmodconfig"
for the first patch yet, so I might have split this wrong, but it was
fairly well separated and I'm pretty sure that this is fine.

I do actually agree that the second patch isn't exactly pretty.
Passing in both dentry and inode is redundant, and calling the
function "inode_permission()" is now a misnomer. But it makes it a lot
easier to see the differences this way in the diff (particularly
word-diff), so I think a renaming and/or dropping the inode parameter
would better be done as a separate patch anyway.

And no, as far as the first patch is concerned, I certainly wouldn't
hate dropping the first 'parent' argument from the vfs_xyzzy()
functions either, since that *should* be redundant, but quite frankly,
I didn't want to think too much about that, and this was the simple
conversions (and all callers were quite happy with dropping the inode
and replacing it with the dentry).

Presumably octeon doesn't do speculative writes, only *buffered* writes.

So writes move down, not up.

But it looks like Cavium is one of those clown companies that have a
"contact us" button for technical documentation rather than actually
making it available.

Christ, why would anybody do business with a tech company that hides
technical details? Seriously, that just stinks of "we have so many
bugs that we cannot make the documentation available".Unlikely. The thing is, in order for the sc to succeed, it has to
already have hit the cache coherency domain (or at least reserved it -
ie maybe the value is not actually *in* the cache, but the sc needs to
have gotten exclusive access to the cacheline).

So just how do you expect a later store (that is *after* the
conditional branch that tests the result of the sc) to move up before
it?

I'm not saying it's physically impossible: speculation is always
possible. But it would require some rather clever speculative store
buffers or caches and killing of same when mispredicted. Which is
actually fairly unlikely, since stores are seldom - if ever - in the
critical path. IOW, "lots and lots of effort for very little gain".

So I'm personally quite willing to believe that a
sc+conditional-branch+st is quite well ordered without any extra
barriers. I'd be more worried about *reads* moving up past the sc
("doesn't reorder reads" to me would imply not moving across the "ll"
part, but it's quite likely that the "sc" actually counts as both a
read and a write).

Without any visible documentation (see aforementioned "clown company"
comment) all of this is obviously just pure speculation.
Oh, please, that's a British-level understatement. It's like calling
WWII "a small bother".

That's too ugly to live.
Do we actually have to use "filename" at all?

We do have bprm->file, and we could get a path from that. It would be
more expensive, but for tracing execve that might be fine. Yes/no?

Or maybe we could just push the "putname(path)" into free_bprm() and
remove it from the callers. That's where we free bprm->interp anyway,
so it would kind of match.

Ok, that's better, but I really think we should just use "getname()"
and "putname()".

That's what the path that *matters* already does (ie the normal
execve() system call), so let's just make all the random cases do the
same thing.

That requires a "getname_kernel()" to create a "struct filename *"
from a kernel string, but hey, that's simple enough.

NOTE! This means that "bprm->filename" is no longer a string: it's a
"struct filename *", so if you want the string, you do
"filename->name".

This actually cleans the normal paths up - look how "open_exec()" used
to create a dummy

on the stack because do_filp_open() wants a 'struct filename' pointer.
I leave that for the external callers (that use it for the
interpreter), but for the main path that actually just goes away,
because now we have that "struct filename *" natively.

It does add code to the special kernel-execve paths, but moving the
"handle errors from getname()" code into do_execve(), even that is
really trivial, eg:

NOTE NOTE NOTE. This is untested, but it looks fine. If I missed
something, the compiler should warn about bad types. I didn't my the
bprm_flat changes, for example, because that's a no-MMU-only file. And
I might have missed something that didn't match my grep patterns, for
example.

How does this look?

Freudian slip or intentional? :-)

Right you are. I was actually aware of that, but grepping for things
it all looked fine. But I got confused by all the insane audit
wrappers, and you're right, it needs some massaging for audit
handling.

And that audit code really is aushit. I think I found a bug in it
while just scanning it: if audit_alloc_name() fails, the filename will
never be added to the audit lists, and name_count will never be
incremented. But then when we call audit_putname it won't actually put
the name, so it all just leaks - and if you have AUDIT_DEBUG enabled
you'd eventually see an error.

I wonder if we could get rid of some of that crap, and make the audit
code use dentry_path() instead of trying to save off pathnames like
that. But I don't know what the audit code actually *uses* the
pathnames for, so what do I know.

Eric? Can you please explain?

Also, here's a slightly updated patch. The change is that:
 - getname_kernel() will now clear 'filename->aname'
 - cleared 'aname' for regular getname too before calling
audit_getname(), so that if that one fails, it will be NULL.
 - audit_putname() will consider a NULL aname to be the same as not
being in audit context, and just do a final_putname() on it.

That should fix the audit filename leak too, afaik.

Eric, please take a look. As well as explain the audit name thing if possible.

Right you are.

I started out with free_bprm() being non-static, and thought I had to
handle other callers. Which is why I made the bprm->filename be the
"struct filename" so that brpm_free() could free it.

And then I only later noticed that free_bprm() was really only used by
fs/exec.c, and turned it static - and you're right, if we always just
free it in do_execve_common(), that simplifies the patch a lot.

Good call. New patch attached. More comments?

Quite frankly, merging drivers can be a f*cking pain. I would
seriously suggest avoiding it *unless* the hardware is literally
almost identical, or the drivers have been written with multi-chip
support from the ground up by people who actually understood the
hardware.

But the reason isn't performance - it's subtle breakage. Merged
drivers have a tendency to break support for chip A when you fix
something for chip B. And the end result is a driver that nobody
understands, and nobody can sanely test.

Sometimes it's simply better to leave old drivers alone.

If you've already merged them, and you think it works, keep it that way.

I was just saying that merging drivers isn't necessarily always the
right thing to do, and can be painful as hell.

                  
You picked the wrong thing to be right. The SDM is wrong.

Last time this came up, Tony explained it thus:and since ia64 is basically on life support as an architecture, we can
pretty much agree that the SDM is dead, and the only thing that
matters is implementation.

The above quote was strictly in the context of just cmpxchg, though,
so it's possible that the "fetchadd" instruction acts differently. I
would personally expect it to have the same issues, but let's see what
Tony says.  Tony?I have nothing against this series of patches, but appreciate it going
through the kconfig maintainer, since we have one.

Michal added to the participants. Dave, I don't think the patches
themselves were cc'd to Michal either, but I didn't check.

Probably. I don't have a Xen PV setup to test with (and very little
interest in setting one up).  And I have a suspicion that it might not
be so much about Xen PV, as perhaps about the kind of hardware.

I suspect the issue has something to do with the magic _PAGE_NUMA
tie-in with _PAGE_PRESENT. And then mprotect(PROT_NONE) ends up
removing the _PAGE_PRESENT bit, and now the crazy numa code is
confused.

The whole _PAGE_NUMA thing is a f*cking horrible hack, and shares the
bit with _PAGE_PROTNONE, which is why it then has that tie-in to
_PAGE_PRESENT.

Adding Andrea to the Cc, because he's the author of that horridness.
Putting Steven's test-case here as an attachement for Andrea, maybe
that makes him go "Ahh, yes, silly case".

Also added Kirill, because he was involved the last _PAGE_NUMA debacle.

Andrea, you can find the thread on lkml, but it boils down to commit
1667918b6483 (backported to 3.12.7 as 3d792d616ba4) breaking the
attached test-case (but apparently only under Xen PV). There it
apparently causes a "BUG: Bad page map . " error.

And I suspect this is another of those "this bug is only visible on
real numa machines, because _PAGE_NUMA isn't actually ever set
otherwise". That has pretty much guaranteed that it gets basically
zero testing, which is not a great idea when coupled with that subtle
sharing of the _PAGE_PROTNONE bit. 

It may be that the whole "Xen PV" thing is a red herring, and that
Steven only sees it on that one machine because the one he runs as a
PV guest under is a real NUMA machine, and all the other machines he
has tried it on haven't been numa. So it *may* be that that "only
under Xen PV" is a red herring. But that's just a possible guess.

Christ, how I hate that _PAGE_NUMA bit. Andrea: the fact that it gets
no testing on any normal machines is a major problem. If it was simple
and straightforward and the code was "obviously correct", it wouldn't
be such a problem, but the _PAGE_NUMA code definitely does not fall
under that "simple and obviously correct" heading.

Guys, any ideas?

If that is indeed the only difference, then we should damn well get
rid of that f*cking stupid _PAGE_NUMA name entirely.

It's misleading crap. Really. Just do a quick grep for that bit, and
you see just *how* confused people are about it:

think about it. Just *THINK* about how broken that code is. The whole
thing is a disaster. _PAGE_NUMA must die. It's shit.Hey, it was a normal two-week merge window, and it's closed now. As
far as I'm aware, I've pulled everything asked from me (with one
exception, see later) and applied all patches I was meant to apply. If
you feel I overlooked your work, it might be due to an email being
caught as spam (it happened several times lately, but I think I caught
them all) or just plain incompetence on my part (that happens too).

I realize that as a number, 3.14 looks familiar to people, and I had
naming requests related to that. But that's simply not how the
nonsense kernel names work. You can console yourself with the fact
that the name doesn't actually show up anywhere, and nobody really
cares. So any pi-related name you make up will be *quite* as relevant
as the one in the main Makefile, so don't get depressed.

Besides, any self-respecting geek will know pi to twenty decimal
places from their dorky youth, so 3.14 isn't really *that* close, is
it?

Anyway, the one missing pull request is the rename2 system call that I
felt I needed to look at myself, and was also hoping for more
commentary on (I'm looking at you, Al). So I didn't feel I had the
mental throughput to look at it during the merge window, and will take
a more leisurely look the upcoming week. I *might* still pull it
before -rc2, but quite frankly it's more likely to be left pending for
3.15.

Other than that? The stuff that actually got merged? It was a pretty
normal merge window, nothing stands out. The statistics are the normal
two-thirds drivers, with the rest being a mix of architecture updates
(ARM dominates, but there's powerpc, x86, mips. s390, and even ia64
shows up with stale xen code removal too) and misc. Where misc is core
kernel, mm, networking, tooling etc etc.

I'm attaching my merge-log, since (as usual) the actual merge window
shortlog is much too long to bother with for general consumption. And
again, note that my merge-log is about who was the submaintainer that
sent me the pull request, not about who necessarily wrote the code.
You need to go to the full git logs to see that level of detail.

Ack. Make it so. I assume you've tested this on x32 (and hopefully
x86-32 on a 64-bit kernel), and that I will get this through the x86
git trees?

I don't think that works. That completely breaks randomize_stack_top().

So I'm not going to pull the parisc tree, this needs to be resolved sanely.

In fact, I think that change to fs/exec.c is just completely broken:

and that "+1" just doesn't make sense, and fundamentally breaks STACK_RND_MASK.

It also seems to be entirely pointless, since the PAGE_ALIGN() that
comes right afterwards will effectively do it anyway.

So NAK on that whole fs/exec.c change. Afaik it's just wrong, and it's stupid.
It causes an interesting warning for me:

and gcc is entirely correct: that loop iterates from 0 to 3, and does this:

but the bb_swing_idx_ofdm[] array only has two members. So the last
two iterations will overwrite bb_swing_idx_ofdm_current and the first
entry in bb_swing_idx_ofdm_base[].

Now, the bug does seem to be benign: bb_swing_idx_ofdm_current isn't
actually ever *used* as far as I can tell, and the first entry of
bb_swing_idx_ofdm_base[] will have been written with that same
"rtldm->default_ofdm_index" value.

But gcc is absolutely correct, and that driver needs fixing.

I've pulled it and will let it be because it doesn't seem to be an
issue in practice, but please fix it. The obvious fix would seem to
change the size of "2" to be "MAX_RF_PATH", but I'll abstain from
doing those kinds of changes in the merge when it doesn't seem to
affect the build or functionality).Why not just remove it entirely, and change all users to
compat_[get|set]_timespec (same for timeval etc, of course).

After all, compat_*_time*() does fall back cleanly for non-x32 cases.
And sure, maybe that particular code is never *needed* for x32
support, but the overhead is generally zero (since in most cases X32
isn't even configured), or very low anyway. So the upside of having
two subtly incompatible interfaces is very dubious, no?

Looks sane to me from a superficial quick read-through.

             

I'm taking this directly, since I'll cut rc1 tomorrow (or maybe later today).Actually, I got the previous one from rjwysocki.net, it just got
delayed by me doing mostly fs and architecture merges yesterday.

Pulled now, going through test-compiles before pushing out

              
So the rule about ABI changes has always been: "If somebody notices
it, we revert it".

IOW, it's not so much that you can't make changes, it's that you
cannot make changes that break programs.

And quite frankly, I have no good way to judge. Your (5) _sounds_
fair, but who knows what odd things some distributions do. We'd have
to be very careful, in *particular* we'd need to make it very very
clear to the krb people that if the change scews over any old users,
it gets reverted with extreme prejudice. At that point, maybe they say
"never mind, we might as well just always make sure we have a session
keyring".

And no, we are *not* going to say "we'll just stop doing this in the
kernel and expect pam_keyinit to do it for us".

But James' suggestion to perhaps have a new version of add_key() with
new semantics could work - if it's worth the pain (because we *would*
have to maintain the old interface basically forever, so it would be
more of a "the new system call doesn't really deprecate the old one,
it just has more convenient semantics").
I think this is a pretty strong argument. Counter-arguments, anybody?

                     

Well, but the afs documentation is clearly wrong, since the
"documented" procedure doesn't actually *work*.

So I don't think "it's documented" is a very strong argument.
Documentation is as useful as used toilet paper, if clearly nobody has
ever done what was "documented".
So the S_IFREG isn't necessary.

And quite frankly, I personally think S_IRUGO | S_IWUSR is _less_
readable than 0644. It's damn hard to parse those random letter
combinations, and at least I have to really think about it, in a way
that the octal representation does *not* make me go "I have to think
about that".

So my personal preference would be to just see that simple 0644 in
proc_create. Hmm?
So I ended up doing the merge differently wrt the f2fs_write_end_io() function.

As far as I can tell, we can just initialize the f2fs_sb_info directly
and unconditionally using the first bio_vec entry, giving us the
simpler

    struct f2fs_sb_info *sbi =
F2FS_SB(bio->bi_io_vec->bv_page->mapping->host->i_sb);

which is still ugly, but whatever. I'm not quite seeing why f2fs
doesn't just set bio->private to the superblock pointer and avoid this
whole complex and long pointer chasing, but there's probably some
reason.

Anyway, it compiles for me, and looks simpler than the alternate
resolutions, but maybe there is some reason I miss why you guys did it
the way you did. And while I could test that it compiles and looks
sane, I don't have a f2fs to actually *test* it on, and maybe I
screwed up royally.

I'll push it out once I've done the rest of the allmodconfig build-test.Heh. The conflict was trivial, and git does 99% of it automatically -
leaving just the header file inclusions to be resolved manually
because they were right next to each other. The resolution just looks
scarier than it is because there were other changes that did resolve
cleanly, but that were "close enough" to each other that the context
overlapped, so they are shown too. 

But yes, had it actually been complicated, I'd probably still have
done it, and perhaps asked you to verify it. We did have a couple of
more serious conflicts in this merge window.

The posix_acl_chmod() code looks wrong.

Not that it looked right before either, but whatever. The code
basically looks like some variation of this in most setattr()
implementations:

but the mode we're changing to (and what ATTR_MODE guards) is actually
attr->ia_mode, not inode->i_mode. And quite frankly, passing in
inode->i_mode looks stupid, since we're already passing in the inode
pointer, so that's just redundant and pointless information.

Anyway, I noticed this after doing the (untested, and still un-acked -
hint, hint) ceph conversion. In that, I made ceph use attr->ia_mode.
Maybe that was wrong, but at least it's not insane and stupid like the
other filesystem implementations are.

Comments?
Dammit, this is pure shit, and after having to deal with yet another
pointless merge conflict due to stupid "cleanups" in Makefiles, IT
DOES NOT EVEN COMPILE.

drivers/clk/clk-si5351.c: In function ‘si5351_i2c_probe’:
drivers/clk/clk-si5351.c:1314:2: error: too many arguments to function
‘si5351_dt_parse’
  ret = si5351_dt_parse(client, variant);
  ^
drivers/clk/clk-si5351.c:1296:12: note: declared here
 static int si5351_dt_parse(struct i2c_client *client)

And no, that's not due to a merge error of mine. It was that way in your tree.

Hulk angry. Hulk smash.

I fixed it up in the merge, but I shouldn't need to. This should have
been caught in -next, and even if you compile for ARM as your primary
target, I know *damn* well that no sane ARM developer actually
compiles *on* ARM (because there are no machines where it's worth the
pain), so you should make sure that the x86-64 build works too.

If I can find compile errors within a couple of minutes of pulling and
it's not a merge error of mine, the tree I'm pulling from is clearly
crap.

So I'm more than a bit grumpy. Get your act together, and don't send
me any more shit.

In fact, I would suggest you send nothing but obvious fixes from now
on in this release. Because I won't be taking anything else.
Agreed. That looks broken even on x86-32. The low address limit is not
even *close* to 4GB in general on 32-bit, since you not only have the
TASK_SIZE, you have the kmap and the vmalloc area. On x86-32,
ARCH_LOW_ADDRESS_LIMIT should be MAXMEM, which iirc is somewhere
around 890MB or so. Not 4G.

                    

Looking at the poison data, it seems that is is the

field that has been overwritten (with all zero).

That doesn't really help me guess where the bug is, though. That code
is crazy. For example, looking at one place where it is set, we have:

which looks buggy in *so* many ways. In particular, we're doing a
kmem_cache_free() on "re", but what happened to "re->event" that we
just used? There was no release of that anywhere. Wut?

So it seems that the lifetime of these "fanotify_event_info"
structures is completely buggered. I don't even see any *attempt* to
maintain reference counts or other lifetime info. People free the
containers that point to them without doing anything at all about the
fsnotify_event that contains the fanotify_event_info that they point
to.

Jan - how is the lifetime of the fanotify_event_info tied to the
lifetime of the fanotify_response_event structure that contains
pointers into it? Because I don't see it.

And considering that it's the response field that gets overwritten, it
really sounds like *that* is the exact issue at play here - there is
some fanotify_response_event structure holding a pointer to the
fanotify_event that is embedded into a fanotify_event_info that has
been freed.

Jan?

Ok, In the meantime, Dave, can you verify whether this hacky patch
fixes your problem?

              
Do we even really need this?

I'd suggest removing it entirely. You might want to retain the whole

  compiletime_assert_atomic_type()

thing on purely the alpha side, but then it's all inside just the
alpha code, without any need for this "native_word" thing.

And if somebody tries to do a "smp_store_release()" on a random
structure or union, do we care? We're not some nanny state that wants
to give nice warnings for insane code.

This annoys me _enormously_:

  CONFIG_ACPI_INT3403_THERMAL:

  This driver uses ACPI INT3403 device objects. If present, it will
  register each INT3403 thermal sensor as a thermal zone.

This is the "help" text for the new ACPI thermal driver.

WTF?

Really, whoever wrote that "help" text wasn't thinking of helping
users. What the f*ck is an INT3403 device object? Is is common? Where?
Is it a standard? Is it something normal people should expect?Hmm. You're doing this for Matrix and now the aty driver.

Maybe the problem is at the fbcon level? Depending on
FBINFO_READS_FAST to decide whether you should scroll or rewrite
sounds a bit silly: even if a device doesn't have fast reads, maybe it
has a fast accelerated BLIT operation and scrolls quickly. Should the
fbcon test perhaps be for FBINFO_READS_FAST _or_ the
FBINFO_HWACCEL_COPYAREA bits?

I dunno. I didn't actually check the ->bcopy implementations, maybe
they don't use copyarea. So I'm just going by a general "this feels
wrong" feeling. 
My point is that I'd expect *anything* that has a hardware blitter to
be faster than rewriting the screen.

FBINFO_READS_FAST is documented to be about "soft-copy" being faster
than re-rendering. Which I take to be about actually doing copying in
*software*.

In particular, updatescrollmode() seems to do this right. It sets
p->scrollmode based on whether there's an accelerated copyarea. But
then SCROLL_PAN/WRAP_MOVE ends up re-testing FBINFO_READS_FAST,
ignoring any hw-accelerated copy-area, and I don't quite see why. Hmm. Is there some reason why these are not going through Tomi? And
cc'd to linux-fbdev for that matter?

It's not that I can't apply patches sent directly to me, but quite
frankly, unless there is a *reason* to not go through the subsystem
maintainer I don't really want to. Afaik Tomi has been active and
responsive, but maybe there's some strife I don't know about. 

Yes, there was a time when fbdev maintainership had responsiveness
issues, but that should be fixed. So I'd much rather have these go
through the channels.

And btw, you actually have access to a TGA card? Trippy.

            

On x86 they are. Not necessarily elsewhere.
smp_mb__before_atomic_xyz() and smp_mb__after_atomic_xyz() will do it,
and are no-op (well, barriers - I don't think it matters) on x86.SInce it's the unlock path,. you need to use the "mb__*before*"
versions, since presumably you want to protect what is inside the lock
from leaking out, not some random thing after the lock from leaking
in. 

              Btw, I suspect you could/should have just used an "octopus merge" to
merge these kinds of small independent branches in one go. Just list
all the branches you want to merge for one single "git merge", and
you're done.

I don't necessarily recommend doing that in general, but octopus
merges are actually quite nice for this kind of situation in that it
avoids having excessive empty merge commits. Now we end up having your
four merges, and then my one merge on top: so we have five merges for
four actual fixes. Octopus merges can end up making the history more
readable by avoiding that kind of somewhat excessive merge activity
that you otherwise easily get from having lots of small topic
branches.

But it's up to you. I like seeing topic branches, and that part is
absolutely a good git habit to get into. Octopus merges then have the
upside that they then avoid having lots of pointless small merge
commits to tie them all together, but they can make it slightly more
challenging to figure out what went wrong if problems happen. So in
this case, one option might have been to merge the three independent
driver fixes with one single octopus merge, and then merge the core
fix separately as a normal merge.

Whatever. Not a big deal, I just thought I'd mention it if you perhaps
didn't realize that git happily merges multiple branches at once. 

Christ. When you start doing octopus merges, you don't do it by half
measures, do you?

I just pulled the sound updates from Takashi, and as a result got your
merge commit 2cde51fbd0f3. That one has 66 parents.

That kind of merge either needs to be split up, or gitk needs to be
made better about visualizing it, because it ends up being *so* wide
that the history is hard to read.

I think you'll find that having that many parents also breaks old
versions of git.

Anyway, I'd suggest you try to limit octopus merges to ~15 parents or
less to make the visualization tools not go crazy.  Maybe aim for just
10 or so in most cases.

It's pulled, and it's fine, but there's clearly a balance between
"octopus merges are fine" and "Christ, that's not an octopus, that's a
Cthulhu merge".

Yeah, I'm definitely ok with octopus merges, and I do them myself
occasionally (especially with the -tip tree, which I get as many
separate pull requests - see for example commit 669fc2f0c70a).

And various maintainers use them too - Ingo does them for x86, Russell
King does them for ARM, Roland Dreier for infiniband, Paul McKenney
for rcu stuff, etc etc. Len Brown used to do them for ACPI all the
time.

But I do ask people to use them judiciously. Octopus merges are good
for the "many small topic branches" kind of thing. But they should
generally be avoided for big merges.

Obligatory git trick of the day:

shows you the octopus merges. A couple of the older ones are bogus and
come from the bad old days when git didn't properly filter parent
lists, so there are redundant parents making a commit _look_ like an
octopus merge even though it's really just merging one branch. Maybe
Takashi hit one of those bugs back when. 
gmail hates you. I just found four of your emails in my spam-box.

I don't know why, since it's not the (somewhat) common issue with bad
or missing SPF information. The only thing I can imagine is that gmail
thinks you're not you, since your "From:" line says

but then the SPF is validated as coming from
smtp.mail=broonie@sirena.org.uk with that as a return path:

so gmail may be dinging you for not using your real "From:". I dunno.
Nothing else looks odd about the email.
gmail still hates you rjwysocki.net address, even if it now says "spf pass".

So it's something else that makes gmail hate you. Spammy ISP?

The correct statistics are actually

as rename detection would have shown.

(Rename detection isn't the default for git, because the resulting
diffs aren't applicable by old broken versions of patch. Some day I
might ask Junio to consider making it the default, but in the name of
interoperability that day is years from now.  GNU patch actually does
understand rename diffs, but other tools like diffstat etc still
don't)

Ugh. It does not compile for me. The problem seems to be that the
gpio-mcp23s08.c driver only compiles in OF environments, but isn't
properly restricted to them:

and similar other errors.

Introduced by commit 4e47f91bf741 ("gpio: mcp23s08: Add irq
functionality for i2c chips") as far as I can tell.

The fact that it doesn't even compile makes me doubt your statement
that it has been in linux-next. It doesn't even pass a basic
allmodconfig build.

I see that you tried to fix it in commit 01d7004181c8 ("gpio:
mcp23s08: depend on OF_GPIO") but screwed up the order of operations.

I fixed it up properly in the merge, but please try to figure out how
the hell this passed through the cracks.
You messed up the pull request too.  The branch name is missing from
that git line, even if you did mention it a few lines earlier. 

              

Hmm. The tag is signed, but you have a marked lack of signatures on
your key. Would you mind trying to correct that?

I've pulled, since hopefully linaro.org is well-maintained and you
have a lot of existing commits, but in general I dislike pulling
things from new people with badly connected gpg signatures, and I
think this is the first time I've pulled from you directly. Since I got a conflict on this one and had to look at the code:
that commit looks wrong (and mainline had fixed it correctly in the meantime).

The problem with

is that this is a callback for acpi_walk_namespace(), and returning a
failure status means that the walk will be interrupted.

So you actually want to return AE_OK if the acpi_get_name() call
fails, because that just skips the failing node. Returning failure
will skip *all* the nodes.

In practice it probably doesn't matter (acpi_get_name() isn't supposed
to fail), but I thought I'd point it out since I had to stare at the
conflict.Not reasonably, no.

The ldl/stc implementation on early alpha was so broken as to be
unusable. It's not actually done in the cache, it WENT OUT ON THE BUS.
We're talking 70's style "external lock signal" kind of things like
the 8086 did for locked cycles before the advent of caches, the kind
that nobody sane has done for a long long time.

So realistically, you absolutely do not want to use those things to
emulate atomic byte/word accesses. The whole point of "load_acquire()"
and "store_release()" is that it's supposed to be cheaper than a
locked access, and can be done with just a barrier instruction or a
special instruction flag.

If you just want to do a store release, on alpha you'd want to
implement that as a full memory barrier followed by a store. It
doesn't get the advantage of a real release consistency model, but at
least it's not doing an external bus access. But you can only do that
store as a 4-byte or 8-byte store.on the older alphas (byte and word
stores work on newer ones).

Of course, it's entirely possible that nobody cares. I'm not seeing this.

Why the hell would you have byte- or halfword-sized versions of the
store_release or load_acquire things on alpha anyway?

What it means is that data structures that do locking or atomics need
to be "int" or "long" on alpha.  That has always been true. What do
you claim has changed?You're missing something.

Just make the "writer" field be an "int" on little-endian archiectures
(like alpha).

There is no reason for that field to be a "char" to begin with, as far
as I can tell, since the padding of the structure means that it
doesn't save any space. But even if that wasn't true, we could make an
arch-specific type for "minimum type for locking".

So my *point* was that it should be easy enough to just make sure that
any data structures used for locking have types that are appropriate
for that locking.The release got delayed by a week due to travels, but I suspect that's
just as well. We had a few fixes come in, and while it wasn't a lot, I
think we're better off for it. At least I hope so - I'll be very
disappointed if any of them cause more problems than they fix. 

Anyway, the patch from rc8 is fairly small, with mainly some small
arch updates (arm, mips, powerpc, s390, sparc, x86 all had some minor
changes, some of them due to a networking fix for the bpf jit). And
drivers (mainly gpu and networking). And some generic networking
fixes. The appended shortlog gives more details.

Anyway, with this, the merge window for 3.14 is obviously open.

This patch-series is not bisectable. Afaik, the first patch will break
the build (or at least cause the end result to not actually work).

This kind of "split up one large patch into many small patches THAT
DON'T ACTUALLY WORK INDIVIDUALLY" model is pure and utter garbage.

So a big NAK on this series as being completely broken.

To fix it, I would suggest:

 - make the first patch change all *existing* users (that only have
the atomic vs uninterruptible semantics) pass in either
TASK_UNINTERRUPTIBLE or TASK_RUNNING depending on whether they had
__GFP_WAIT or not.

   So the first patch would not change *any* semantics or behavior, it
would only change the calling convention.

 - do the cleanup patches to block/blk-mq-tag.c to not have those
"gfp" calling convention, and instead passing in the state natively

 - add the TASK_INTERRUPTIBLE case last (which includes the new
"signal_pending_state()" logic in percpu_ida_alloc())

that way, all patches compile cleanly and should each work
individually, and they all do clearly just one thing. And the biggest
patch in the series (the first one) doesn't actually make any semantic
changes.
I'm not pulling this. Passing in TASK_RUNNING to prepare_to_wait() is
insane (because if it ever were to actually wait, that would be a
bug), and afaik this would be the first time anybody ever does that.

Yes, yes, it may "work", but I'm not pulling that kind of hack just
before a release.

Quite frankly, it looks like you want to have a tristate argument ("no
wait", "wait interruptibly", "wait uninterruptibly") to that
percpu_ida_alloc() function. Fine. But dammit, using this kind of
hackery, and then having two *different* calling conventions (one
mis-using the gfp_t for legacy reasons, and one now using the task
state flags in odd ways) is just not acceptable.

Now, neither of those two is perfect, but I can see why you want to
use the task state ones to say which kind of interruptible you want. 
But I really don't like suddenly having a
prepare_to_wait(TASK_RUNNING) caller without any discussion, and I
*really* don't like having two completely different models for this
hack.

So quite frankly, I'd much prefer:

 - talk to the scheduler people, and make them aware of the fact that
you are going to pass in TASK_RUNNING to prepare_to_wait(). It works
with the code as-is, as long as you don't actually then wait.

 - Don't do that wrapper function with a totally different calling
convention logic. Instead, just change all the callers explicitly.
From a quick look, you really only have a couple of cases:

  (a) target/iscsi, which wants the new ternary argument

  (b) vhost/scsi.c, which uses GFP_ATOMIC and can be changed to TASK_RUNNABLE

  (c) block/blk-mq-tag.c, which already hates the current insane
thing, and uses __GFP_WAIT and (gfp & ~__GFP_WAIT) and other hacks,
and is obviously *very* aware of the internal hackery in the current
percpu_ida_alloc() argument. So I'm getting the feeling that that
whole thing might actually be *happier* with the TASK_xyz flags.

So I really think this needs cleanup, and that hacky "passing in
TASK_RUNNING to prepare_to_wait()" needs to be made official. And yes,
that implies that it's too late to try to push this through for 3.13,
this goes into the next merge window and can be backported.

Added the appropriate people to the Cc. Adding in the appropriate people. 
Andy, Paul? The changes to the pinctrl driver since 3.11 look trivial,
but I guess the GPIO and ACPI ID addition ends up also enabling all
the old code that Alan probably never ran in 3.11 because the driver
didn't trigger on his machine.

Alan, do you have a crash picture/log?
Daniel/Dave?

Thanks,

Hmm. Doesn't that mean that a new lock owner can come in *before*
you've called debug_mutex_unlock and the lockdep stuff, and get the
lock? And then debug_mutex_lock() will be called *before* the unlocker
called debug_mutex_unlock(), which I'm sure confuses things.

          Hmm. It seems to be that the *normal* sequence should be:

 - get i_mutex, call lookup, which gets sbi->lock (reiserfs_write_lock)

but in the mounting path, we have special circumstances.

That finish_unfinished() function does

 - reiserfs_write_lock_nested() .
 - remove_save_link
 - iput(inode) with the write lock held

and that can apparently end up taking i_mutex in open_xa_dir (and then
recursively the write lock, but that's an explicitly recursive lock,
so that part should be ok).

Now, I don't think this can *really* deadlock with the normal order of
operations, because during mounting there is no other process that can
take those in the reverse order (since the filesystem isn't live), but
I do wonder if we should just release the reiserfs write lock over the
iputs. We release it in other parts anyway (like for the quota off)

Jeff, you already touched this exact case in commit d2d0395fd177
("reiserfs: locking, release lock around quota operations") except
that was for those quota operation cases.

Even if it's not a real problem, making lockdep happy sounds like a
good idea. Of course, the trouble is that this code path almost never
gets exercised (which is why this hasn't been noticed earlier), so
testing. 

Jeff? Comments?

Quite frankly, I'll prefer to not merge it now, and then 3.13 will get
it from stable, when it does things like this.

Partly because it fixes a power-only bug, but potentially changes
non-power behavior. If it was all in arch/powerpc, I wouldn't mind.

               

Does something like this fix it for you?

(The above is not even compile-tested, because x86 doesn't use
GENERIC_SCHED_CLOCK. So I did the patch blindly, but I think you get
the idea. )

Hmm. Only with lockdep, right?

Does lockdep perhaps read the scheduler clock? Afaik, we have
lockstat_clock(), which uses local_clock(), which in turn translates
to sched_clock_cpu(smp_processor_id()). 

So if that code now tries to read the scheduler clock when
update_sched_clock() is doing a update and has done a
write_seqcount_begin(). Ugh.

On the x86 vclock_gettime() side, we only do this for the reader. Why
did you make the generic version do it for the writer too, adding the
necessity for those new operations? It's only the reader side that
doesn't want it.

Talking about the new operations, that "*_no_lockdep()" naming annoys
me. It doesn't match the spinlock naming, which is to just use
"raw_*()" instead. Wouldn't it be nice to make the naming be
consistent too? Especially when it's paired with raw_local_irq_save()
that shares that "raw_" model for non-checking stuff.

Ack on this and on 2/2. I'm assuming I'll get them through the -tip
tree, which is where the problem came from. No?

               

So Andrew suggested just removing the BUG_ON(), but it's been there
for a *long* time.

And I detest the patch that was sent out that said "Should I check?"

Maybe we should just remove that mlock_vma_page() thing instead in
try_to_unmap_cluster()? Or maybe actually lock the page around calling
it?

Maybe. But dammit, that's subtle, and I don't think you're even right.

It basically depends on mlock_vma_page() and munlock_vma_page() being
able to run CONCURRENTLY on the same page. In particular, you could
have a mlock_vma_page() set the bit on one CPU, and munlock_vma_page()
immediately clearing it on another, and then the rest of those
functions could run with a totally arbitrary interleaving when working
with the exact same page.

They both do basically

but one or the other would randomly win the race (it's internally
protected by the lru lock), and *if* the munlock_vma_page() wins it,
it would also do

but if mlock_vma_page() wins it, that wouldn't happen. That looks
entirely broken - you end up with the PageMlocked bit clear, but
try_to_munlock() was never called on that page, because
mlock_vma_page() got to the page isolation before the "subsequent"
munlock_vma_page().

And this is very much what the page lock serialization would prevent.
So no, the PageMlocked in *no* way gives serialization. It's an atomic
bit op, yes, but that only "serializes" in one direction, not when you
can have a mix of bit setting and clearing.

So quite frankly, I think you're wrong. The BUG_ON() is correct, or at
least enforces some kind of ordering. And try_to_unmap_cluster() is
just broken in calling that without the page being locked. That's my
opinion. There may be some *other* reason why it all happens to work,
but no, "TestSetPageMlocked should provide enough race protection" is
simply not true, and even if it were, it's way too subtle and odd to
be a good rule.

So I really object to just removing the BUG_ON(). Not with a *lot*
more explanation as to why these kinds of issues wouldn't matter.This looks sane to me. Andrew?

            

Do we have actual users of this? Because I'd almost be inclined to say
"we just don't support field widths on sscanf() and will warn" unless
there are users.

We've done that before. The kernel has various limited functions. See
the whole snprint() issue with %n, which we decided that supporting
the full semantics was actually a big mistake and we actively
*removed* code that had been misguidedly added just because people
thought we should do everything a standard user library does. 

Limiting our problem space is a *good* thing, not a bad thing.

If it's possible, of course, and we don't have nasty users

Here's both x86 people and filesystem people involved, because this
hacky RFC patch touches both.

NOTE NOTE NOTE! I've modified "cp_new_stat()" in place, in a way that
is x86-64 specific. So the attached patch *only* works on x86-64, and
will very actively break on anything else. That's intentional, because
that way it's more obvious how the code changes, but a real patch
would create a *new* cp_new_stat() for x86-64, and conditionalize the
existing generic "cp_new_stat()" on not already having an
architecture-optimized one.

Basically, under some filesystem loads, "stat()" and friends are the
most common ops (think tree traversal, but also things like git
verifying the index). And our "cp_new_stat()" function (which is the
common interface, ignoring 32-bit legacy stuff) is generic, but
actually pretty disgusting. It copies things to a temporary 'struct
stat' buffer on the kernel stack, and then uses copy_to_user() to copy
it to user space. The double copy is quite noticeable in profiles, and
it generates a big stack frame too.

By doing an architecture-specific cp_new_stat() function, we can
improve on that.

HOWEVER. On x86, doing an efficient field-at-a-time copy also requires
us to use put_user_try() and put_user_catch() in order to not have
tons of clac/stac instructions for the extended permission testing.
And the implementation of that was actually fairly non-optimal, so to
actually get the code I wanted, I had to change how that all worked
too, using "asm_volatile_goto()".

Thus both x86 and FS people on the list.

Comments? This would obviously be a 3.14 issue, I'm not suggesting
we'd do this now. I just want to lay the ground-work. 

It's tested in the sense that "it works for me", and profiles look nice, but. 

Interestingly, looking at the cp_new_stat() profiles, the games we
play to get efficient range checking seem to actually hurt us. Maybe
it's the "sbb" that is just expensive, or maybe it's turning a (very
predictable) conditional branch into a data dependency chain instead.
Or maybe it's just random noise in my profiles that happened to make
those sbb's look bad.

What do people think about this access_ok() simplification that gets
rid of inline asm, and instead optimizes the (reasonably common) case
of a constant size. The complex case that requires overflow checking
*might* get a bit slower (it's not clear: it really wasn't generating
wonderful code), but the simple case of a known constant size actually
gets simpler.

Random aside: this simplification made the special spurious
access_ok() check in kernel/futex.c (the NULL user pointer check in
futex_init()) just go away entirely, because the compiler could now
see that it can never trigger for a NULL pointer.  Not a performance
issue, but it was kind of funny to notice how getting rid of the
inline asm actually made the compiler able to notice that.

It turns out that the assembly variant doesn't actually produce that
good code, presumably partly because it creates a long dependency
chain with no scheduling, and partly because we cannot get a flags
result out of gcc (which could be fixed with asm goto, but it turns
out not to be worth it.)

The C code allows gcc to schedule and generate multiple (easily
predictable) branches, and as a side benefit we can really optimize
the case where the size is constant.

I think we should. You could make it to do something like eighteen
expensive page faults in a row for EFAULT, and that's just disgusting,
when there is no reason to do it.

But to be honest, the resulting assembly is also easier to read,
because it doesn't have those annoying bogus branch targets all over
in the middle of the code. That was actually my main issue - looking
at the generated fs/stat.s file and not puking ;)

(it's still hard to read with all the fixup section stuff, but it's
better. And it really does generate better code, so. )

Oh, you're right, I forgot to actually declare the label so that gcc
sees that it's a local one.

So it needs a

   __label__ put_user_fail;

in the put_user_try() (and yes, maybe the label name should have
underscores prepended or something, just to make sure it's internal).

But gcc is perfectly fine with multiple labels in different scopes if
you do that. We already use that in a few places, so this isn't even a
new pattern for us.On most _real_ loads, the kernel footprint is pretty small, and
obviously if you do IO or other stuff, the cost of some wasted CPU
cycles in stat() is negligible. And even if you loop doing normal
'stat()' (and everything is cached), over a filesystem, the actual
path walk itself is still dominant, although at that point the CPU
overhead of the stat data copy starts to at least show up in profiles.
Showing up in profiles is why I started looking at it.

And then, for the extreme case, my test program went from 0.92s to
0.80s. But that stupid test program just does ten million fstat(0,&st)
calls, in order to show the highest cp_new_stat() CPU overhead (with
minimal cache footprint). So at that point it's about a 13%
difference, when the only other overhead really is just the system
call itself. The profile went from

  10.42%  a.out  [k] copy_user_enhanced_fast_string
   5.91%  a.out  [k] cp_new_stat

(the "copy_user_enhanced_fast_string" is the "rep movsb" that copies
things to user space) to

   6.69%  a.out  [k] cp_new_stat

(and here there is no separate user-copy, since it's all done directly
inside the optimized cp_new_stat).

It's worth pointing out that the 5.91% -> 6.69% profile change of
cp_new_stat() is *not* because cp_new_stat() got slower: it's simply a
direct result of the 13% performance improvements - 13% of the cycles
went away, and so 5.91% becomes 6.69%.

But the above 13% is really the extreme case with hot caches etc.
Normally it's not really noticeable in any bigger picture load. For
example, on a big "git diff", the cp_new_stat() function shows up at
0.63% of the load, so making it faster isn't going to make any very
noticeable difference.

The reason I care really is that that stat overhead always does show
up on the kinds of profiles I care about, even if it's never all that
high. In other words, it's more of an annoyance over how stupid that
code was than a huge performance increase.
restore_fpu_checking() hackery for X86_FEATURE_FXSAVE_LEAK. 

Which also explains why it only triggers on E-350 - it's only relevant
for those K7/K8 CPU's that use this.

Maybe just add a fcnlex to before the emms? Something like this
(TOTALLY UNTESTED!!) attached patch.

Ok, good.

Peter, do you want to take it (feel free to add my sign-off), or
should I just commit it?

Also, is there a way to have a "likely not true" version of that
"static_cpu_has()"? There seems to be no way to make the non-K7/K8
case the fallthrough code.  Not that this is likely that
performance-critical, but. 

Btw, I think we could optimize this a bit further for the wakeup case.

wake_futex() does a get_task_struct(p)/put_task_struct(p) around its
actual waking logic, and I don't think that's necessary. The task
structures are RCU-delayed, and the task cannot go away until the
"q->lock_ptr = NULL" afaik, so you could replace that atomic inc/dec
with just a RCU read region.

Maybe it's not a big deal ("wake_up_state()" ends up getting the task
struct pi_lock anyway, so it's not like we can avoid toucing the task
structure), but I'm getting the feeling that we're doing a lot of
unnecessary work here.

This only triggers for the actual case of having a task to wake up,
though, so not your particular test load.Do you have some broken compiler? I really tend to hate this kind of
workarounds, because as mentioned, they can actually hide valid
warnings, and it seems to be due to just stupid compilers. Are we
perhaps better off trying to get people off the broken compiler
versions instead?

Jens? I'm going to do an rc8 tomorrow, and I haven't seen any pull requests. 

Yeah, it's a hack, and it's wrong, and we should figure out how to do
it right. Likely we should just tie the lifetime of the i_security
member directly to the lifetime of the inode itself, and just make the
rule be that security_inode_free() gets called from whatever frees the
inode itself, and *not* have an extra rcu callback etc. But that
sounds like a bigger change than I'm comfy with right now, so the
hacky one might be the band-aid to do for stable. 

The problem, of course, is that all the different filesystems have
their own inode allocations/freeing. Of course, they all tend to share
the same pattern ("call_rcu xyz_i_callback"), so maybe we could try to
make that a more generic thing? Like have a "free_inode" vfs callback,
and do the call_rcu delaying at the VFS level. 

And maybe, just maybe, we could just say that that is what
"destroy_inode()" is, and that we will just call it from rcu context.
All the IO has hopefully been done earlier  Yes/no?
Yeah.Ok, so we can't change destroy_inode, and we'd need to add a new op
for just freeing it.

Painful mainly because there are so many filesystems, but it shouldn't
be *complicated*.

LSM stacking is a pipedream right  now anyway, isn't it? It's been
talked about for years and years, I've never seen a patch-set that is
even remotely something we'd seriously consider.Hmm. Looks like the futex code is somehow stuck in a loop, calling
get_user_pages_fast().

The futex code itself is apparently so low-overhead that it doesn't
show up in your 'perf top' report (which is dominated by all the
expensive debug things that get_user_pages_fast() etc ends up doing),
but that's the only looping I can see. Perhaps the "goto again" case
for transparent huge pages in get_futex_key()? Or the
"retry[_private]" cases in futex_requeue()? Some error condition that
causes us to retry forever, rather than returning the error code?

Added a few more people to the cc.  Ideas?

I think you need to look closer.

We have at least also that "futex_proxy_trylock_atomic() returns
-EAGAIN" case. Which triggers at some exit condition. Another thread
in the same group, perhaps never completing the exit because it's
waiting for this one? I dunno, I didn't look any closer (but this does
make me think "Hey, we should add Oleg to the Cc too", since
PF_EXITING is involved).  So maybe there is some situation where that
EAGAIN will keep happening, forever. 

Now, I'm *not* saying that that is it. It's quite possible/likely some
other loop, but I do have to say that it sure isn't _obvious_. And
that whole EAGAIN return case is quite deep and special, so . 
PS: Oleg - the whole thread is on lkml. Ping me if you need more context.

Hmm. Ok, so something is calling [__]get_user_pages_fast() and
put_page() in a loop, but the trace doesn't show what that "something"
is, because it is itself not ever called.

However, that pattern does seem to imply that the loop is in
get_futex_key(), because all the other loops I see seem to be calling
other things as well.

And the __get_user_pages_fast() call implies that it's the THP case
that triggers the "unlikely(PageTail(page))" case. And anyway,
otherwise we'd see lock_page()/unlock_page() too.

So it looks like __get_user_pages_fast() fails, and keeps failing.
Andrea, this is your code, any ideas? Commit a5b338f2b0b1f ("thp:
update futex compound knowledge") to be exact.
Hmm.  Is any of the addresses unchecked, perhaps?
__get_user_pages_fast() does an access_ok() check, while
get_user_pages_fast() does *not* seem to do one.

That looks a bit dangerous. Yeah, users should have checked the
address range, but there really is no reason not to do it in
get_user_pages_fast().

And it looks like the futex code is actually seriously buggered. It
only does the access_ok() check for the non-shared case.

Why?

Shouldn't we do something like the attached?

So I think that kernel/futex.c part of the patch might be a good idea,
but on x86-64 (which is what Dave is running), the

test in get_user_pages_fast() should have been equivalent. And even on
32-bit, we do check the _PAGE_USER bits in the page tables, so I guess
it's all good on a get_user_pages_fast() side.

So never mind. It's not the address checking.

And I think I see what's up.

I think what happens is:
 - get_user_pages_fast(address, 1, 1, &page) fails (because it's read-only)
 - get_user_pages_fast(address, 1, 0, &page) succeeds and gets a large-page
 - __get_user_pages_fast(address, 1, 1, &page) fails (because it's read-only).

so what triggers this is likely that Dave now does large-pages, and
one of them is a read-only mapping.

So I would suggest replacing the second "1" in the
__get_user_pages_fast() call with a "!ro" instead. So how about this
second patch instead (the access_ok() move remains).

Comments?

Btw I suspect this is just disgustingly expensive, and I don't think
there's a really good reason for it.

May I suggest:

 - getting rid of the PG_compound_lock bit-lock

   bitlocks are expensive and unfair, and don't even get lockdep checking

 - replace it with a (small, say 32-256 entries) array of hashed sequence locks

 - just hash based on the "struct page" pointer, and teach this code
to do a read_seqcount_begin/read_seqcount_retry sequence instead for
the page lookup.

I think you can get rid of all the irq disables too, and the sequence
lock should be pure memory reads for the read-case that we care about.

Hmm? This is obviously orthogonal to your series, I just reacted to
seeing that bitlock thing that needs atomics for both locking and
unlocking and the irq disable, and just generally looks like the worst
possible way to do these things.
Just making it all more obvious.

Nobody actually uses that argument any more (it goes back to the old
i386 "let's manually verify that we have write permissions, because
the CPU doesn't do it for us in the trap handling"), and it should
probably be removed.

But you're right that it's at least misleading. I'd love to remove it
entirely, because it's not even syntax-checked, and it's confusing.
But that would be a humongous patch.

So these days, "access_ok()" literally just checks that the address is
in the user address space range. And that would seem to always be
appropriate for futexes, so why not just do it in the generic code?Yeah. It turns out we do do the access check indirectly - by looking
at the PAGE_USER bit, even if we don't necessarily check the actual
limits. So get_user_pages_fast() is fine.
Yup, see my email from ten minutes ago, we found the same thing. And
that would seem to explain the endless loop, and also the timing
(since Dave mentions he started doing large-pages lately).

So I think the "__get_user_pages_fast(address, 1, !ro, &page)" thing
should work.

Dave, can you re-create that trinity run and test that patch? I think
we've got this, but it might be nice to leave the hung machine up and
running until it's verified.  Although I don't really see what else we
could need or get out of it, so. I'd argue that half a year later the bug was *fixed*, but it was only
fixed for the regular pages. Leaving largepages buggy.

Well, the loop should be visible in the profile, since it's still active.

So doing something like

     perf record -e cycles:pp -g -a sleep 60

to get a good profile for a minute, and then looking at the
instruction-level profiles in futex_requeue() should be possible.

Of course, inlining etc often makes those rather hard to see, and the
bulk of the profile is clearly all about the lock debugging overhead,
but if the loop is in futex_requeue() then that *should* be visible in
the profile, even if it may not be anywhere near the top. 

Dave?
 "Forbidden

  You don't have permission to access /junk/perf.data.xz on this server."

also, we'd need the vmlinux file to actually decode the data, I think.

Ok, /proc/kallsyms would do it, but never mind. I think you already
pinpointed where the loop is with the trace file, so no need for
trying to decode the profile.Ugh. Looking at kernel/futex.s even *without* debugging enabled is
pretty messy. Although much of it seems to be because the hash is
actually fairly complex. jhash2() really ends up expanding to a lot of
instructions.

But especially if that kernel was compiled with debug information,
then 'perf report' is pretty good about matching it up with the source
code, so it might not be *too* hard to try to figure out where the
loop is.

And if it's not in futex_requeue() (there are loops at other levels,
like futex_get_key()), then having the callchain information for the
costly operations will hopefully give a hint where the top loop is. So
profiling data can be very powerful for things like this.
No it's not.

Thomas, stop this crap already. Look at the f*cking code carefully
instead of just dismissing cases.

The worrisome EAGAIN case is

and now futex_requeue() will do "goto repeat" for that EAGAIN case.

So, Christ, Thomas, you have now *twice* dismissed a real concern with
totally bogus "that can never happen" by explaining some totally
unrelated *simple* case rather than the much more complex case.

So please. Really. Truly look at the code and thing about it, or shut
the f*ck up. No more of this shit where you glance at the code, find
some simple case, and say "that can't happen", and dismiss the
bug-report.

So far Dave's bug-reports have generally pretty much universally shown
real bugs. Being dismissive about it is not helpful, quite the
reverse.

Maybe the loop I'm pointing at cannot happen, but *your* explanation
for why it couldn't happen was pure and utter garbage, and was clearly
because you hadn't even bothered to look at all the cases.

Hmm. Most of that was all in your _previous_ pull request. When I pull
this time, I just get the few fixes for  Renesas DMA masks.

I pulled that and edited up the resulting merge message, but you
should double-check that I now have what you expected. Because your
pull request seems a bit confused about what's been going on. Mind posting the actual bug message, and preferably a way to trigger it?This looks completely broken to me. You do a "kobject_put()" and then
after you've dropped that last use, you wait for the completion of
something that may already have been free'd.

Wtf? Am I missing something?What we really really want to do is to have some way to add config
options based on shell scripts and compiler support. That would also
get rid of a lot of Makefile trickery etc.

Then we could just make CC_STACKPROTECTOR_STRONG depend on
CC_SUPPORTS_STACKPROTECTOR_STRONG or whatever.

                

Ack. Looks good to me. I've wanted this for a long time for other
reasons, we should finally just do it.

That said, we should make sure that the shell execution thing gets
access to $(CC) etc variables that we have in

For the compiler options, it would hopefully be sufficient to just do
something like

or something like that. No?I normally do these things from the airport when I fly out, but since
I had re-installed on my laptop I chickened out and decided to do it
before leaving, just to make sure I had everything set up on the
laptop. Wouldn't you guess, the one time I decide to be prudent, it
all worked without a hitch. 

Anyway, things have been nice and quiet, and if I wasn't travelling,
this would probably be the last -rc: there isn't really anything
holding up a release, even if there are a couple of patches still
going through discussions and percolating through maintainers. But
rather than do a real 3.13 next weekend, I'll be on the road and
decidedly *not* opening the merge window, so I'll do an rc8 next week
instead, needed or not.

And who knows, maybe something critical comes up now that people are
slowly emerging out of their food comas after the holidays, but at
least so far I get the feeling that 3.13 is doing fine.

In this rc, about half of the updates are networking (both drivers and
core), and the rest is a mix of other drivers (gpu, input, pci core,
some random of patch as well) and arch updates (s390 and powerpc, some
arm noise). And with some random stuff thrown in (cifs/gfs2, some mm
stuff from Andrew, etc).

But it's all pretty small.

Shortlog appended. Go forth and test,

Hell no. "flags" needs to be a thread-private variable, or at least
protected some way (ie the above could work if everything is inside a
bigger lock, to serialize access to FLAGS).

No. I think it makes sense to put a big warning on any users you find,
and fart in the general direction of any developer who did that broken
pattern.

Because code like that is obviously crap.

                

I still hate this whole separate counter thing. It seems really annoying.

If re-ordering things didn't work out, then why can't just the counter
we *already* have in the spinlock itself work as the counter? Your
counter update logic seems to basically match when you take the
spinlock anyway.

The *testing* side doesn't actually care about how many waiters there
are, it only cares about whether there are waiters. And it can look at
the wait-list for that - but you want to close the race between the
entry actually getting added to the list using this counter. But the
place you increment the new counter is the same place as you take the
spinlock, which does that ticket increment. No?

So I still think this can be done without that new counter field, or
any new atomics.

hb_waiters_pending() could be something like

which considering where you increment the new count should be
equivalent to your "!!hb->waiters". The smp_mb_after_atomic_inc()" on
the spinlock side would become smp_mb_after_spin_lock() instead.

Yes? No? Why?
Not necessarily. But I don't see why threads spinning on it would be
special? If you spin on things, you've already updated the head
counter, so even spinners *are* visible, even if they haven't actually
gotten the lock yet.

                 

I don't understand. Why is that any better with the separate atomic count?

The only place you increment the count - hb_waiters_inc() - is called
in exactly two places:

 - in requeue_futex(), which already holds the spinlock on both queues

 - in queue_lock(), immediately before getting the spinlock (which
will do the SAME ATOMIC INCREMENT, except it's just doing it on a
different member of the structure, namely the spinlock head)

so if you drop the waiters increment, and instead use the spinlock
head increment as the "waiters", you get exactly the same semantics.
If anything, for the requeue_futex() case, the spinlock increment was
done *earlier*, but that's really not relevant.

So how could we miss this? Explain to me what the separate counter
does that isn't done by the spinlock head counter.
So that's why I suggested checking the ticket lock _and_ the state of
the node list.

The ticket lock state gives you the racy window before the entry has
actually been added to the node list, and then after the lock has been
released, the fact that the list isn't empty gives you the rest.

I think we might have to order the two reads with an smp_rmb - making
sure that we check the lock state first - but I think it should be
otherwise pretty solid.

And maybe there is some reason it doesn't work, but I'd really like to
understand it (and I'd like to avoid the extra atomics for the waiters
count if it at all can be avoided)Yeah, I said "spin_contended()" myself initially, but it needs to be
"spin_is_locked()". It's hopefully unlikely to ever actually be
contended (which in ticket lock terms is "head is _more_ than one away
from the tail").
Ok, so there's the "q->lock_ptr = &hb->lock" assignment in between,
but if the ordering of that is critical, it should be documented in
the memory ordering rules, because it sure is subtle and not obviously
visible anywhere. 

            
Hmm. Trying to answer this myself by looking at the code. And I just
realized that I described things wrong ("spin_contended()" rather than
"spin_is_locked()"). So that was a bug in my description.

One difference is that by avoiding the counter, the "do we have
waiters" is two values rather than one ("is spin locked" + "node list
head is empty"). So there are possible memory ordering issues wrt
reading those two fields, but I don't see it mattering: since you'd
need to read both "not locked" _and_ "list empty", any re-ordering
that shows both of those cases should be able to show the "waiters ==
0" case in the explicit separate counter model.

I don't know why I care, though. For some reason that extra counter
just makes me go "I'm convinced it's not needed", even though I can't
really explain why I should care about it.

I really think we want to put an upper bound on the read-ahead, and
I'm not convinced we need to try to be excessively clever about it. We
also probably don't want to make it too expensive to calculate,
because afaik this ends up being called for each file we open when we
don't have pages in the page cache yet.

The current function seems reasonable on a single-node system. Let's
not kill it entirely just because it has some odd corner-case on
multi-node systems.

In fact, for all I care, I think it would be perfectly ok to just use
a truly stupid hard limit ("you can't read-ahead more than 16MB" or
whatever).

What we do *not* want to allow is to have people call "readahead"
functions and basically kill the machine because you now have a
unkillable IO that is insanely big. So I'd much rather limit it too
much than too little. And on absolutely no sane IO susbsystem does it
make sense to read ahead insane amounts.

So I'd rather limit it to something stupid and small, than to not
limit things at all.

Looking at the interface, for example, the natural thing to do for the
"readahead()" system call, for example, is to just give it a size of
~0ul, and let the system limit things, becaue limiting things in useer
space is just not reasonable.

So I really do *not* think it's fine to just remove the limit entirely.As expected, things have been quiet over the holiday week. So various
small random updates: drivers (infiniband, gpu, cpufreq, libata,
block), some small filesystem fixes (ext4/jbd2), and a few ARM SoC
things. Tiny x86, percpu and cgroup fixes.

Nothing you'd normally even notice, just 81 fairly small commits.

Please cc the guilty parties when sending patches like this.

Also, just out of interest, please describe how this bug affected
things. Did we overflow the saved_auxv[] array, or what? Also, how
does this change affect architectures that _don't_ have that
ELF_HWCAP2 thing, ie everything but powerpc?

Acks, people?Did you actually *see* the problem, or was this just from looking at the code?

I don't hate the patch, and it might be the right thing to do in any
case (just to avoid depending on subtle things), but this really *is*
subtle, and I'm adding Oleg to the participants since it is his code
(going back to 2006, no less).

We have coredump serialization in exit_mm() that I think *should* make
this all ok - if we still see p->mm matching our mm, I don't think it
should be able to get to __exit_signal() and make the sighand go away,
so the lock_task_sighand() shouldn't ever fail. But I might miss
something, and as mentioned the patch might be a good idea regardless
just to avoid overly subtle rules that can confuse people.

Oleg?

Can you please post the oops, just in case that has any more hints.
Maybe the serialization is broken somehow on ARM, or there is some
ARM-specific bug. This code _is_ fairly subtle, but I don't think any
of the patches I've seen so far should really fix anything.

              
So commit 1bf49dd4be0b ("./Makefile: export initial ramdisk
compression config option") seems to be totally broken.

And I'm not saying that because Jan fixed a make-3.80 incompatibility
in commit 7ac181568342 ("fix build with make 3.80")

I'm saying that because it sets and exports the INITRD_COMPRESS
environment variable completely incorrectly, as far as I can tell.

And it looks like nobody noticed, because apparently dracut didn't use
to care about that INITRD_COMPRESS environment variable at all. But as
of a F20 update yesterday, apparently dracut actually does care, and
as a result nothing actually works.

That setting is f*cking moronic. It sets INITRD_COMPRESS to 'lz4' if
RD_LZ4 is set. But RD_LZ4 is *always* set (unless you go into some
expert settings), so basically what that commit does is to always set
INITRD_COMPRESS to lz4.

Why is that wrong?

It's wrong because

 (a) most sane people don't even have lz4 _installed_, so dracut won't
actually succeed

 (b) there's no way to select the compression level (unlike the
INITRAMFS_COMPRESSION thing that actually has a choice)

 (c) even if you *do* have lz4, it doesn't actually work, because
while that causes the new F20 dracut to compress the initramfs with
lz4, the end result is completely broken, because the F20 "lsinitrd"
scripts don't understand the end result, so now the whole kernel
install fails.

Now, it could be argued that this is a F20 bug, and the fact that F20
has a dracut that can generate a lz4 initrd, but has other tools that
then cannot handle it is arguably indeed a buglet in F20. So the (c)
part is arguably a F20 problem.

HOWEVER.

(a) and (b) are very much kernel bugs.

I'm going to remove that export of INITRD_COMPRESS, since right now
that value is broken and useless. No way does it make sense to
mindlessly default to a compressor that most people don't have, and
that doesn't work with most tools.

Yes, it fixes (a), at least to some degree, in that at least
defaulting to bzip2 is a lot more sane than defaulting to lz4. I
suspect most everybody has bzip2 installed. And at least on my current
F20 install, it looks like lsinitrd understands to use zcat, bzcat or
xzcat on the resulting initrd image (and bzcat does that bzip2
decoding).

So I think Jeff's patch at least fixes the symptoms.

That said, I think it does nothing *but* fix the symptoms, and we're
actually still better off with the 3.12 behavior which was to never
set INITRD_COMPRESS at all. Because quite frankly, there's currently
no way for the kernel to know what the right compressor is. bz2 may
well work, but can you guarantee it? I certainly can't. 

Now, if we asked the user, that would be a different thing. But right
now we very much don't ask the user, and we just pick one at random.

We're better off not picking a compression method at all, at which
point the distro "installkernel" will do whatever the distro does.I ended up committing the minimal change that I was testing, leaving
the core code in, just disabling it so that everything works for me on
F20.

I may happen to think that the kernel shouldn't try to pick the
format, but I'm willing to leave the door open for somebody arguing
for that approach.

               
Agreed, either of those would work.

Of course, so does just "distro selects whatever compression method it
thinks is best, and when you compile a kernel for that distro, you
need to make sure that that kernel knows how to uncompress the
initrd".

Which quite frankly is the sanest approach of all. *Especially*
considering that right now we default to supporting all the initrd
compression methods.

And it has the advantage of not needing anything like this at all.

When you compile a kernel, you already need to compile in support for
the stuff the distro needs. This is no different, really.

So I think the whole "kernel tells the distro what compression method
to use" approach is broken and silly, and the wrong way around.Ho ho ho,
 Christmas is almost upon us, and -rc5 is the last rc before most of
us gorge ourselves into insensibility. Or cry into our lonely beers.
Or go out for Chinese food. Or whatever you happen to do.

Things seem to be slowly calming down, and I expect that the next week
is going to be calmer yet, for all the obvious reasons. This might
also be a good time to say that even _if_ things continue to calm
down, I think we'll be going to at least -rc8 regardless, since LCA is
fairly early this year, and I won't be opening the merge window for
3.14 until after I'm back from those travels.

Anyway, about rc5: about 40% drivers (gpu, networking, sound,
misc-you-name-it), 15% architecture updates (mainly powerpc this
time), 10% filesystems (ceph/cifs), 10% documentation, and the rest
"misc", including some core kernel (scheduler) and mm (numa) fixes.

Nothing really exciting stands out. The bugs I was involved with were
all sufficiently subtle and unusual that I didn't feel like they
raised any red flags at this point, which is just how I want it. It's
the "how did that ever even pass cursory testing" bugs that make me
upset, and if those existed, people were appropriately ashamed and
quiet about them ;)

So despite me planning on dragging out the rc's a bit, there doesn't
actually look to be any real technical reason for doing that, at least
so far. It all looks good, so please jump in and help test,

Ok, there's a BUG_ON() in the middle, the "bad page" part is just this:
at free_pages() time, and I don't see anything bad in the printout wrt
the page counts of flags.

Which makes me wonder if this is mem_cgroup_bad_page_check()
triggering. Of course, if it's a race, it may be that by the time we
print out the counts they all look good, even if they weren't good at
the time we did that bad_page() *check*.

And the fact that we do have a concurrent BUG_ON() triggering with a
zero page count obviously does look suspicious. Looks like a possible
race with memory compaction happening at the same time aio_free_ring()
frees the page.

Somebody who knows the migration code needs to look at this. ChristophL?
Well, it was once again in aio_free_ring() - double free or freeing
while already in use? And this time the other end of the complaint was
allocating a new page that definitely was still busily in use (it's
locked).

And there's no sign of migration, although obviously that could have
happened or be in progress on another CPU and just didn't notice the
mess. But yes, based on the two traces, fs/aio.c:io_setup() would seem
to be the main point of interest.

Have you started doing something new in trinity wrt AIO, and
io_setup() in particular? Or anything else different that might have
started triggering this?

But we do have new AIO code, and these two in particular look suspicious:

 - new page migration logic:

    71ad7490c1f3 rework aio migrate pages to use aio fs

 - trying to fix double frees and error cases:

    e34ecee2ae79 aio: Fix a trinity splat
    d558023207e0 aio: prevent double free in ioctx_alloc
    d1b9432712a2 aio: clean up aio ring in the fail path

and some kind of double free in an error path would certainly explain
this (with io_setup() . And the first oops reported obviously had that
migration thing. So maybe those "fixes" weren't fixing things at all
(or just moved the error case around).

Btw, that "rework aio migrate pages to use aio fs" looks odd. It has
Ben LaHaise marked as author, but no sign-off, instead "Tested-by" and
"Acked-by".

Al, Ben, Kent, see the beginning thread on lkml
(https://lkml.org/lkml/2013/12/18/932). Any comments?
Yeah, that looks horribly buggy, if that's the intent.

You can't just put_page() to remove something from the page cache. You
need to do the whole "remove from radix tree" rigamarole, see for
example delete_from_page_cache(). And you can't even do that blindly,
because if the page is under writeback or otherwise busy, just
removing it from the page cache and freeing it is wrong too.
It already does that in put_aio_ring_file() afaik. No?

                Nobody commented on your request for comments, so I applied my patch
and pulled your branch, because I'm going to do -rc5 in a few and at
least we want this to get testing.

Dave, let's hope that the leak fixes and reference count fixes solve
your problem.
Ok, I'm sorry, but that's just pure bullshit then.

So it has the page array in the page cache, then mmap's it in, and
uses get_user_pages() to get the pages back that it *just* created.

This code is pure and utter garbage. It's beyond the pale how crazy it is.

Why not just get rid of the idiotic get_user_pages() crap then?
Something like the attached patch?

Totally untested, but at least it makes *some* amount of sense.

Ok, that can't work, since the ring_pages[] allocation happens later.
So that part needs to be moved up, and it needs to initialize
'nr_pages'.

So here's the same patch, but with stuff moved around a bit, and the
"oops, couldn't create page" part fixed.

Bit it's still totally and entirely untested.

Btw, I think this actually fixes a bug, in that it doesn't leak the
page reference count if the do_mmap_pgoff() call fails.

That said, that looks like just a memory leak, not explaining the
problem Dave sees. And maybe I'm missing something.

And no, I still haven't actually tested this at all. Is there an aio
tester that is worth trying?Again, adding Oleg to the cc. And I don't think this is correct, since
SIGKILL can actually kill uninterruptible processes too
(TASK_WAKEKILL) so the basic premise of the patch is incorrect. And
again, we do have that whole issue with exit_mm() serialization.Hmm. Do you have any idea why 3.4.69 still seems to do better at
higher thread counts?

No complaints about this patch-series, just wondering. 

Hmm.  Can you point me at the (fixed) microbenchmark you mention?

              
There's something wrong with that benchmark, it sometimes gets stuck,
and the profile numbers are just random (and mostly in user space).

I think you mentioned fixing a bug in it, mind pointing at the fixed benchmark?

Looking at the kernel footprint, it seems to depend on what parameters
you ran that benchmark with. Under certain loads, it seems to spend
most of the time in clearing pages and in the page allocation ("-t 8
-n 320"). And in other loads, it hits smp_call_function_many() and the
TLB flushers ("-t 8 -n 8"). So exactly what parameters did you use?

Because we've had things that change those two things (and they are
totally independent).

And does anything stand out in the profiles of ebizzy? For example, in
between 3.4.x and 3.11, we've converted the anon_vma locking from a
mutex to a rwsem, and we know that caused several issues, possibly
causing unfairness. There are other potential sources of unfairness.
It would be good to perhaps bisect things at least *somewhat*, because
*so* much has changed in 3.4 to 3.11 that it's impossible to guess.

So what?

The mb() isn't necessarily against the *write*, it is also against the *read*.

If the cflush is needed before the monitor, it's likely because the
monitor instruction has some bug with already-existing cachelines in
shared state or whatever.

Which means that we want to make sure that the cflush is ordered wrt
*reads* from that cacheline too.

cflush has nothing specifically to do with writes.That is possible. I haven't seen the exact details of the monitor
errata . If it's just about the write buffer and cache interaction,
then I agree that the barrier after the write in
current_set_polling_and_test() is sufficient.  But in the absense of
knowledge, I think we should have it after the read too.

              

Not necessarily.

Don't get me wrong - I think that it's a good idea to at least have
the option to complain about certain errors, and leave markers in the
logs about things that look suspicious.

But looking through the recent list of commits that explicitly mention
a CVE, the only one I find where a syslog message would make sense is
the HID validation ones. There, adding a warning about malicious HID
devices sounds like a good idea.

But a *lot* of the rest is just checking ranges or making sure we have
proper string handling etc that just wouldn't be practical to check.
So the error itself may be "hard to reach accidentally", but
*checking* it would be so complex/painful that it would likely just
introduce more room for bugs.

So I think the "WARNING" thing is a good idea, but I think it is a
good idea if it's used very judiciously. IOW, not for "random CVE"
(because quite frankly, most of them seem to be utter shit), but for
serious known issues. And for those issues *only*.

If I start seeing patches adding warnings "just because there's a
CVE", then I'm not in the least interested. But if there is some known
root-kit or similar, then by all means. No, it's a very stupid module if it does that.

It should require the crda hook not at module load time, but at first
ifconfig time.

We've had bugs like this before.  Doing user-mode callbacks at module
loading time is a disaster exactly because it doesn't work well with
built-in modules.

The fact that those things apparently also don't time out or notice
when they fail seems to then just exacerbate the bad decision.

We have literally had this *exact* same issue with firmware loading.
Network drivers shouldn't try to load firmware at module load time.
Same deal.

What's the broken path? Is this driver-specific, or is it generic to
the 802.11 code?Note that "xadd()" has different semantics from "atomic_add_return()".

xadd() returns the original value, while atomic_add_return() returns
the result of the addition.

In this case, we seem to want the xadd() semantics. I guess we can use
"atomic_add_return(val,&atomic)-val" and just assume that the compiler
gets it right (with the addition and the subtraction cancelling out).
Or maybe we should have a "atomic_add_return_original()" with xadd
semantics?Can you verify for me that you re-did those numbers? Because it used
to be that the fair queue write lock was slower than the numbers you
now quote. 

Was the cost of the fair queue write lock purely in the extra
conditional testing for whether the lock was supposed to be fair or
not, and now that you dropped that, it's fast? If so, then that's an
extra argument for the old conditional fair/unfair being complete
garbage.

Alternatively, maybe you just took the old timings, and the above
numbers are for the old unfair code, and *not* for the actual patch
you sent out?

So please double-check and verify.

Ok, thanks.

Sounds good. I just wanted to make sure the numbers were sane.Ugh.

This is too late since I want to do 3.12 today, and it seems to not be
a regression - from what I can tell, the problem has always existed,
no?

So I'll consider this post-3.12. However, it also strikes me that we
should just clean things up, and make "i_ino" be an u64, and be able
to do this without the vfs_getattr_nosec() games.

Hmm?Yeah, I think the circumstances have changed. 32-bit is less
important, and iget() is much less critical than it used to be (all
*normal* inode lookups are through the direct dentry pointer).

Sure, ARM is a few years away from 64-bit being common, but it's
happening. And I suspect even 32-bit ARM doesn't have the annoying
issues that x86-32 had with 64-bit values (namely using up a lot of
the register space).

So unless there's something hidden that makes it really nasty, I do
suspect that a "u64 i_ino" would just be the right thing to do. Rather
than adding workarounds for our current odd situation on 32-bit
kernels (and just wasting time on 64-bit kernels).
Pulled. However, I notice that the following issue remains with module
key signing, and I *think* it was introduced in the -rc1 pull:

 - start with clean kernel sources

 - build kernel ("make -j16" or whatever after doing your config)

 - build kernel again ("make -j" - nothing has changed):

   . 
   X.509 certificate list changed
     CERTS   kernel/x509_certificate_list
     - Including cert /home/torvalds/v2.6/linux/signing_key.x509
     AS      kernel/system_certificates.o
     LD      kernel/built-in.o
   . 

   which causes it to relink the kernel.

 - build kernel yet again, and now finally it doesn't build/link anything.

So some rule for the certificate list seems to be broken. I'm pretty
sure this didn't happen in 3.12. Any ideas?

David, Eric, netdev,
 this seems to be due to __udp4_lib_rcv() doing udp_sk_rx_dst_set(),
which takes the 'sk->sk_dst_lock' spinlock. This all happens in a
software irq context.

And on the other hand, inet_csk_listen_start() does sk_dst_reset(),
which takes the same lock *without* bh-disable, so it could deadlock
if an interrupt comes in, and bh processing happens, and we try to
take the lock recursively.

Now, if I read everything correctly, I think that this is all fine in
practice because inet_csk_listen_start() is only ever called for TCP
sockets (inet_listen seems to exit unless it's a SOCK_STREAM), and
that this is a false lockdep warning due to a very hacky use of
sk_dst_lock by UDP.

However, even if that's the case, then to make lockdep happy, maybe
UDP and TCP sockets need to initialize that sk_dst_lock in different
lockdep classes. Because we should make sure that lockdep is happy
either way.

Or maybe use a different lock for that UDP hack.

This seems to have been introduced by commit 975022310233 ("udp: ipv4:
must add synchronization in udp_sk_rx_dst_set()").

Hmm?

Nope.

I assume you meant the 'drm-fixes' branch, but you didn't actually
*say* that, and when I pull it I don't get the same diffstat you
claim, so I'm unpulling it and waiting for you to verify and
double-check.

Ok, I took it directly. Thanks.

So I delayed this a couple of days to get back to my normal Sunday
release schedule, but I'm not entirely happy with the result. Things
aren't calming down the way they should be, and -rc4 is bigger than
previous rc's. And I don't think I can just blame the two extra days.

Anyway, what that means from a practical standpoint is that I'm going
to be *very* grumpy at anybody who sends me unnecessary crap. If it's
not  regression or marked for stable, then just don't send it to me.
Because you *will* be called names if you can't follow those simple
rules. Comprende?

Anyway, the bulk of changes for rc4 is drivers (notably networking and
gpu, but there's usb, input and media driver updates too). Aside from
that, there are the usual arch updates (mainly ARM) and the slightly
less usual selinux updates.  And generic networking, with some random
stuff thrown in for good measure. The shortlog isn't very short, but
it's appended for your reading pleasure.

We're obviously heading for the holiday season, and I'm really hoping
that will help calm things down too for the rest of the release
candidates. 

Jens, ping?

Patch looks fine, and it looks completely bogus to set

     null_mq_reg.nr_hw_queues

to 'nr_online_nodes', when we've potentially done "setup_queues()"
with just a single queue.

So I'm going to apply this as-is, I'm just wondering why I haven't
heard anything from you. 

Pulled. 

I do note that your pgp key still has very few signatures, and none of
them in my normal key ring. Any possibility that you might expand on
your signatures a bit?

(The key is useful even so to show that all the pulls come from the
same person - or at least somebody with access to the key - but it
would be nice to fill out the trust network a bit. )Ok, looks good. So I'm taking these even though it's outside the merge
window - they're small, and until you enable DCACHE_WORD_ACCESS on
big-endian they don't actually do anything.

Ben - you might want to check if this makes DCACHE_WORD_ACCESS useful
on PowerPC too. Right now that requires an efficient fls()
implementation, but I think you have "cntlz" like ARM does.

                 
Hmm. Did you try to time this?

Also, I really have #ifdef's in code, and I think we'd be better off
just exposing a function that does this

thing. I think it would logically go together with zero_bytemask(),
call it something like "bytemask_from_count()" or something. Hmm? It's
basically just the reverse of "count_masked_bytes()", which we also
have an abstraction for.

So the #ifdef really looks out of place for me. We've generated all
these nice abstractions for all the other mask handling, and then you
add that ugly ifdef for this case. I think I finally found it.

I've spent waaayy too much time looking at and thinking about that
code without seeing anything wrong, but this morning I woke up and
thought to myself "What if. "

And looking at the code again, I went "BINGO".

All our reference counting etc seems right, but we have one very
subtle bug: on the freeing path, we have a pattern like this:

which on the face of it is trying to be very careful in not accessing
the pipe-info after it is released by having that "kill" flag, and
doing the release last.

And it's complete garbage.

Why?

Because the thread that decrements "pipe->files" *without* releasing
it, will very much access it after it has been dropped: that
"__pipe_unlock(pipe)" happens *after* we've decremented the pipe
reference count and dropped the inode lock. So another CPU can come in
and free the structure concurrently with that __pipe_unlock(pipe).

This happens in two places, and we don't actually need or want the
pipe lock for the pipe->files accesses (since pipe->files is protected
by inode->i_lock, not the pipe lock), so the solution is to just do
the __pipe_unlock() before the whole dance about the pipe->files
reference count.

Patch appended. And no wonder nobody has ever seen it, because the
race is unlikely as hell to ever happen. Simon, I assume it will be
another few months before we can say "yeah, that fixed it", but I
really think this is it. It explains all the symptoms, including
"DEBUG_PAGEALLOC didn't catch it" (because the access happens just as
it is released, and DEBUG_PAGEALLOC takes too long to actually free
unmap the page etc).

Once more on this issue, because I've been unable to stop thinking
about it, and noticed that it's actually even subtler than I thought
initially.

So I still think this was the bug, and the problems Simon and Ian saw
should be fixed, but it ends up being even more interesting than just
the above explanation.

Yes, the inode->i_lock is what protects the "pipe->files" counter, and
we do in fact on the allocation side have cases where we update the
counter with just the spinlock held, without holding the pipe mutex.
So when we decrement the 'files' counter and release the i_lock
spinlock, technically it's now freeable.

BUT.

It turns out that on the *releasing* side, the old code actually held
the pipe->mutex in all situations where it was decrementing the
pipe->files counter, so the race we saw happen *shouldn't* have
happened, because even though the i_lock spinlock was the real lock,
the pipe->mutex lock *should* have serialized the two threads
releasing the pipe, and thus the bug shouldn't have been visible.

So we have two threads serialized by that pipe->lock mutex, and yet
one of them writes to the structure *after* the other one (remember:
serialized!) decides that it can free it.

In other words: while the fix for pipes was simply to move the mutex
unlock up, this actually shows a big fundamental issue with mutexes.
That issue being that you cannot use them (at least as is) to protect
an object reference count. And I wonder if we do that somewhere else.

And I kind of knew about this, because we historically had very strict
rules about the whole "don't use semaphores to implement completions",
which has the exact same issue: a semaphore isn't "safe" wrt
de-allocation, and we have some places that use completions exactly
because completions guarantee that the very last access to them on the
waiter is on the waiter side (who can then free the memory), and all
other users are "safe".

And mutexes and semaphores don't have those guarantees: they just
serialize what is *inside* the mutex/semaphore, but not necessarily
the lock *ITSELF*.

So this "obviously correct" pattern is in fact fundamentally buggy:

even though it *looks* safe. Because by definition only the last
thread to get the object mutex will be the one freeing it, right?

Wrong. Because what can happen is (roughly):

but in the meantime, CPU1 is busy still unlocking:

and in particular notice how CPU1 wrote to the lock *after* CPU2 had
gotten the lock and free'd the data structure. In practice, it's only
really that "spin_unlock_mutex(&lock->wait_lock, flags);" that happens
(and this is how we got the single byte being incremented).

In other words, it's unsafe to protect reference counts inside objects
with anything but spinlocks and/or atomic refcounts. Or you have to
have the lock *outside* the object you're protecting (which is often
what you want for other reasons anyway, notably lookup).

So having a non-atomic refcount protected inside a sleeping lock is a
bug, and that's really the bug that ended up biting us for pipes.

Now, the question is: do we have other such cases? How do we document
this? Do we try to make mutexes and other locks safe to use for things
like this?I doubt that makes much of a difference. It's still just "CPU cycles"
away, and the window will be tiny unless you have multi-socket
machines and/or are just very unlucky.

For stress-testing, it would be much better to use some hack like

in __mutex_unlock_common() just before the spin_unlock(). Make the
window really *big*.

That said:

So I actually don't think we do. Why? Having a sleeping lock inside
the object that protects the refcount is actually hard to do.

You need to increment the refcount when finding the object, but that
in turn tends to imply holding the lock *before* you find it. Or you
have to find it locklessly, which in turn implies RCU, which in turn
implies non-sleeping lock.

And quite frankly, anybody who uses SRCU and a sleeping lock is just
broken to begin with. That's just an abomination.  If the RT codepaths
do something like that, don't even tell me. It's broken and stupid.

So protecting a refcount with a mutex in the same object is really
quite hard. Even the pipe code didn't actually do that, it just
happened to nest the real lock inside the sleeping lock (but that's
also why it was so easy to fix - the sleeping lock wasn't actually
necessary or helpful in the refcounting path).

So my *gut* feel is that nobody does this. But people have been known
to do insane things just because they use too many sleeping locks and
think it's "better".The lock we're moving up isn't the lock that actually protects the
whole allocation logic (it's the lock that then protects the pipe
contents when a pipe is *used*). So it's a useless lock, and moving it
up is a good idea regardless (because it makes the locks only protect
the parts they are actually *supposed* to protect.

And while extraneous lock wouldn't normally hurt, the sleeping locks
(both mutexes and semaphores) aren't actually safe wrt de-allocation -
they protect anything *inside* the lock, but the lock data structure
itself is accessed racily wrt other lockers (in a way that still
leaves the locked region protected, but not the lock itself). If you
care about details, you can walk through my example.So quite frankly, if the caller guarantees that it has looked up the
vma, and verified it, then I'm ok with it.

But in that case, I think you should pass in the vma as an argument,
and verify it for now.  Because quite frankly, the reason I reacted to
this all is that it is NOT AT ALL obvious that the callers actually do
that.

That uprobe_write_opcode() also only works if the instruction is
within a single page, another thing that it doesn't actually check.
And again, it is not at all obvious by looking at the callers that
that check has ever been done. The interface looks like you can just
rewrite an arbitrary byte range using that function.

In other words: looking at that function, my immediate reaction is
still "it's buggy". Why? Because it seems to make assumptions about
the callers that are never checked and are not at all obvious that
they have ever *been* checked. In fact, callers can clearly call that
thing without ever even looking up the vma (which it must to do check
that it's not shared), and we look it up *again* by using that slow
nasty get_user_pages() thing.

Also, quite frankly, I think the routine is still just horrible. If I
read it right, even after that cleanup you do a page table walk
*THREE* times:

 - 2x get_user_pages()
 - page_check_address()

and that's without the whole "retry" thing. It all seems pretty damn
pointless. Wouldn't it be better to do it *once*, and then the retry
logic is for "oops, we need to cow the page we looked up, so we need
to drop the page table lock and allocate a new page".

So I'd actually prefer this to (a) pass in the vma (to make it obvious
that the caller must look it up!) (b) add a few sanity checks (not
page-crossing) and I think possibly (c) make it a bit smarter still.
No we don't. get_user_pages() is expensive as hell.

We'd be *much* better off using get_user_pages_fast() if possible -
and I bet _is_ possible in 99% of all cases.

So saying that we get vma "for free" is complete BS. We get it "for
very expensive."

No. I think we should just do something like

and actually make the logic obvious. We should *also* make sure we do
the right thing for THP, because the VM_HUGETLB check isn't obviously
sufficient. Does the code really do the right thing for a writable
hugepage? I'd like that to be *obvious*.

Quite frankly, I think that adding a few of these kinds of *obvious*
checks and then just looking up the page table _once_ - instead of
three times - would make the code safer and faster.

As it is, it is neither safe nor fast. It *may* be safe if callers do
all the right black magic, but it's by no means obvious.Yeah, and we need to look up the page table entry anyway, so what we
actually want here is just the page table walker, none of the "get
page" crap at all.

So the core function should (I think) just do something like:

with a retry for the "uhhuh, it's not dirty and private, and we don't
have a new page", so we do

which gets us a single page table walk for the common case, and a
retry for the "uhhuh, we needed to page it in or allocate a new page"
condition.

Put another way: I actually think the existing "__replace_page()" code
is closer to being good than that disgusting uprobe_write_opcode()
function. I think you may be getting rid of the wrong ugly function.
There is indeed the non-locking version, however, since we want to get
the pte-pointer anyway, I really think you were right in complaining
about get_user_pages_fast() in the first place, and we'd be much
better off just doing the page table lookup ourselves. Which we have
to do later *anyway*.

The whole [__]get_user_pages[_fast]() family of functions are for
people who want the *page*. We want more than that. We really want the
page table entry, since we have to potentially change it.

Grr.

I've pulled it, but looking at that history, it's just pure and utter
f*cking garbage.

It was rebased *minutes* before sending it, as far as I can tell. Why?

And it has a pointless merge that you must have created with "--no-ff"
for no apparent good reason.

WTF? What the hell happened here, and why? As mentioned, it's in my
tree, but I was *this* close to just unpulling and saying "fuck that"
when I started looking at it.  I'm still on a Friday release schedule, although I hope that
changes soon - the reason I didn't drag this one out to Sunday is that
it's already big enough, and I'll wait until things start calming
down.

Which they really should, at this point. Hint hint. I'll start
shouting at people for sending me stuff that isn't appropriate as
we're starting to get later into the release candidates.

That said, it's not like rc3 is somehow unmanageably large or that
anything particularly scary has happened.  I'd have *liked* for it to
be smaller, but I always do.  And nothing particularly nasty stands
out here.

The bulk here is drivers (net, scsi, sound, crypto. ) and ARM DT
stuff, but there's the usual randon stuff too, with arch updates
(pa-risc, more ARM, x86) and some filesystem and networking updates.

Go wild.

You've done something to the git tree since your test request, because
now the signed tag isn't there any more, now it's just a regular
branch.

Mind re-uploading the *tag*, which is where the signature is?

              

Ahh. And that was actually what you had done the first time around for
your test too - but back then I didn't actually *merge* it. Then I
just looked at the top commit and said "yup, it's signed".

And now that I tried to merged it, I noticed my merge commit didn't
get the signage I expected.

And that only happens if I pull a tag that's signed (which also gets
me the message I expect from signed pulls). 

Anyway, now I have both a signature on the merge and on your actual
commit. Thanks,
Unlikely to be faster, but perhaps more robust and more portable. Maybe.

And the reason we use the int3-based approach is that doing TLB
shootdowns of kernel mappings is completely out of the question, not
to mention not portable (kernel code may not even be in a page table
to begin with).

Remember: the current text_poke is all about kernel code. So the rules
for user space may well be entirely different.

           

Looks completely broken. Where do you guarantee that it's just a single page?

Yes, on x86, UPROBE_SWBP_INSN_SIZE is a single byte. But quite
frankly, on x86, exactly *because* it's a single byte, I don't
understand why we don't just write the damn thing with a single
"put_user()", and stop with all the idiotic games. No need to
invalidate caches, even, because if you overwrite the first byte of an
instruction, it all "just works". Either the instruction decoding gets
the old one, or it gets the new one. We already rely on that for the
kernel bp instruction replacement.

And on non-x86, UPROBE_SWBP_INSN_SIZE is not necessarily 1, so it
could cross a page boundary. Yes, many architectures will have
alignment constraints, but I don't see this testing it.

Whatever. I think that code is bad, and you should feel bad. But hey,
I think it was pretty bad before too.
I think the TLB shootdown should guarantee that it's ok on other
CPU's, since that's basically what we do on mmap.

But looking closer at this, I think I see why the old code did what it
did. I think it's breaking shared mmap pages on purpose rather than
dirtying them. Which is probably the right thing to do.

              

Note that it's not just about the "atomically written", it's also
about the guarantee that it's atomically *read*.

x86 can certainly atomically write a 4-byte instruction too, it's just
that there's no guarantee - even if the instruction is aligned etc -
that the actual instruction decoding always ends up reading it that
way. It might re-read an instruction after encountering a prefix byte
etc etc. So even if it's all properly aligned, the reading side might
do something odd.
Please do it this way instead, because user space will get the
callback version wrong and then - because it never actually triggers
in practice in normal situations - it will cause very *very* subtle
bugs that we can't fix.

Making the kernel serialize the accesses is the right thing to do.
Just a new per-mm mutex should trivially do it, then you don't even
have to check the "current->mm == bp_target_mm" thing at all, you just
make the bp handler do a simple

and return. All done.

Plus the callback thing is pointless if we can do the instruction
switch atomically (which would be true for UP, for single-thread, and
potentially for certain sizes/alignments coupled with known rules for
particular micro-architectures). So it's not a particularly good
interface anyway.

Btw, I also think that there's a separate problem wrt shared pages.
Should we perhaps only allow this in private mappings? Because right
now it has that "current->mm == bp_target_mm" thing, and generally it
only works on one particular mm, but by using "get_user_pages_fast(,
1,. )" it really only requires write permissions on the page. So it
could be shared mapping, and I could easily see people doing that on
purpose ("open executable file, then use text_poke() to change it for
this architecture") and THAT DOES NOT WORK with the current patch if
somebody else is also running that app. 

If we just care about a core sync, then:

 - on_each_cpu() is overkill, since you really only want each CPU that
can run that process.

   And we have that bitmask already: it's the same bitmask that we use
for TLB flush purposes.

 - calling "sync_cores()" cross-cpu is overkill, since the IPI will
already sync the other cores. And the current core is already going to
be synchronized wrt the code change, since we're in kernel mode and
the code is in user mode.

So I actually think your patch does too much - although it's possible
that we should have a system call argument saying "sync just this
process" or "sync all cores" so that people can literally do some
"modify global instruction in shared library" games if they really
really want to.

That said, the thing I really do *not* like about this patch is that
it makes the "insert 'int3' instruction" visible to user space. That
makes the "global instruction" replacement impossible, for example,
because while we'd sync all cores, we have no way to protect against
other processes getting the breakpoint exception and just SIGSEGV'ing
randomly as a result of *that*.

And even if we say "well, we don't care about the global case" (which
is quite possibly the right thign to do), it's actually hard for
threaded libraries to sanely catch SIGSGV for the breakpoint case too.
And since the only reason for this existing IN THE FIRST PLACE is the
threaded case, I have to say that I absolutely despise this "simpler"
interface.

The interface may be simpler, but it's garbage.

I didn't like the first patch either, but the first patch with
"replace the stupid pseudo-signal back-call with just waiting" at
least seems to be a good interface. Unlike this kind of stuff.

Oh, I agree. The interface of the original patch was just inane/insane.

The timeout and the callback is pointless. The only thing the system
call should get as an argument is the address and the replacement
instruction.  So

  int text_poke(void *addr, const void *opcode, size_t len)

sounds fine to me. And it would do:
 - take some (possibly per-mm) mutex
 - write the one-byte int3
 - do the IPI
 - write the other bytes
 - do the IPI
 - do the first byte
 - release the (possibly per-mm) mutex

and then in the BP handler we'd just take the mutex, see if the first
byte of the exception is still int3, if it's not, just return silently
(because that means that we hit the race).

And I would seriously suggest just open-coding the above simple
sequence instead of trying to force-reuse the text_poke_bp() function
we already have. Because I think doing this on kernel code is
*very*different* (for irq reasons _and_ for IPI mask reasons).

Hmm? It doesn't sound too bad. And I really don't see the point of
some timeout handling or anything like that.

That's just moronic. People would make up totally random timeouts, so
from an interface standpoint it's just horrid, horrid.

Giving user space random knobs that you don't understand yourself, and
the monkeys in user space are guaranteed to mis-use is just entirely
the wrong thing to do.

Much better to then just making the interface itself be about
batching, which isn't as hard as you make it out to be. Make it an
array of those addr/replace/len things. And we have that
"restart_block" for system calls, and we'd limit batching to some
random smallish number ("128 instructions, just because"), while still
being easily interruptible in between those blocks. That limits you to
two IPI's per 128 instructions replaced - and at that point even
*that* is just an internal kernel random tuning thing, not some insane
user interface.

But is such batching really even worth it? If' it's not *that* much
more effort, maybe it's worth it, but do we have known users that
really would have thousands and thousands of cases all at once?So sysexit/sysret doesn't count as a serializing instruction, no. But
it doesn't need to, because *self*-modifying code doesn't need a
serializing instruction, only a branch. It's only *cross*-modifying
code that needs a serializing instruction.

So the IPI is sufficient for the cross-modifying case, and the sysret
is sufficient for the self-modifying case. And we also don't need to
worry about "what happens if we schedule to another CPU, and
self-modifying becomes cross-modifying", because the scheduling will
then do the serializing instruction.

So IPI for other CPU's (limited to the mm-mask) and just a system call
for local CPU should be perfectly fine.

Well, with the modern logic ("only wait for async probing if the
module itself did async probing") the ahci and svw modules didn't
really change any behavior.

But other modules did. I wonder, for example, if people insmod the dm
module, and expect all devices to exist afterwards. Which the old
logic of "we always wait for all async code regardless of whether we
started it ourselves" would do, but the new logic does not.

Something similar might hit the (non-modular) md auto-detect ioctl.

So maybe we should just special-case those two issues, and say "let's
just wait for async requests here"

Something like the appended (whitespace-damaged) diff. Does that make
a difference to you guys? And if it does, can you check *which* of the
two async_synchronize_full() calls it is that matters for your cases?

Hmm. The initcall debugging doesn't actually show any of the "wait for
async events", because those debug messages come from
"do_one_initcall()", and the waiting happens later. Plus your messages
don't actually show where you are trying to - and failing - to mount
the root filesystem.

Without that kind of information, it's kind of hard to guess. Maybe
you could add a few printk's to your kernel? Add one to
do_init_module() *after* the

thing, and another to fs/namespace.c: do_mount() (just put something like

or whatever, so that we can see when that happens. 

Or not. Rather, it would make things much worse. The virtual address
is much simpler and better to avoid needing any page table lookup etc
crap. The key is just the mm and the virtual address, and no silly
page table walks etc necessary.

Of course, I have no idea if people are properly using the private
futexes. glibc _should_ use them, but who the heck knows. 

Yeah, pthread mutexes seem to do it. Sadly we don't do it for
mm_release(), so the case of clear_child_tid doesn't trigger it, and
that happened to be what I tested (I did "git diff" with the threaded
pre-population of the stat data).  And we can't change that because
it's ABI. Sad.

So we do have a couple of corner cases where we *could* use the
private ones but don't. But I guess that thread creation/exit is
heavy-weight enough that that particular one doesn't really matter.

What you think in this case doesn't really matter, does it? There are
actual facts, and then there is your thinking, and guess which one
matters?

Peter is absolutely correct, and has shown remarkable restraint trying
to explain it to you. The fact is, the x86 bitop instructions act on a
signed index. Making the index be "unsigned long" would violate the
actual *behavior* of the function, so it would be singularly stupid.

Talking about "ideal implementation" is also singularly stupid.
There's this fascinating thing called "reality", and you should try to
re-aquaint yourself with it.

Don't bother replying to this thread.
Your thing is *very* consistent, it's once more four bytes into that
pipe-info. And it's once more that exact same "increment second word
in the allocation" pattern.

But the fact that it goes away when you enable other debug options is
really really annoying. It's consistent only if you don't have the
options on that might help us debug it further. Damn.
So the thing is, with slub debugging, slub shouldn't be merging
different slab caches.

HOWEVER.

The pipe-info structure isn't using its own slab cache, it's just
using "kmalloc()". So it by definition will merge with all other
kmalloc() allocations of the same size (or, to be exact, of "similar
enough size to hit the same size bucket"). In your case it's the
192-byte-sized bucket.

But all the debugging code talks purely about pipe_info allocations -
both the previous kmalloc/kfree _and_ the kmalloc() that actually sees
the slub debugging error. So if it's mixing with something else, I'm
not seeing what that would be. It would have to be an older allocation
(as it "it got re-allocated to a pipe in between") or another type
that was the similar size.

Which doesn't look all that likely. Not when your problems are so
consistent, and seem to be *always* about that pipe_inode_info.

But dammit, that it such a simple set of allocations. I still don't
see how they could be to blame. And if it's some suspicious access to
the pipe mutex (that second word is still the "wait_lock" spinlock in
the pipe inode mutex) I really would have expected the mutex debugging
to have screamed loudly. Or the DEBUG_PAGEALLOC.

I'm really not very happy with the whole pipe locking logic (or the
refcounting we do, separately from the "struct inode"), and in that
sense I'm perfectly willing to blame that code for doing bad things.
But the fact that it all goes away with debugging makes me very very
unhappy.

Something like the attached patch (which also enables debugging just
for that slab).

I still don't see what could be wrong with the pipe_inode_info thing,
but the fact that it's been so consistent in your traces does make me
suspect it really is *that* particular slab. But I dunno, and this
would just confirm it.
Oh, damn, I thought you had found it, and got very excited and already
wrote a long email about things I wanted you to try. And then I
started looking closer. 

That test is wrong. Both of those fields are 32-bit, so testing them
against 0x6b/0x6c is bogus: you're just catching real cases. The
reason it catches omreport is presumably because omreport runs as some
special user that happens to have uid 107 (on my machine that happens
to be qemu). And having a usage count of 108 isn't particularly
strange either - creds get a lot of re-use.

So close. It *might* still be one of those cases, but it doesn't
really sound very likely. "bi_cnt" is deep inside the struct bio, and
"usage" is at offset 0, not offset 4. And the ns_fh isn't very
interesting.

Al, I really hate the "pipe_lock()" function that tests for
"pipe->files" being non-zero. I don't think there are any valid cases
where pipe->file ever *could* be zero, and if there are, they are
fundamentally racy.

Is there really any reason for that test? It used to test for
"pipe->inode" and that kind of made sense as the pipe didn't even have
a lock (it re-used the inode one). But these days that test makes zero
sense, except as a "don't even bother locking if this is some fake
internal pipe", but is that even valid?

Also, why does "pipe_release()" still use i_pipe. I'd much rather it
used "file->private_data" like everything else (and then it
unconditionally clear it). We are, after all, talking about releasing
the *file*, and we shouldn't be mixing up that inode in there.

IOW, what is wrong with the attached patch? Then we could/should

 - make the free_pipe_info() happen from the drop_inode()

 - delete pipe->files counter entirely because it has no valid use

Hmm?

We're back on the normal weekly schedule, although I'm planning on
eventually slipping back to a Sunday release schedule. In the
meantime, it's like Xmas every Friday! Or, perhaps more appropriately
for the date, Hanukkah.

Anyway, everything looks normal for an -rc2. Nothing hugely alarming,
lots of small fixes all over, and the statistics look normal too (just
over half drivers, the rest is one third arch updates, one third
Documentation, and one third misc - filesystems, kernel, crypto. )

Nothing particularly stands out. If you thought rc1 is too scary to
test, jump on in now. It's all good,

Didn't this already get fixed by the reverts I did and Mimi acked?

Commits 4c1cc40a2d49 ("Revert "KEYS: verify a certificate is signed by
a 'trusted' key"") and 34ef7bd3823b ("Revert "ima: define '_ima' as a
builtin 'trusted' keyring"") to be exact.

It *does* seem to be hung in some congestion thing. Possibly brought
on by being low on memory due to a memory leak.

I'm not seeing any suspicious locks: the sshd that holds the mmap_sem
locks for a fork() seems to be in congestion-wait for the disk due to
trying to allocate memory, exactly like all the trinity children are.

You don't have memory information in your dump, but to me it looks
like you're basically out of memory (there's a *lot* of
trinity-children trying to allocate pages), and the oom killer isn't
triggering for whatever reason.

Of course, it could easily be a kernel memory leak too (rather than
trinity), triggered by your odd workload.Hmm. Why? Afaik, you only care about "empty or not". And if you don't
need the serialization from locking, then afaik you can just do a
"plist_head_empty()" without holding the lock.

NOTE!

The "list_empty()" function is very much designed to work even without
holding a lock (as long as the head itself exists reliably, of course)
BUT you have to then guarantee yourself that your algorithm doesn't
have any races wrt other CPU's adding an entry to the list at the same
time. Not holding a lock obviously means that you are not serialized
against that.  We've had problems with people doing

because they wouldn't wake people up who just got added.

But considering that your atomic counter checking has the same lack of
serialization, at least the plist_head_empty() check shouldn't be any
worse than that counter thing.  And doesn't need any steenking atomic
ops or a new counter field.This would seem to be a nicer approach indeed, without needing the
extra atomics.

Davidlohr, mind trying Thomas' approach?

So this code sequence still makes me very unhappy.

Why does not just a simple unconditional

work for the "not registered with sysfs" case? And if the sysfs code
really gets confused, why not

(btw, looking at the sysfs code, this looks *very* suspicious in
sysfs_remove_dir():

and I would suggest that "sd = kobj->sd" should be done under the
lock, because otherwise the lock is kind of pointless. )

Greg?No,while it is true that I've been traveling, I've also been actively
merging, and no, that pull request isn't lost.

I don't really like the look of it though (particularly all the magic
new keys changes), so I've been delaying it and merging other regular
stuff that I don't have any issues with. I'm leaving that pull for
later in order to go over it more carefully when I'm not doing fifteen
other pulls the same day. 

If somebody wants to explain about the rationale new keys code, that might help.Ok, I've finally pulled this - at least tentatively (ie I have it on
my machine, it's going through allmodconfig test-builds and I'll test
booting asap, but I expect all that to succeed, and will push out the
merge if/when it does so).

However, I have to say, that pulling this was more than a little
annoying. I would really have preferred seeing the key subsystem
changes in a separate branch, to make it easier for me to do the
"normal updates" first, and then do just the big changes separately.
The key subsystem changes had absolutely _nothing_ in common with the
rest of the security subsystem changes afaik.

The fact that they got mixed up also made it all messier to go
through, since there were (two sets of) fixes after the internal
merges of the other security stuff - if the key subsystem changes had
been a branch of its own, the fixes would have remained on that branch
too.

So for future cases: if there are big independent overhauls of core
subsystems, I'd really like to see them kept separate, ok?

Also, David, with (for example) 1700+ new lines in lib/assoc_array.c,
I would really *really* like code like this to have some review by
outsiders. Maybe you did have that, but I saw absolutely no sign of it
in the tree when I merged it. That code alone made me go "Hmm, where
did this come from, how was it tested, why should I merge it?".

I do see *some* minimal comments on it from George Spelvin on lkml. I
don't see any sign of that in the commit messages, though. I'd also
see no comments on where the algorithm came from etc. It clearly has
subtle memory ordering (smp_read_barrier_depends() etc), so I'm
assuming it is based on something else or at least has some history to
it. What is that history?

In short: this is exactly the kind of thing I *don't* enjoy merging,
because the code that needed review was

 - mixed up with other changes that had nothing to do with it
 - had no pointers to help me review it
 - had zero information about others who had possibly reviewed it before

and quite frankly, without the extended explanation of what the
changes were for (which I also didn't get initially), I'd probably
have decided half-way that it's not worth the headache.

In the end, the code didn't look bad, and I didn't find any obvious
problems, but there's very much a reason why I merged this over two
weeks after the first pull request was originally sent to me.

So guys, make it easier for me to merge these kinds of things, and
*don't* do what happened this time. Ok? Pretty please?

. 

So by now, the changes are hopefully smaller and general "updates"
rather than big new features. At that point, I don't care too deeply.
It's the "this is a whole new big way of doing things, and there's
fundamental new core code" that I would really like to be able to see
separately in order to make it much easier for me to pull and walk
through independently of the normal "updates to subsystem" stuff.
Quite frankly, I just want to *know* about things like this, there
doesn't have to be some fixed format for it.

It can be part of the pull request, explaining where the code comes
from, how it's been tested, who has looked at it etc etc. Or it can be
just free-form in the commit messages.

And again, this - for me - is about core code that isn't deeply
embedded in some internal subsystem. So the reason I reacted to the
assoc_array.c thing is that it was clearly set up to be generic, and
it does end up being pretty different and affects "normal" users. So I
go off looking for "ok, where can I find this discussed" for doing
some minimal due diligence on a new set of core code, and I find very
little on lkml, and nothing else. And the commit message talks about
what it does, but not about the "who looked at it", so I basically had
nothing to judge it by except just looking at the code.

Which didn't look horrible, but it really would have made me happier
hearing about people looking it over, and about you testing it in user
space etc etc.A "reviewed-by" line is fine, but so is really just free-form explanations too.

Not everything has to be a "xyz-by:" thing, especially things that are
more important for one-time use at merge time rather than anything
"this is a common pattern and people might want to do statistics about
it over time".

It's too late for this case, and for all I know there are no similar
cases coming up in the security layer, but when I have something that
ends up being pending for a two weeks, I want to at least try to
educate people about *why* it was pending, and what I found difficult
about the process. Maybe it happens again, maybe it won't. But if it
does, we'll have at least spoken about the issues.

In theory, you can use notes, but I don't actually like it either
technically _or_ from a "flow of information" standpoint, so I'd much
rather see people try to note things in the commit messages, and then
for the merge itself try to have explanations in the "please pull"
message.So you had an extra week to prepare your pull requests, and if you
were planning on sending it in the last two days thinking I'd close
the merge window on Sunday as usual, I can only laugh derisively in
your general direction, and call you bad names. Because I'm not
interested in your excuses. I did warn people about this in the 3.12
release notes. As it was, there were a few people who cut it fairly
close today. You know who you are.

If there are pull requests I missed (due to getting caught in spam
filters, or not matching my normal search patterns), and you think you
sent your pull request in time but it got overlooked, ping me -
because I don't have anything pending I know about, but mistakes
happen.

Talking about mistakes.  I suspect it was a mistake to have that
extra week before the merge window opened, and I probably should just
have done a 3.12-rc8 instead. Because the linux-next statistics look
suspicious, and we had extra stuff show up there not just in that
first week. Clearly people took that "let's have an extra week of
merge window" and extrapolated it a bit too much. Oh, well. Live and
learn.

Anyway, other than that small oddity, this was a fairly normal merge
window. By patch size we had a pretty usual ~55% drivers, 18%
architecture code, 9% network updates, and the rest is spread out (fs,
headers, tools, documentation). Featurewise, the big ones are likely
the nftables and the multi-queue block layer stuff, but depending on
your interests you might find all the incremental updates to various
areas interesting. There are some odd ones in there (LE mode Powerpc
support. )

Go forth and test, and start sending me regression fixes. And really,
if you didn't send me your pull request in time, don't whine about it.
Because nobody likes a whiner.

Shortlog of merges appended. The real shortlog is much too big to be
readable, as always for rc1.

No, it didn't miss my cutoff, and it wasn't even eaten by the gmail
spam filters, but it *had* missed my normal pattern for checking that
I had merged all git pull requests, which is why I had overlooked it.

Pulled now (going through the allmodconfig build test before being
pushed out shortly).

Btw, the reason it missed my normal check is that I search for
"in:inbox git pull" as a sanity check that I don't have anything
pending. Now, *most* of the time I tend to catch things regardless
just by virtue of reading email carefully, but when I'm busy it
definitely helps if your email matches that pattern.

The easiest (and most common) way to do that is to have the subject
line be something like

    [GIT PULL] sound fixes #2 for 3.13-rc1

but there are other patterns. For example, David Miller tends to have
instead a subject line like

    [GIT] Networking

and then "Please pull" in the body of the email. So the "in:inbox git
pull" thing basically catches pretty much everybody.

Except for you. 

You use David Miller's subject line, but you don't have the "please
pull" part, so your email missed my "did I pull everything that I had
pending" check.

For very similar reasons, for people sending me a patch, it really
*really* helps if there's a "[patch]" (possibly with a n/m number
pattern) in the subject line.   Or make sure you send me a git diff,
because I also tend to search for the string "diff --git". Of course,
the main person who sends me patches is Andrew Morton, so I mostly
just search for "from:akpm" :)

Yeah, I think we default to a 10% "dirty background memory" (and
allows up to 20% dirty), so on your 16GB machine, we allow up to 1.6GB
of dirty memory for writeout before we even start writing, and twice
that before we start *waiting* for it.

On 32-bit x86, we only count the memory in the low 1GB (really
actually up to about 890MB), so "10% dirty" really means just about
90MB of buffering (and a "hard limit" of ~180MB of dirty).

And that "up to 3.2GB of dirty memory" is just crazy. Our defaults
come from the old days of less memory (and perhaps servers that don't
much care), and the fact that x86-32 ends up having much lower limits
even if you end up having more memory.

You can easily tune it:

    echo $((16*1024*1024)) > /proc/sys/vm/dirty_background_bytes
    echo $((48*1024*1024)) > /proc/sys/vm/dirty_bytes

or similar. But you're right, we need to make the defaults much saner.

Wu? Andrew? Comments?
Right. The percentage notion really goes back to the days when we
typically had 8-64 *megabytes* of memory So if you had a 8MB machine
you wouldn't want to have more than one megabyte of dirty data, but if
you were "Mr Moneybags" and could afford 64MB, you might want to have
up to 8MB dirty!!

Things have changed.

So I would suggest we change the defaults. Or pwehaps make the rule be
that "the ratio numbers are 'ratio of memory up to 1GB'", to make the
semantics similar across 32-bit HIGHMEM machines and 64-bit machines.

The modern way of expressing the dirty limits are to give the actual
absolute byte amounts, but we default to the legacy ratio mode. Nothing currently actually *sets* the BDI_CAP_STRICTLIMIT flag, though.

So it's a potential fix, but it's certainly not a fix now.It definitely doesn't work. I can trivially reproduce problems by just
having a cheap (==slow) USB key with an ext3 filesystem, and going a
git clone to it. The end result is not pretty, and that's actually not
even a huge amount of data.

I'm not sure this has ever worked, and in the last few years the
common desktop memory size has continued to grow.

For servers and "serious" desktops, having tons of dirty data doesn't
tend to be as much of a problem, because those environments are pretty
much defined by also having fairly good IO subsystems, and people
seldom use crappy USB devices for more than doing things like reading
pictures off them etc. And you'd not even see the problem under any
such load.

But it's actually really easy to reproduce by just taking your average
USB key and trying to write to it. I just did it with a random ISO
image, and it's _painful_. And it's not that it's painful for doing
most other things in the background, but if you just happen to run
anything that does "sync" (and it happens in scripts), the thing just
comes to a screeching halt. For minutes.

Same obviously goes with trying to eject/unmount the media etc.

We've had this problem before with the whole "ratio of dirty memory"
thing. It was a mistake. It made sense (and came from) back in the
days when people had 16MB or 32MB of RAM, and the concept of "let's
limit dirty memory to x% of that" was actually fairly reasonable. But
that "x%" doesn't make much sense any more. x% of 16GB (which is quite
the reasonable amount of memory for any modern desktop) is a huge
thing, and in the meantime the performance of disks have gone up a lot
(largely thanks to SSD's), but the *minimum* performance of disks
hasn't really improved all that much (largely thanks to USB ;).

So how about we just admit that the whole "ratio" thing was a big
mistake, and tell people that if they want to set a dirty limit, they
should do so in bytes? Which we already really do, but we default to
that ratio nevertheless. Which is why I'd suggest we just say "the
ratio works fine up to a certain amount, and makes no sense past it".

Why not make that "the ratio works fine up to a certain amount, and
makes no sense past it" be part of the calculations. We actually
*hace* exactly that on HIGHMEM machines, where we have this
configuration option of "vm_highmem_is_dirtyable" that defaults to
off. It just doesn't trigger on nonhighmem machines (today: "64-bit").

So I would suggest that we just expose that "vm_highmem_is_dirtyable"
on 64-bit too, and just say that anything over 1GB is highmem. That
means that 32-bit and 64-bit environments will basically act the same,
and I think it makes the defaults a bit saner.

Limiting the amount of dirty memory to 100MB/200MB (for "start
background writing" and "wait synchronously" respectively) even if you
happen to have 16GB of memory sounds like a good idea. Sure, it might
make some benchmarks a bit slower, but it will at least avoid the
"wait forever" symptom. And if you really have a very studly IO
subsystem, the fact that it starts writing out earlier won't really be
a problem.

After all, there are two reasons to do delayed writes:

 - temp-files may not be written out at all.

   Quite frankly, if you have multi-hundred-megabyte temptiles, you've
got issues

 - coalescing writes improves throughput

   There are very much diminishing returns, and the big return is to
make sure that we write things out in a good order, which a 100MB
buffer should make more than possible.

so I really think that it's insane to default to 1.6GB of dirty data
before you even start writing it out if you happen to have 16GB of
memory.

And again: if your benchmark is to create a kernel tree and then
immediately delete it, and you used to do that without doing any
actual IO, then yes, the attached patch will make that go much slower.
But for that benchmark, maybe you should just set the dirty limits (in
bytes) by hand, rather than expect the default kernel values to prefer
benchmarks over sanity?

Suggested patch attached. Comments?

Yes. Most people will use the defaults, but there will always be
people who tune things for particular loads.

In fact, I think we have gone much too far in saying "all policy in
user space", because the fact is, user space isn't very good at
policy. Especially not at reacting to complex situations with
different devices. From what I've seen, "policy in user space" has
resulted in exactly two modes:

 - user space does something stupid and wrong (example: "nice -19 X"
to work around some scheduler oddities)

 - user space does nothing at all, and the kernel people say "hey,
user space _could_ set this value Xyz, so it's not our problem, and
it's policy, so we shouldn't touch it".

I think we in the kernel should say "our defaults should be what
everybody sane can use, and they should work fine on average". With
"policy in user space" being for crazy people that do really odd
things and can really spare the time to tune for their particular
issue.

So the "policy in user space" should be about *overriding* kernel
policy choices, not about the kernel never having them.

And this kind of "you can have many different devices and they act
quite differently" is a good example of something complicated that
user space really doesn't have a great model for. And we actually have
much better possible information in the kernel than user space ever is
likely to have.Sure. That said, the patch I suggested basically makes the numbers be
at least roughly comparable across different architectures. So it's
been at least somewhat tested, even if 16GB x86-32 machines are
hopefully pretty rare (but I hear about people installing 32-bit on
modern machines much too often).Yes, but then the temp-file is long-lived enough that it *will* hit
the disk anyway. So it's only the "create temporary file and pretty
much immediately delete it" case that changes behavior (ie compiler
assembly files etc).

If the temp-file is for something like burning an ISO image, the
burning part is slow enough that the temp-file will hit the disk
regardless of when we start writing it.

Sure. And I think that if you have a big database, that's when you do
end up tweaking the dirty limits.

That said, I'd certainly like it even *more* if the limits really were
per-BDI, and the global limit was in addition to the per-bdi ones.
Because when you have a USB device that gets maybe 10MB/s on
contiguous writes, and 100kB/s on random 4k writes, I think it would
make more sense to make the "start writeout" limits be 1MB/2MB, not
100MB/200MB. So my patch doesn't even take it far enough, it's just a
"let's not be ridiculous". The per-BDI limits don't seem quite ready
for prime time yet, though. Even the new "strict" limits seems to be
more about "trusted filesystems" than about really sane writeback
limits.

Fengguang, comments?

(And I added Maxim to the cc, since he's the author of the strict
mode, and while it is currently limited to FUSE, he did mention USB
storage in the commit message. ).

Not my experience. It may be true for some really cheap devices, but
normal USB keys seem to just get really slow, probably due to having
had their flash rewrite algorithm tuned for FAT accesses.

I *do* suspect that to see the really bad behavior, you don't write
just one large file to it, but many smaller ones. "git clone" will
check out all the kernel tree files, obviously.

                     I don't understand why you say that.

The ordering of the access to the asm("sp") register is totally
irrelevant. You are "correct" in saying that the compiler is within
its right to re-order them, but that is the worst kind of correct:
it's totally immaterial. In fact, we *want* the compiler to not just
re-order the accesses to %sp, but to notice that it can combine them,
and do CSE on that whole expression when it is used multiple times
within the same function (like it often is used).

So the compiler can very much decide to re-read %sp all it wants, and
re-order those reads all it wants, and that's not the bug at all.
Putting a clobber or a volatile on it would disable the optimization
we *want* to happen.

So don't bark up the wrong tree.

The bug seems to be that gcc re-orders the *memory* *accesses* through
that point, which is not correct in any way, shape, or form. If we
have a write to a memory location followed by a read of the same
memory location, the compiler ABSOLUTELY MUST NOT RE-ORDER THEM. The
write obviously changes the value of the read.

It seems that some gcc alias analysis completely incorrectly thinks
that they are not the same memory location, and do not alias. My guess
would be that gcc sees that that they are based on the stack pointer
with "different" offsets, and decides that the memory locations must
be different - without noticing that the "& ~(THREAD_SIZE - 1)" will
end up generating the same address for both of them.

There may be some insane "two different objects on the stack cannot
alias" logic, which is true for *objects* on the stack, but it sure as
hell isn't true for random accesses through asm("sp").

If I read this thread correctly, you're all talking about something
else than the actual bug, and are trying to say that there is
something wrong with re-ordering the access to %sp itself. Missing the
_real_ bug entirely. See above.No, because it is immaterial.

We *want* gcc to optimize away multiple accesses to "sp". Because it
doesn't *matter* whether "sp" changes or not, the *result* is always
the same. That's what the "const" means.

The "& ~(THREAD_SIZE - 1)" part will remove all the bits that can
change. Really. So the result *is* constant (within one thread).
Marking it constant and telling gcc that it can combine these things
is correct.

Guys, read my email again.

The bug is not that gcc can re-order or combine the accesses to "sp".
WE WANT THAT TO HAPPEN.

The bug is *outside* that "current_thread_info()" macro/inline
function. It's the *dereference* of the pointer that gcc re-orders.
AND THAT IS WRONG.

Gcc seems to mess up the alias analysis, and decide that the
deferences cannot alias. Which is wrong. They clearly *can* alias,
exactly because the value of "sp & ~(THREAD_SIZE - 1)" ends up having
the same value all the time.
Why? You're wrong.

I mean, anybody who disagrees with me is pretty much wrong just on
pure principles, but I actually mean a deeper kind of wrong than that.
I mean a very objective "you're clearly wrong".

.  and then you use a totally bogus example to try to "prove" your point.

The example you use has nothing to do with what the function in case
actually does.

The function we actually talk about RETURNS THE SAME VALUE EVERY TIME
IT IS CALLED, regardless of arguments (which it doesn't happen to
have). Ergo, it is "const".

Your example is pure and utter shit, since you still get confused
about what is actually const and what isn't.

The fact is, "current_thread_info()" returns a constant value (within
that execution context). Always has, always will. It fundamentally HAS
to, since that is - by definition - what that "thread info" is all
about.

The dereference (your "->somewhere_local") happens *outside* that
const function scope. And no, that dereference is not const. But
that's not what we tell gcc, and never has been.

So don't try to confuse things by trying to make
"current_thread_info()" something it isn't.

Basically, your whole argument boils down to "if the function did
something else than what it does, then it wouldn't be const, so we
shouldn't mark it const". But that argument is BULLSHIT, because the
fact is, the function *doesn't* do what you try to claim it does.

Quite frankly, I got maybe ten patches into this series, at which
point I just threw my hands up and said: "This is too ugly to live".

The naming in fs/iov-iter.c is disgusting. :ii_iov_xyz? WTF?

Random "flag" value for marking things atomic? F*ck me, that's ugly.

A separate phase for checking addresses instead of just doing it in
the loop that loops over iovec's? Why? It sure as hell isn't because
it's more efficient, and it doubly sure as hell isn't because it's
prettier.

At that point, I just couldn't take it any more.

I really don't see the point of all this crap. All this for the loop
driver? If so, it had better at least be prettier than it is.

Just to clarify, I think it might be fixable. But it does need fixing,
because I really feel dirty from reading it. And I may not care all
that deeply about what random drivers or low-level filesystems do, but
I *do* care about generic code in mm/ and fs/, so making those iovec
functions uglier makes me go all "Hulk angry! Hulk smash" on the code.

The whole "separate out checking from user copy" needs to go away.
There's no excuse for it.

The whole "if (atomic) do_atomic_ops() else do_regular_ops()" crap
needs to go away. You can do it either by just duplicating the
function, or by having it use a indirect function for the copy (and
that indirect function acts like copy_from/to_user() and checks the
address range - and you can obviously then also have it be a "copy
from kernel" thing too if you want/need it). And no, you don't then
make it do *both* the conditional *and* the function pointer like you
did in that discusting commit that mixes the two with the struct
iov_iter_ops).

The "__" versions that don't check the user address range needs to die entirely.

The whole crazy "ii_iov_xyz" naming needs to go away. It doesn't even
make sense (one of the "i"s is for "iov".

That "unsigned long data" that contains an iovec *? WTF? How did that
ever start making sense?

IOW, there are many many details that just make me absolutely detest
this series. Enough that there's no way in hell I feel comfortable
pulling it. But they are likely fixable.Ok, it looks straightforward enough to just replace the kmalloc/kfree
with using a slab allocation using the page_ptl_cachep pointer. I'd do
it myself, but I would like to know how it got lost? Also, much
testing to make sure the cachep is initialized early enough.

Or should we just revert the commit that added the pointless/unused
slab pointer?

Andrew, Kirill, comments?

Also note the other issue Al found: see commit 2a46eed54a28 ("Wrong
page freed on preallocate_pmds() failure exit") that I just pushed
out.
Ahh. That was the one I thought was broken, but yes, while the upper
bits of %rcx are calculated and not zeroed, they end up not actually
getting used. So yeah, I'll believe it's correct.

               
At least gcc-4.8.2 generates that instruction (try multiplying an
integer value by 9), so I guess gas will be happy. Except gcc uses
"leal", so to be safe. 

I don't have a huge preference, but I hate the current situation (with
Fenghua's patch) where it's not consistent. One path uses just 32-bits
of the count (thanks to the "mov %edx,%ecx") while another path uses
64 bits.

One or the other, but not a mixture of both.

And only tangentially related to this: I do think that we could be
stricter about the count. Make it oops if the high bits are set,
rather than overwrite a lot of memory. So I would not be adverse to
limiting the count to 31 bits (or even less) explicitly, and thus
making the while 32-vs-64 bit issue moot.

This seems to be just pure stupid.

Why the hell should we ever ask the user whethr they want to force
CPUMASK offstack?

Seriously, it only leaves room for mistakes and stupidities. There is
no reason for any normal use-case to ask the user about this. Even the
help message is pure and utter garbage ("This is a bit more expensive,
but avoids stack overflow").

The fact is, an on-stack CPUMASK is *smaller* than a pointer to an
offstack one when NR_CPU is small. The question makes no sense then.
And when NR_CPU is huge, the question makes no sense _either_, since
we can't have the cpumasks on stack.

Asking the user questions that make no f*cking sense to ask is stupid.
And I'm not knowingly pulling stupid crap.

Agreed. And we could even decide to ask somewhere in that range.

I think 128 bits is still safely "don't bother with an external
pointer" (it's just two words, it's like a "struct list_head" -
there's no way that should be unsafe on the stack). Once we get to 256
bits I start going "Hmm, that's 32 bytes, maybe an external allocation
makes sense. "

I'd personally put the cut-off point at just keeping it on-stack if
it's smaller than or equal to 256. But I agree that at that point it's
really just a judgement call.

That one looks broken.

Well, it looks like it might "work", but do so by hiding the issue for
one case, while leaving it in the more general case.

Why does it do that

    up_read(&tty->termios_rwsem);
    tty_flush_to_ldisc(tty);
    down_read(&tty->termios_rwsem);

only if TTY_OTHER_CLOSED is set? If flushing the ldisc can generate
more data, then we should do it *unconditionally* when we see that we
currently have no data to read.

As it is, it looks like the patch fixes the TTY_OTHER_CLOSED case
("read all pending data before returning -EIO"), but it leaves the
same broken case for the O_NONBLOCK case and for a hung up tty.

The O_NONBLOCK case is presumably just a performance problem (the data
will come at _some_ point later), but it just looks bad in general.
And the tty_hung_up_p() looks actiely buggy, with the same bug as the
TTY_OTHER_CLOSED case that the patch tried to fix.

Hmm? Am I missing something?

The code is a bit confusing in *other* ways too: if you look later, it
does this:

    n_tty_set_room(tty);

which is documented to have to happen inside the termios_rwsem.
HOWEVER, what does that do? It actually does an _asynchronous_
queue_work() of &tty->port->buf.work, in case there is now more room,
and the previous one was blocked. And guess what that workqueue is all
about? Right: it's flush_to_ldisc() - which is the work that
tty_flush_to_ldisc() is trying to flush. So we're actually basically
making sure we've flush the previous pending work.

So even the tty_flush_to_ldisc(tty) that gets done in that patch is
not necessarily sufficient, because the work might not have been
scheduled because the flip buffer used to be full. Then flushing the
work won't do anything, even though there is actually more data. Now,
that is a very unlikely situation (I think it requires two concurrent
readers), but it looks like it might be real.

So I suspect we should *unconditionally* do

     n_tty_set_room(tty);
     up_read(&tty->termios_rwsem);
     tty_flush_to_ldisc(tty);
     down_read(&tty->termios_rwsem);

if we don't have any pending input. And then test input_available_p()
again. And only if we don't have any input after that flushing do we
start doing the whole TTY_OTHER_CLOSED and tty_hung_up_p() tests.

Hmm?Greg,
 what happened to this patch? Is it still waiting in some random tree
of yours, or did it get lost?I don't care deeply - it's marled for stable, but the race it fixes is
so unlikely as to be almost irrelevant, so it's not like I need it
now.

I was just going through emails that came in during travel, and making
sure it's not lost.

So I'll happily forget about it, knowing that it is in your queue.Ok, applied.

What do you think about then just abstracing out that now common sequence 
of re-allocating a larger buffer, while clearing m->count?

IOW, something like the appended. 

Hmm.  Al - this looks like a major oversight, but it also looks like
the wrong place to initialize count/from in, just because it doesn't
follow any sane patterns.

My gut feel is that this needs more cleanup and some sane helper
function that always initializes those fields when allocating a new
buffer. Rather than the "initialize in random places and then miss a
few".

Afaik, those fields currently get (re-)initialized when:

 - We do the memset() of the whole seq_file structure at seq_open() time.

 - at the top of traverse()

 - count (but not from) gets reinitialized when growing the buffer or
after traverse() fails in seq_read()

and it really doesn't give me that happy fuzzy feeling of "that all
makes sense". Charley's patch seems to fix a missing initialization,
but I'd *really* like to have it all make more sense, and feel that
we're not missing some *other* initialization.

Al?
So while resolving some fairly trivial conflicts here, I noticed that
commit a76e9bd89ae7 ("i2c: attach/detach I2C client device to the ACPI
power domain") that I got earlier through the ACPI/PM tree calls
acpi_dev_pm_detach() even when the device "->remove()" function fails.
But it only sets clientdata to NULL if it succeeds.

That looks a bit odd.

I didn't try to fix it, though. I just thought I'd point out the oddity.

                 

It might be interesting to try, and if it used to work with the acpi
method, use bisection to see when it stopped working. 

I agree that a delay of 60s may well be reported as a hang, but at
least for the Dell case I can test, the hang is definitely at least
close to infinite. Definitely longer than a couple of minutes.

So the Sony and Dell issues may be different. That said, vt-d was
suspected for both, and apparently does match your kernel versions, so
it's entirely possible that the fundamental cause is the same even if
the symptoms are slightly different.
Well, that's presumably true of *all* the machines. Because I bet
windows boots on it. The details may matter.

The solution is to act more like Windows. There was some talk about
one likely fundamental difference being in how we enable VT-d. Maybe
we should just change that?

Seriously, if the "fix" is potentially something as simple as
disabling VT-d before reboots, let's just do it. Not add these quirks.
We have people to test a patch, but what _is_ that patch?Is there really any point in having the option for unfair at all?

From your timings, it looks like the unfair locks are more expensive
for the writer side, but since pretty much the whole point of rwlocks
is when readers are the common case, I don't think we care.

And I'm not at all convinced we want the complexity of two different
kinds of rwlocks with different semantics and extra code for said
semantics. 

Your *original* fair rwlocks were unusable, since they didn't allow
for the irq semantics that most users need, but afaik your current
version always makes an irq/bh-context reader work even when the lock
is otherwise trying to be fair, so this whole dual behavior seems to
be largely pointless.

No?
Sure. And we've never cared before.

When we switched over to the ticket spinlocks, we actually had
*numbers* about how the old unfair spinlocks could be faster. And we
still didn't leave a "unfair spinlocks" option. Because it didn't make
sense.

Why would it make sense here?

                
Sorry, but I don't believe in "may be" as an excuse for complexity.
Especially since the "may be" faster performance is often coupled with
"known latency problems due to unfairness".

I'd want numbers for real loads if we really want the extra complexity.

Right now, the real numbers I can point to is in the size of the
patch, and the extra code complexity. Yes, I see the microbenchmark
numbers, but those are pretty much irrelevant to real loads.[ This was in my spam collection. I don't quite know why, but it might
signify problems with your email setup. Quite often, when gmail is
unhappy about kernel developer emails, it's been because their email
provider ends up doing something odd.

But the headers actually have "spf=pass" and "dkim=pass", so it's
nothing obvious. ]

That said, I don't much like the patch either. The "fixed' version
looks worse than the original. If it's an unsigned type, no extra code
will be generated, and if it's a signed type, it's correct. In either
way, the code looks good, and the range test means that people reading
it don't even need to worry about whether the type is signed or not.

If this patch was written because of some f*cking broken compiler
warning, then just tell the compiler to shut the hell up about it.
This is a clear example of where compiler warnings are actually making
things worse.
Ahh. Yes. Googling for "hetzner online spam", and there's a *lot* of
complaints. See for example

   http://www.spamrankings.net/rankv2/2013/09/01/monthly/world/volume/cbl/all/regular/

which puts it #2 _worldwide_ in September. I have no idea how accurate
that is, but. 

I would suggest everybody who uses hetzner actively drop them, and
talk publicly about *why* they drop them. Your business may not be all
that lucrative to them (compared to the spam), but still. 

Not so. You are relevant in the sense that you need to communicate why
people should take patches from you.

I know people who just put you in their kill-files because they can't
bother with the endless argumentative flames you generate. At which
point all the code you produce is kind of irrelevant, because people
won't see it.

Seriously, felipe. You pissed off Junio, who is *hard* to annoy. Just
learn from your mistakes, instead of repeating them over and over.

It's not just code. You need to also explain *why* people should apply
it, and stop the f*cking idiotic arguing every time somebody comments
about your patches.

Because it literally seems to be *EVERY* single time.My point is that I have sixteen pointless messages in my mbox, half of
which are due to just your argumentative nature.

In other words, my point is that you are wasting my - and other
peoples - time just because you *always* have to turn every single
thing into a big argument when somebody comments on a patch.

              Ok, I finally got around to the random tree, but your proposed merge
resolution makes no sense, so I didn't end up applying it.This part undoes your commit 6265e169cd31 ("random: push extra entropy
to the output pools"), and the "entropy_total" field will now never be
non-zero when "r->initialized" is set. So then the rest of your
commit, which looks like this:

can never trigger (because "r->initialized && r->entropy_total >=
2*random_read_wakeup_thresh" is never true).

So your merge resolution diff makes no sense to me.

That said, the networking change seems to be simpler and largely
equivalent to that commit, so maybe you meant to basically castrate
that commit 6265e169cd31? Or maybe the diff you posted was incomplete
and just doesn't show the undone part (because "git diff --cc" will
not show parts that match the other branch).

So I think the correct resolution is to basically take the code that
the networking tree added, and undo the "If the input pool is getting
full. " part of commit 6265e169cd31.

Hmm?

Just verifying that we're on the same page before merging this. I was vacillating whether to do an rc8 or just cut the final 3.12, but
since the biggest reason to *not* do a final release was not so much
the state of the code, as simply the fact that I'll be traveling with
very bad internet connection next week, I didn't really want to delay
the release. Sure, we had a number of driver reverts, and there was an
annoying auto-NUMA memory corruption fix series, but none of it was
really worth delaying 3.12 for.

But the fact that I'm going to be (effectively) off-line next week
means that I'm *not* opening the merge window for 3.13 yet - since I
won't have the bandwidth to really do merges anyway.

That doesn't mean that you can't send me pull request for the merge
window early, of course - maintainers can *always* send their pull
requests early rather than late, if they have everything lined up and
ready. But if you have some feature that still wants polishing, you
basically get a free week to do that.

So the two-week merge window for 3.13 will start a week from now. You
have an extra week. But that also means that I will be doubly
disappointed in anybody who then leaves their merge request until the
*end* of that two-week merge window.

Anyway. 

Onto a totally different topic: we're getting to release numbers where
I have to take off my socks to count that high again. I'm ok with
3.<low teens>, but I don't want us to get to the kinds of crazy
numbers we had in the 2.x series, so at some point we're going to cut
over from 3.x to 4.x, just to keep the numbers small and easy to
remember. We're not there yet, but I would actually prefer to not go
into the twenties, so I can see it happening in a year or so, and
we'll have 4.0 follow 3.19 or something like that.

Now, it's just a number (since we've long since given up on
feature-related releases), and it's at least a year away, so why do I
even mention it at all?

The reason I mention it is because I've been mulling over something
Dirk Hohndel said during LinuxCon EU and the kernel summit. He asked
at the Q&A session whether we could do a release with just stability
and bug-fixes, and I pooh-poohed it because I didn't see most of us
having the attention span required for that
(cough*cough*moronic*woodland creature*cough*cough).

So I may be pessimistic, but I'd expect many developers would go
"Let's hunt bugs.  Wait. Oooh, shiny" and go off doing some new
feature after all instead. Or just take that release off.

But I do wonder.  Maybe it would be possible, and I'm just unfairly
projecting my own inner squirrel onto other kernel developers. If we
have enough heads-up that people *know* that for one release (and
companies/managers know that too) the only patches that get accepted
are the kind that fix bugs, maybe people really would have sufficient
attention span that it could work.

And the reason I mention "4.0" is that it would be a lovely time to do
that. Roughly a years heads-up that "ok, after 3.19 (or whatever),
we're doing a release with *just* fixes, and then that becomes 4.0".

Comments?

That, together with not really knowing the whole edac area, makes me
not wonderfully happy about pulling directly. I a.lso see another edac
pull request in my mailbox, making me suspect that there are
maintainership problems (historically I've been getting the code
either though Greg, Borislav or Mauro). So I'd also like to make sure
that I at least _know_ about any disputes going on in this area?

So before I pull this, I'd quickly like to validate the gpg key, and
am cc'ing more people to see if I'm stepping into some political
nightmare. I can be ok with that, but I want to do so knowingly, not
blindly. Ugh. This patch makes me angry. It looks way too ad-hoc.

I can well imagine that our current one-entry cache is crap and could
be improved, but this looks too random. Different code for the
CONFIG_MMU case? Same name, but for non-MMU it's a single entry, for
MMU it's an array? And the whole "largest" just looks odd. Plus why do
you set LAST_USED if you also set LARGEST?

Did you try just a two- or four-entry pseudo-LRU instead, with a
per-thread index for "last hit"? Or even possibly a small fixed-size
hash table (say "idx = (add >> 10) & 3" or something)?

And what happens for threaded models? Maybe we'd be much better off
making the cache be per-thread, and the flushing of the cache would be
a sequence number that has to match (so "vma_clear_cache()" ends up
just incrementing a 64-bit sequence number in the mm)?

Basically, my complaints boil down to "too random" and "too
specialized", and I can see (and you already comment on) this patch
being grown with even *more* ad-hoc random new cases (LAST, LARGEST,
MOST_USED - what's next?). And while I don't know if we should worry
about the threaded case, I do get the feeling that this ad-hoc
approach is guaranteed to never work for that, which makes me feel
that it's not just ad-hoc, it's also fundamentally limited.

I can see us merging this patch, but I would really like to hear that
we do so because other cleaner approaches don't work well. In
particular, pseudo-LRU tends to be successful (and cheap) for caches.

This is specialized enough that I would *really* like the name to be
more descriptive. Compare to the special "smp_read_barrier_depends()"
maco: it's unusual, and it has very specific semantics, so it gets a
long and descriptive name.

Memory ordering is subtle enough without then using names that are
subtle in themselves. mb/rmb/wmb are conceptually pretty simple
operations, and very basic when talking about memory ordering.
"acquire" and "release" are less simple, but have descriptive names
and have very specific uses in locking.

In contrast "smp_tmb()" is a *horrible* name, because TSO is a
description of the memory ordering, not of a particular barrier. It's
also not even clear that you can have a "tso barrier", since the
ordering (like acquire/release) presumably is really about one
particular *store*, not about some kind of barrier between different
operations.

So please describe exactly what the semantics that barrier has, and
then name the barrier that way.

I assume that in this particular case, the semantics RCU wants is
"write barrier, and no preceding reads can move past this point".

Calling that "smp_tmb()" is f*cking insane, imnsho.Ugh. Maybe. Can you guarantee that those are the correct semantics?
And why talk about the hardware semantics, when you really want
specific semantics for the *software*.

Ok, that sounds more along the lines of "these are the semantics we
want", but I have to say, it also doesn't make me go "ahh, ok".
I don't think this is true. acquire+release is much stronger than what
you're looking for - it doesn't allow subsequent reads to move past
the write (because that would violate the acquire part). On x86, for
example, you'd need to have a locked cycle for smp_acqrel_mb().

So again, what are the guarantees you actually want? Describe those.
And then make a name.

I _think_ the guarantees you want is:
 - SMP write barrier
 - *local* read barrier for reads preceding the write.

but the problem is that the "preceding reads" part is really
specifically about the write that you had. The barrier should really
be attached to the *particular* write operation, it cannot be a
standalone barrier.

So it would *kind* of act like a "smp_wmb() + smp_rmb()", but the
problem is that a "smp_rmb()" doesn't really "attach" to the preceding
write.

This is analogous to a "acquire" operation: you cannot make an
"acquire" barrier, because it's not a barrier *between* two ops, it's
associated with one particular op.

So what I *think* you actually really really want is a "store with
release consistency, followed by a write barrier".

In TSO, afaik all stores have release consistency, and all writes are
ordered, which is why this is a no-op in TSO. And x86 also has that
"all stores have release consistency, and all writes are ordered"
model, even if TSO doesn't really describe the x86 model.

But on ARM64, for example, I think you'd really want the store itself
to be done with "stlr" (store with release), and then follow up with a
"dsb st" after that.

And notice how that requires you to mark the store itself. There is no
actual barrier *after* the store that does the optimized model.

Of course, it's entirely possible that it's not worth worrying about
this on ARM64, and that just doing it as a "normal store followed by a
full memory barrier" is good enough. But at least in *theory* a
microarchitecture might make it much cheaper to do a "store with
release consistency" followed by "write barrier".

Anyway, having talked exhaustively about exactly what semantics you
are after, I *think* the best model would be to just have a

  #define smp_store_with_release_semantics(x, y) . 

and use that *and* a "smp_wmb()" for this (possibly a special
"smp_wmb_after_release()" if that allows people to avoid double
barriers). On x86 (and TSO systems), the
smp_store_with_release_semantics() would be just a regular store, and
the smp_wmb() is obviously a no-op. Other platforms would end up doing
other things.

Hmm?

So I have to say, that if we can fix this with just adding a single
new pci_set_master() call, we should do that before we decide to
revert.

If other, bigger issues then come up, we can decide to revert. But if
there's a one-liner fix, let's just do that first, ok?

Mind sending a patch?

             
That's an oddly spelled nitpick "correction".

If you really want to fix it, it's "robust" and "ward off problems".
But it's too late now, and the wrong spelling and word choice is in
the git tree and released in the -rc3 I just pushed out.

No, none of the invalidate_page users really need to sleep. If doing
this makes some people not do stupid sh*t, then that's all good. So at
least _that_ worry was a false alarm. We definitely don't want to
support crap in the VM, and sleeping during teardown is crap.

                

Tough.

I spoke up the first time this came up and I'll say the same thing
again: we're not screwing over the VM subsystem because some crazy
user might want to do crazy and stupid things that nobody sane cares
about.

The whole "somebody might want to . " argument is just irrelevant.
Some people want to sleep in interrupt handlers too, or while holding
random spinlocks. Too bad. They don't get to, because doing that
results in problems for the rest of the system.

Our job in the kernel is to do the best job technically that we can.
And sometimes that very much involves saying "No, you can't do that".

We have limitations in the kernel. The stack is of limited size. You
can't allocate arbitrarily sized memory. You must follow some very
strict rules.

If people can't handle that, then they can go cry to mommy, and go
back to writing user mode code. In the kernel, you have to live with
certain constraints that makes the kernel better.This looks totally invalid.

The slab constructor is *not* called on every allocation. Quite the
reverse. Constructors are called when the underlying allocation is
initially done, and then *not* done again, even if that particular
object may be allocated and free'd many times.

So the reason we can do

    atomic_set(&anon_vma->refcount, 0);

in a constructor is that anybody who frees that allocation will do so
only when the refcount goes back down to zero, so zero is "valid
state" while the slab entry stays on some percpu freelist.

But the same is ABSOLUTELY NOT TRUE of the value "1", nor is it true
of the anon_vma->root. When the anonvma gets free'd, those values will
*not* be the same (the refcount has been decremented to zero, and the
root will have been set to whatever the root was.

So the rule about constructors is that the values they construct
absolutely *have* to be the ones they get free'd with. With one
special case.

Using slab constructors is almost always a mistake. The original
Sun/Solaris argument for them was to avoid initialization costs in
allocators, and that was pure and utter bullshit (initializing a whole
cacheline is generally cheaper than not initializing it and having to
fetch it from L3 caches, but it does hide the cost so that it is now
spread out in the users rather than in the allocator).

So the _original_ reason for slab is pure and utter BS, and we've
removed pretty much all uses of the constructors.

In fact, the only valid reason for using them any more is the special
case: locks and RCU.

The reason we still have constructors is that sometimes we want to
keep certain data structures "alive" across allocations together with
SLAB_DESTROY_BY_RCU (which delays the actual *page* destroying by RCU,
but the allocation can go to the free-list and get re-allocated
without a RCU grace-period).

But because allocations can now "stay active" over a
alloc/free/alloc-again sequence, that means that the allocation
sequence MUST NOT re-initialize the lock, because some RCU user may
still be looking at those fields (and in particular, unlocking an
allocation that in the meantime got free'd and re-allocated).

So these days, the *only* valid pattern for slab constructors is
together with SLAB_DESTROY_BY_RCU, and making sure that the fields
that RCU readers look at (and in particular, change) are "stable" over
such re-allocations.

Your patch is horribly wrong.

No.

The mmap_sem (and the page_table_lock) only protects a single VM instance.

The anon_vma chains, in contrast, can span multiple VM instances,
since it grows over fork().

Now, you're right that we used to not always lock the root every time,
but the code has changed since. We used to lock each anon_vma as we
copied them, now we lock the root only once (see anon_vma_clone()).
And that works exactly because they all share the same root anon_vma
on the chain. You can't change that to just randomly lock one non-root
vma.

So I'll add my voice to the chorus that says "that's wrong", when you
changed "anon_vma->root->lock" to "anon_vma->lock".

We *may* be able to do finer-grained anon_vma locking again, but it's
definitely not some mindless "let's just change it back" thing. It
needs real code changes (and I bet it's not just anon_vma_clone()),
and it needs a lot of care.This seems somewhat insane:

(removed the "old" lines to make it more readable).

Why pass in "struct vm_area_struct **vma_prealloc" when you could just
pass in a plain and more readable "struct vm_area_struct *vma"?

My *guess* is that you originally cleared the vma_prealloc thing if
you used it, but in the patch you sent out you definitely don't (the
_only_ use of that "vma_prealloc" is the line that loads the content
into "vma", so this interface looks like it is some remnant of an
earlier and more complicated patch?

So this would be the *perfect* place to just downgrade the semaphore
from a write to a read.

Do the vma ops under the write semaphore, then downgrade it to a
read-sem, and do the page teardown with just mmap_sem held for
reading. 

Comments? Anybody want to try that? It should be fairly
straightforward, and we had a somewhat similar issue when it came to
mmap() having to populate the mapping for mlock. For that case, it was
sufficient to just move the "populate" phase outside the lock entirely
(for that case, we actually drop the write lock and then take the
read-lock and re-lookup the vma, for unmap we'd have to do a proper
downgrade so that there is no window where the virtual address area
could be re-allocated)

The big issue is that we'd have to split up do_munmap() into those two
phases, since right now callers take the write semaphore before
calling it, and drop it afterwards. And some callers do it in a loop.
But we should be fairly easily able to make the *common* case (ie
normal "munmap()") do something like

    down_write(&mm->mmap_sem);
    phase1_munmap(. );
    downgrade_write(&mm->mmap_sem);
    phase2_munmap(. );
    up_read(&mm->mmap_sem);

instead of what it does now (which is to just do
down_write()/up_write() around do_munmap()).

I don't see any fundamental problems, but maybe there's some really
annoying detail that makes this nasty (right now we do
"remove_vma_list() -> remove_vma()" *after* tearing down the page
tables, and since that calls the ->close function, I think it has to
be done that way. I'm wondering if any of that code relies on the
mmap_sem() being held for exclusively for writing. I don't see why it
possibly could, but. 

So maybe I'm being overly optimistic and it's not as easy as just
splitting do_mmap() into two phases, but it really *looks* like it
might be just a ten-liner or so.  And if a real munmap() is the common
case (as opposed to a do_munmap() that gets triggered by somebody
doing a "mmap()" on top of an old mapping), then we'd at least allow
page faults from other threads to be done concurrently with tearing
down the page tables for the unmapped vma. So I suspect we could make the mmap_sem write area *much* smaller for
the normal cases.

Look at do_mmap_pgoff(), for example: it is run entirely under
mmap_sem, but 99% of what it does doesn't actually need the lock.

The part that really needs the lock is

but we hold it over all the other stuff too.

In fact, even if we moved the mmap_sem down into do_mmap(), and moved
code around a bit to only hold it over those functions, it would still
cover unnecessarily much. For example, while merging is common, not
merging is pretty common too, and we do that

allocation under the lock. We could easily do things like preallocate
it outside the lock.

Right now mmap_sem covers pretty much the whole system call (we do do
some security checks outside of it).

I think the main issue is that nobody has ever cared deeply enough to
see how far this could be pushed. I suspect there is some low-hanging
fruit for anybody who is willing to handle the pain. 

Quite frankly, wouldn't it be much nicer to just fix "load_msg()" instead?

Using "size_t" also gets rid of the games we play with DATALEN_MSG/SEG.

IOW, something like the attached. 

Of course, we *also* should fix ns->msg_ctlmax to make clear you can't
use negative numbers there. No question about that. I think it would
be better to even avoid INT_MAX, because there are memory use concerns
and CPU usage ones too (we generate that list of
smaller-than-page-size fragments).

Hmm?

Ugh. I hate hate hate the timing, and this is much larger and scarier
than what I'd like at this point, but I don't see the point to
delaying this either.

So I'm pulling them. And then I may end up doing an rc8 after all.

What does that matter? If you have multiple callers, they might try to
free that one sd twice, since they could both see a non-NULL case.

Not as far as I can tell. kobject_del() calls sysfs_remove_dir(), and
I'm not seeing why that would be under the mutex.  The only locking I
see is that sysfs_assoc_lock, which _isn't_ held for the reading of
kobj->sd.

Now, there may be other reasons for this all working (like the fact
that only one user ever calls kobject_del() on any particular object,
but it sure as hell isn't obvious. The fact that you seem to be
confused about this only proves my point.

Besides, the "design pattern" of having a lock for the assignment, but
then reading the value without that lock seems to be all kinds of
f*cking stupid, wouldn't you agree? I'm really not seeing how that
could _ever_ be something you make excuses for in the first place.
Even if there is some external locking (which, as far as I can tell,
there is not), that would just raise the question as to what reason
that spinlock has to exist at all.

The code doesn't make any sense with the locking the way it is now. It
might _work_, of course, but it sure as hell doesn't make sense.

Good analysis.

HOWEVER.

I actually think even your version is very dangerous, because we pass
in the *address* to that count, and the only real reason to do that is
because we might call it in a loop, and we want the function to update
that count.

And even your version still underflows from 0 to really-large-count.
It *returns* when underflow happens, but you end up with the counter
updated to a large value, and then anybody who uses it later would be
screwed.

See, for example, the inline list_lru_walk() function in <linux/list_lru.h>

So I think we should either change that "unsigned long" to just
"long", and then check for "<= 0" (like list_lru_walk() already does),
or we should do

to make sure that we never do that underflow.

I will modify your patch to do the latter, since it's the smaller
change, but I suspect we should think about making that thing signed.

Hmm?Hmm. No obvious ideas come to mind, but I'm adding more people to the cc.

Clearly the wait_event_interruptible_timeout() in the RCU grace-period
thread causes this, but I'm not seeing why shutdown would trigger it.

The code disassembles to

so the oops is in the final

where "%ecx" is "vec->prev" (f8c551f4). That looks like it might be a
perfectly valid pointer, but clearly it isn't (it's about 115M off the
top of virtual memory, I think that might be in the vmalloc area).

So I'm *guessing* that something did a vfree() on some data structure
that contained active timers - and then later on the RCU thread ended
up being the next thing that tried to add a timer after the
now-non-existing one.

And your other oopses do seem to have a similar pattern, even if their
actual oops is elsewhere. They oops in run_timer_softirq, also taking
a page fault in the 0xf9.  range, so it might well be a vmalloc
address there too.

But I sure as hell can't start to guess what that would be.

I'm wondering it CONFIG_DEBUG_OBJECTS (and then
CONFIG_DEBUG_OBJECTS_FREE=y and CONFIG_DEBUG_OBJECTS_TIMERS=y) might
help catch this. Hmm.  I just got a run_timer_softirq oops on my own laptop, slightly
different. That was not during shutdown, although there was a "yum
upgrade" finishing when that happened, so it's quite likely that there
was a service shutdown (and then restart).

I think it's related. But my oops has almost no information: the IP
that was jumped to was bogus, and the callchain is just CPU idle
followed by the softirq -> run_timers_softirq handling, so there's no
real way to see *what* triggered it.

The bad rip was ffffffffa051e250, which is not a valid code address.
It *might* be a module address, though. So this might be triggered by
rmmod on some module that doesn't remove all its timers. 

Ideas?
Yes, but nobody has actually been able to trigger it with those. It's
pretty rare, and the debug options are so expensive that they aren't
reasonable to enable generally. 

So we need to try to figure out how to trigger it, or narrow things
down some way. 

              

Ok, still trying to figure this out, and I do have another bug as a
result. I don't think this one is really the fundamental one either
that caused my crash during "yum upgrade", nor necessarily Knut's
problem during shutdown, but I'll keep looking.

And who knows.  Maybe this *does* explain Knut's issue.

Appended is a warning I get with DEBUG_TIMER_OBJECTS. Seems to be a
device-mapper issue. Alasdair, Neil, comments? It looks like
dm_destroy() is freeing an delayed_work entry that is still active. 

I don't know exactly which field in the 'struct mapped_device' has
that delayed-work thing, but I assume it's the kobject.  Somebody who
knows this code better, please take a look!

.  and here's another one. This time it looks like nf_conntrack_free()
is freeing something that has a delayed work in it (again, likely an
embedded 'struct kobject'). Looks like it is the

that triggers this. Which probably means that there are still slab
entries on that slab cache or something, but I didn't dig any deeper. 

David? Patrick? Pablo? Jozsef? Any ideas? This was immediately preceded by

and I think it's that delayed "kobject_release()" that triggers this.

Notice that kobject_release() can be delayed *without* the magic
kobject debugging option by simply having a reference count on it from
some external source. So this particular issue is probably triggered
by my extra debug options in this case (I'm running with all those
nasty "try to find bad object freeing" options, and doing module
unloading etc), but can happen without it (it's just very hard to
trigger in practice without the debug options).
.  and one more case of freeing a delayed work object (likely a kobject again):

This time it looks like it's in the PCI layer, freeing the msi irq information.

It looks like that code simply does

    kobject_del(&entry->kobj);
    kobject_put(&entry->kobj);
    list_del(&entry->list);
    kfree(entry);

and the problem is that the "entry->kobj" may have *other* references
to it, thanks to people accessing it through /sys, so despite doing a
kojbect_del/kobject_put(), it's not at all ok to then do a "kfree()"
on it. The embedded kobj might still be in use.

Afaik, that code should do the kfree() on the kobject in the _release_
method, not synchronously like that.

We already have a msi_kobj_release(), I'm wondering why that doesn't
do the kfree().

Bjorn? Yinghai? Greg, comments about that msi kobj usage?
Adding more people, so quoting the whole email for them.

We definitely have some module unload issues. Guys, try the following
a few times to unload modules:

    lsmod | grep ' 0 '| cut -d' ' -f1 | xargs sudo rmmod

(a few times because unloading one module will then potentially make
other modules unloadable).

On my machine, I can trigger this, for example:

so at least we have a cpufreq/sysfs interaction bug. There may be others.

This particular cpufreq issue may be triggered by the fact that
acpi-cpufreq isn't actually in use (pstate is). Or it might be some
generic cpufreq/sysfs bug. Rafael, Greg, ideas?

I don't see that this particular one would be the one that causes the
timer issues, but it's an example of the fact that module unload tends
to be special and not necessarily well tested.

I think:

 - EBUSY is a better error return than EEXIST here/

  - do the ENODEV case first, because claiming something is busy when
you couldn't have used it anyway is kind of stupid)

but yeah, this looks like the right thing. Let's make sure there
aren't any annoying error messages etc printed out as a result of
this, though.Ugh. I won't comment on the actual kvm part of this patch, somebody
who knows that code should do so.

But I reacted to this:
"Bad address"? Christ people, are you guys making up error numbers
with some kind of dice-roll? I can just see it now, somebody sitting
there with a D20, playing some kind of kernel-specific D&D, and
rolling a ten means that you get to slay the orc, and pick an error
number of EFAULT for some random kernel function. Because quite
frankly, "random dice roll" is the _only_ thing that explains "Bad
address" sufficiently.

Please, whoever wrote virt/kvm/kvm_main.c:: kvm_init_debug(), WTF?
EFAULT means "user passed in an invalid virtual address pointer",
which is why the error string is "Bad address". It makes absolutely NO
SENSE here. Perhaps EEXIST or EBUSY.
The solution to this would be to simply return an error-pointer. See
<linux/err.h>. That's what we do for most complex subsystems that
return a pointer to a struct: rather than returning "NULL" as an
error, return the actual error number encoded in the pointer itself.

But that would require every user of debugfs_create_dir() to be
updated to check errors using IS_ERR() instead of checking against
NULL, and there's quite a few of them.

So I think just making the error be EEXIST is a simpler solution right now.
copy_from_user_nmi() itself is all kinds of nasty.

Using __get_user_pages_fast() for a single page is quite expensive,
and mucks around with the page counts etc.

If copy_from_user_nmi() just did the (simple) page table walk by hand,
it could avoid *all* of that. No page count stuff - just have
interrupts disabled over not just the page walk, but the copy too - to
guarantee that no cross-CPU TLB flush can come in.

So instead of trying to improve __get_user_pages_fast() - which is
impossible because the interface fundamentally means that it has to
iterate over things and check page counts - you could simplify the
caller instead.

That is, if we really care any more. Maybe this "do the
copy_from_user_nmi() just once" is already good enough that nobody
much cares.Careful! There is one magic piece of state that you need to
save-and-restore if you do this, namely %cr2. Taking a page fault
always writes to %cr2, and we must *not* corrupt it in the NMI
handler.

Also, right now, it looks like we call notify_page_fault() in the
atomic page fault case, and that would be deadly from within an NMI.

But if you move the "in_atomic()" check earlier in __do_page_fault(),
you can *try* to do something like this:

or something close to that. But you absolutely *have* to save/restore
%cr2 (the above tries to avoid writing it if it didn't change,
somebody should check the timings on that to see whether it makes
sense or not).
Oh, ok then, we should be good to go. I wonder why we needed that
special "_nmi()" version, then. 

Please do check that NMI increment the irq-counts etc.  Otherwise
you'll need to add the explicit "pagefault_disable/enable()" pair
around the __copy_from_user_inatomic(). 

Btw, it's not just the commit history. The actual file layout is
terminally horrible too. The actual LWN article made it look like ktap
was just a user-space tool, and I was thinking that it was like
tools/pert/, just in staging.

But looking at the tree, it looks like parts of it is a kernel module,
and parts of it is the user space thing, and it's totally impossible
to see which is which, it's just all mixed up in the same directory
structure.

Maybe I misunderstood, but that was my reaction from a very quick look.

Btw, it's really not just gcc 3.x. That code was (a) incomprehensible,
(b) wrong and (c) caused problems for LLVM too.

It was wrong because "__builtin_constant_p(ww_ctx == NULL)" simply
makes no sense.

Why?

That expression is largely equivalent to
"__builtin_constant_p(ww_ctx)" (because iff ww_ctx is constant, then
the comparison to NULL is constant), which is actually much easier to
read, while carrying a totally different semantic meaning. Making
things worse, the comparison to NULL *may* be marked constant under
some very random situations (ie the compiler could turn a "taking an
address of a variable is never NULL" kind of knowledge and combining
it with other knowledge, and turn a complicated "ctx" expression into
a "I know this cannot be NULL" thing, and thus the "== NULL" is a
constant, even though ctx itself is some dynamic calculation).

Whoever wrote the original should be shot. And this commit shouldn't
have been marked as being somehow about gcc-version dependence, but
about removing completely crap code.Stop this idiotic "blame gcc bug" crap. Which part of my explanation
for why it was *NOT* a compiler bug did you not understand?
See my "largely equivalent" comment, with the *EXTRA* logic that gcc
may actually find cases where the comparison is a constant even if the
ww_ctx thing itself isn't a constant.

Stop the f*cking around already! The  whole "we expect ww_ctx to be
null" thing shows that YOU DO NOT SEEM TO UNDERSTAND WHAT THE TEST
ACTUALLY IS!

The expression

   __builtin_constant_p(ww_ctx == NULL)

has ABSOLUTELY NOTHING to do with whether ww_ctx is NULL or not!
Christ, can you really not understand that?

For example, ww_ctx could be "&static_variable", and the compiler can
- and some compiles _will_ - say that ww_ctx clearly cannot be NULL,
so "ww_ctx == NULL" is 0, which is a constant, so the
__builtin_constant_p() expression returns true. See? That expression
has absolutely NOTHING to do with whether you passed in NULL or not.
NOTHING.

That __builtin_constant_p() tests whether the comparison is
*CONSTANT*. And "0" is just as much a constant as "1" is. Really. So
the whole f*cking expression is total and utter crap, because it is
entirely and utterly senseless. It lacks all meaning. It's not
actually testing for NULL at all. Never was, never will.

The *ONLY* thing it is testing for is "how much can the compiler
optimize this", and as such the *ONLY* thing it tests for is compiler
differences.

Really. Seriously. If you start blaming the compiler for different
compilers giving different results, the only thing *that* shows is
that you didn't understand the expression to begin with.
NO NO NO NO. No a f*cking thousand times. It's not "too broken in
gcc". It's too broken in the source code, and the fact that you don't
even understand that is sad. You wrote the code, and you seem to be
unable to admit that *your* code was buggy.

It's not a compiler bug. It's your bug. Stand up like a man, instead
of trying to flail around and blame anything else but yourself.

So guys, get your act together, and stop blaming the compiler already.

Dammit, even if that is true, then write the conditional *correctly*.

As mentioned, the conditional

    __builtin_constant_p(ww_ctx) && ww_ctx == NULL

is actually sensible, in a way the original one was *not*. It actually
tests what you apparently intended to test, and is more readable to
humans to boot.

And no, it still isn't actually guaranteed to do what you want it to
do. Historically, in gcc, __builtin_constant_p() really only ever
worked in macros, because by the time you use it in inline functions,
a constant NULL in the caller will have been turned into a argument
variable in the inline function, and __builtin_constant_p() would be
done before that was optimized away. Over the years, gcc has pushed
some of the builtin evaluation deeper down, and these days it actually
works within inline functions, but my point that
__builtin_constant_p() is about a certain level of compiler
optimization is very much true: you're actually testing for a compiler
optimization detail.

I know the LLVM people had similar issues with this comparison, so
these days it's not even just about gcc versions. We may never have
cared very much about icc, but llvm is actually an interesting target
compiler.

Well, more than that - the "optimization" has been done at the source
code level, so that the behavior is no longer a matter about how well
the compiler optimizes it any more.

I'm not complaining about the fix. I'm complaining about how the fix
was claimed to be due to a compiler bug. The "documentation" for the
fix (ie the commit message) was actively misleading.

Side note: testing "can the compiler optimize this expression at
compile time" is actually sometimes an interesting question, so it can
be a valid thing to test.

But people should understand that the question is literally about THAT
(ie visibility into compiler optimization) rather than about the value
itself.

So generally, the only thing that a __builtin_constant_p() test can be
used for is in *conjunction* with having an actual test for an actual
value, and then having special-case logic that pertains to that value.

So for example, *this* is a valid test:

and it's useful for triggering a compile-time optimized code-sequence
that is only true for NULL. But because __builtin_constant_p() is
about "how well can the compiler optimize this", that "else" statement
had better be able to handle the generic case too.

And yes, there are a few places where we do expect a certain minimal
set of optimizations. So in some cases we *might* have the rule that
the only valid use of NULL in a case like the above is when the
pointer passed in is passed in as a constant. And then we might say
"we rely on the compiler always returning true for
__builtin_constant_p(NULL)", and then we might say "so the "generic"
version of the code is guaranteed to never see NULL".

But notice how *different* that

    __builtin_constant_p(ww_ctx) && ww_ctx == NULL

test is from

    __builtin_constant_p(ww_ctx == NULL)

and really, the two tests are *fundamentally* really really different.
The first one can make sense. While the second one is pure and utter
garbage.The KS week is over, and thus the seventh - and likely the last - rc
for 3.12 is out, and I'm back on the normal Sunday schedule.

The slowdown in -rc sizes sadly reversed itself here, mostly due to
the networking updates that hadn't come in for rc5-rc6. You can see
that in the diffstat, with more than 50% being networking (both driver
and core) patches. The rest is generally other drivers (gpu, media,
scsi, thermal, HID) and some smaller arch updates (s390, parisc, x86).

Nothing looks particularly shocking, though, so we're still on track
for 3.12.  Which does mean, that with me having more travel coming up,
the 3.13 merge window will likely end up being a bit messed up.

I can usually schedule around these kinds of things, but with two
different trips and just a week in between, I have the choice of
either just delaying 3.12 for no good reason, or just saying "ok,
we'll just keep the merge window open a bit longer because I won't be
able to be as responsive as I should be".

But who knows. If something particularly worrisome comes up during the
upcoming week, I might just say "Ok, we'll do an rc8 instead". So I'm
keeping my options open.

Plase stop sending me untested crap that doesn't even compile cleanly!
No it's not. "Already up-to-date.".

Also, even if it was, I'm not feeling the reason to pull it. "Cleanup
sprintf formatting of firmware version"? "Remove superfluous mask of
pcie_cap_reg"? Seriously?

Multi-hundred-line pull requests that come in this late had better
look a bit more serious than that.

There's *HUNDREDS* of lines of changes, all of which seems to come
from the change to use the (admittedly much superior) native bit
handling fucntions, instead of the shit-for-brains abortion that is
"esas2r_lock_set/clear_flags()"

And I see why you'd want to get rid of that horrible crap, but I see
absolutely _zero_ reason to do it in -rc7.

As far as I can tell from the patch, a few lines are for actual
problems like oopses. And the rest is pure and utter garbage that
should have happened during the merge window.

So I literally *cannot* pull this, because the git tree doesn't
contain what you said it contains. But even if it did, I would not
*want* to pull this kind of completely brainless crap.

You need to curse at the people who send you this kind of crap.
Seriously. This is not acceptablee timing.

The new driver exception was about supporting new hardware, so we can
add things like new ID's etc (or even whole new drivers) later in the
development process than the merge window.

And any "new code" (that is behind a new config option or totally new
system call or something like that, so that the code cannot be
triggered by old users) can admittedly get changes more easily if it
cannot regress (since it didn't exist before). But that's more a "I'll
let it slide for rc2-3" thing than any big concept, and it's
definitely not about rc7.

And neither of those issues have ever made it acceptable to random
pointless churn. If it doesn't add support for new hardware, it's not
under some kind of "new hardware support" exception. And just "it's
new code, so we cannot possibly regress" is an excuse for making
slightly more invasive changes (especially to ABI's etc that need to
get fixed before a release), but it's not some blanket "we can do
anything" exception.

And quite frankly, even the "new driver" exception was about hardware
that normal people actually are expected to use.  Things that are
relevant for a possibly large number of users. If it's something that
random people can be expected to pick up and use in Fry's or Office
Depot, we want to make sure we have a driver available as soon as
humanly possible.

In contrast, if it's a driver for some enterprise hardware that is
only used by people who are going to run enterprise kernels anyway (ie
often several years down the line), there is ABSOLUTELY NO REASON to
merge it late in the release window. The enterprise users aren't using
bleeding edge kernels anyway, their time constraints are different.

So if people thought it was some carte blanche for "anything goes",
people were confused.

You need to think about *WHY* we had a "new driver" exception. And
realize that it probably will never actually be relevant for any SCSI
driver (not counting things like USB storage etc).

It looks like it is the access to "lock->key" that takes a page fault.
The pointer looks good (%r13=ffff8801654cec98), so I'm pretty sure
this is due to DEBUG_PAGEALLOC and a free'd page.

So it looks like ep_unregister_pollwait() calls remove_wait_queue() on
a wait-queue head that has already been free'd.

I have this dim memory of us having fought this before. But maybe I'm
just remembering some of the old signalfd-vs-epoll races.

Oleg, does this trigger any memory for you? Commit 971316f0503a
("epoll: ep_unregister_pollwait() can use the freed pwq->whead") just
makes me go "Hmm, this is *exactly* that that commit is talking
about. "

Ok, Oleg, going back to that whole thread, I think that old bug went like this:

 (a) normally all the wait-queues that epoll accesses are associated
with files, and as such they cannot go away for any normal file
activity. If the file exists, the waitqueue used for poll() on that
file must exist.

 (b) signalfd is special, and it does a

     which means that the wait-queue isn't associated with the file
lifetime at all. It cleans it up with signalfd_cleanup() if the signal
handlers are removed. Normal (non-epoll) handling is safe, because
"current->sighand" obviously cannot go away as long as the current
thread (doing the polling) is in its poll/select handling.

 (c) as a result, epoll and exit() can race, since the normal epoll
cleanup() is serialized by the file being closed, and we're missing
that for the case of sighand going away.

 (d) we have this magic POLLFREE protocol to make signal handling
cleanup inform the epoll logic that "oops, this is going away", and we
depend on the underlying sighand data not going away thanks to the
eventual destruction of the slab being delayed by RCU.

 (e) we are also very careful to only ever initialize the signalfd_wqh
entry in the SLAB *constructor*, because we cannot do it at every
allocation: it might still be in reused as long as it exists in the
slab cache: the SLAB_DESTROY_BY_RCU flag does *not* delay individual
slab entries, it only delays the final free of the underlying memory
allocation.

 (f) to make things even more exciting, the SLAB_DESTROY_BY_RCU depend
on the slab implementation: slub and slob seem to delay each
individual allocation (and do ctor/dtor on every allocation), while
slab does that "delay only the underlying big page allocator" thing.

Agreed so far? Ugly, ugly, ugly, and I think there's exactly one
person who understands all of this. Namely you.

Anyway, so we protect the magic wait_queue_head_t from going away from
under us. We even make sure that we are very careful about only
initializing the sighand spinlock and this magic signalfd_wqh only
once per RCU lifetime. We're going to a lot of trouble to make this
all work. And it looks like it should work.

BUT.

It's enough that *one* other user of "poll_wait()" does something
similar to signalfd, and *doesn't* go through all this effort to make
it ok.

And I see a few worrisome cases. For example, look at "tty_poll()". It
ends up doing something very similar, except it uses the tty instead
of sighand. And exactly like the sighand struct, the tty allocation
lifespan can - thanks to hangup() - be shorter than the file
allocation lifespan.

Peter? Does a tty hangup end up actually possibly freeing the tty
struct? Looking at it, I'm starting to think that it only affects
f_op, and the "struct tty" stays around, in which case this is all
fine.

Hmm? There might be other cases. 

Ugh. How reliable is the double fault? Because bisecting it to the
merge that didn't even have any conflicts in it as far as I can
remember means that there's something really subtle going on wrt some
semantic conflict or other. Or, alternatively, it means that the
bisect failed because the double fault isn't 100% reliable. 

Anyway, the stack is crap when the original fault happens at
"boot_tvec_bases+0x1fe", and that causes the double fault debug code
to take *another* fault, which means that it doesn't even show the
right code sequence. Too bad. So ignore the latter part of the oops,
but the top part looks valid:
<boom, it crashes again here>

but it has jumped into a data section and is executing random data as
code, and there is no sign of where it jumped *from*, since the random
code clearly corrupted the stack - resulting in the double fault in
the first place.

So the oops is almost entirely useless as a debug aid in this
situation. I'm almost hoping that your bisect was wrong, and you could
try to see if you could do that again. Ok, that makes way more sense. 

Ok, can you post some of the more promising oopses so that we can try
to figure out what kobject it is that causes problems?Ok, they may be randomly selected, but they are all the same. Which is
good, I guess, we're only talking about one bug.

Anyway, they all have RIP:run_timer_softirq+0x12c/0x1b8, and the code is

where that constant is LIST_POISON2 and the "and $2" seems to be
TIMER_IRQSAFE. So the trapping instruction *looks* like it's doing
__list_del() on the timer, and timer->next is NULL.

So somebody added a timer, and then deallocated/cleared the structure
before it triggered. The problem is, I can't see a way to figure out
_who_ did that.

I *think* r14 contains the function we're going to jump to in the
oops, and that could be interesting to know, but it's not decoded, so
you'd have to match it up against a symbol map. 
Actually, Fenguguan, never mind. Instead, change the "pr_debug()" in
kobject_release() to  "pr_alert()", so that it gets printed out. Or
just boot with the "ignore_loglevel" thing so that debug messages are
actually visible.

At that point, we should be able to match the oops workqueue list
address with the address of the delayed kernel object that gets
printed out.Actually, I think it's the same bug.

You *cannot* just reuse the storage. Doing a "memset()" doesn't
improve anything. The basic issue is that if you reuse it, you're
buggy. End of story.

Why? It's refcounted, and it's out of your hands. Reusing it is wrong
- because it might still be used. The fact that you "released" it is
immaterial. Others can have refcounts (and through /sys etc,
historically really do have them).Ok, so it's definitely parport_pc. I don't see what it would do wrong, though.

               

Yeah. And quite frankly, normally that whole DEBUG_KOBJECT_RELEASE
thing is hopefully only enabled in debug kernels (like maybe the
Fedora rawhide one, or at developers), so being a bit more verbose is
likely ok.
The timer itself comes simply from the delayed_work that is used to
delay the freeing of the kobject.

So that is not the surprising part.

The surprising part is that I don't see parport_pc doing anything
odd/bad with its kobject embedded in the 'struct dev'. It seems to
just do a platform_device_register_simple() followed by a
platform_device_unregister().

At least that's true for the normal parport_pc_probe_port() case that
just passes in a NULL dev.  But I only glanced at the driver, so I
might have missed something.

Ok. The list corruption (which also pointed at parport_pc) might well
be corrupted by removing the entries before or after the parport_pc,
and moving the corruption to parport_pc that way (through the
"prev->next = next" thing in list handling). So maybe it was something
else all along. You could enable CONFIG_DEBUG_LIST to see if that
triggers some dump earlier. I'll catch up with your emails if it kills me. 

Ugh. Ok. So something else is wrong too.

And this one doesn't leave any hint of where it came from. Damn. The
register contents aren't being very helpful either.

The second one is the same thing.  And equally unhelpful as far as I can tell.

I think we need more powerful debugging aids to make these useful.Ok, I think we have something.

So this is the same bug, now the code decodes to

so the NULL pointer in %rdi (which _should_ be a list pointer) got
loaded from %rax. So the prime suspect is an allocation near

and looking backwards in your dump, we have

so it looks like that "parparport_pc.632" thing.

Another one, same code:and for that boot we had

so now it's 'parport_pc.956'.

Anyway, that does look like a strong pattern.

Does the problem go away if you disable the parport_pc driver?Umm.

Wasn't this pretty much the argument for the COMPLETELY F*CKED UP
change to make the vma locking use a semaphore?

The whole "it's more convenient to use sleeping locks" argument is
PURE AND UTTER SHIT when it comes to really core code. We are *much*
better off saying "this is core, we care so deeply about performance
that you had better use your brain before you do this".

Seriously. Your argument is bad, but more importantly, it is
*dangerously* bad. It's crap that results in bad code: and the bad
code is almost impossible to fix up later, because once you encourage
people to do the easy thing, they'll do so.

Yes, preempt_disable() is harder to use than sleeping locks. You need
to do pre-allocation etc. But it is much *much* more efficient.

And in the kernel, we care. We have the resources. Plus, we can also
say "if you can't handle it, don't do it". We don't need new features
so badly that we are willing to screw up core code.

As to your kernel/events/core.c example, things like that are
generally pretty easy to fix. The good thing about preemption-disable
is:

 - it's pretty easily debuggable, because you do get big warnings of
"you screwed up exactly _here_"

 - it's so cheap that especially when it comes to things that can
sleep, it's perfectly ok to drop the preemption count, do the sleeping
thing, re-disable preemption and then check if we still needed the
data.

Oh, and I'm sure there are several users that currently depend on
being able to sleep over get_online_cpu's.  But I'm pretty sure it's
"several", not "hundreds", and I think we could fix them up.

I think we'd generally want to have it be something the loop asks for.

If the loop is just some kind of "gather statistics" thing, I don't
think it's required. The cost per loop is so low (usually adding up a
couple of words) that the downside drowns the upside.

And we could easily look at MAXSMP (or NR_CPUS) at compile-time, and
not do it for common small values (although it looks like Fedora
defaults to 128 CPU's for their distro kernels, which seems a bit
excessive - too many by far for normal people, too few for the crazy
big ones).You can't do that right now - since you have to get the cpu list. So
it may not be with "preemption enabled", but it should always be under
the locking provided by get_online_cpus().  That one allows sleeping,
though.

I personally would *love* to make CPU hotplug be a lockless thing
entirely. But I detest stop-machine too, because it has these really
annoying properties.

So if we want to make it zero-cost to look at online CPU data, can we
avoid even the stop-machine synchronization, instead saying that the
cpu hotplug bitmap is updated completely locklessly, but if you see a
bit set, the data associated with that CPU is guaranteed to still be
available.

IOW, just use "RCU semantics" on a per-bit level. When we offline a CPU, we do

     clear_bit(cpu, cpu_online_mask);
     rcu_synchronize();
     .  now we can free all the percpu data and kill the CPU . 

without any locking anywhere - not stop-machine, not anything. If
somebody is doing a "for_each_cpu()" (under just a regular
rcu_read_lock()) and they see the bit set while it's going down, who
cares? The CPU is still there, the data is accessible. 

I'm sure there's some reason the above wouldn't work, but the above
would seem to be pretty optimal. Why do we really force this big
locking thing? The new patches make that locking _smarter_, but it's
still a damn big lock. Could we possibly go _beyond_ the lock?We could _easily_ add preemption points in for_each_cpu() for the
MAX_SMP case. We can even do it in the macro itself.

So I wouldn't worry about things like that. I'd expect the "Oh damn,
we have to allocate" case to be more painful, but I think that is
fairly easily handled by having something like a "redo_cpu()" macro
that you can call inside for_each_cpu(). So the pattern for the
(nopefully-not-all-that-common) thing would be something like

which is certainly more work than a sleeping lock is, but this looks
like a initialization pattern.

Looking at the things that are actually performance-critical, they
tend to be things like "gather all the statistics for all CPUs".

Honesty in advertizing: I only grepped a couple of users of
get_online_cpus(), and they all seemed like they'd be perfectly happy
with preemption-off. But maybe I was just lucky in my few samples: I
have anecdotes, not data.I'm at PDX, about to fly out to the kernel summit, and it has almost
become a tradition to do an rc release using the airport wifi. So here
it is. 

The patch is still busily being pushed out, but the git trees are
up-to-date, and the tar-file should already be out. Nothing major
happened last week., and the upcoming week is likely to be quiet too,
since a lot of core maintainers will be in Edinburgh for the KS.

Shortlog with more details appended, but the overview is just mainly
driver updates. USB, infiniband, acpi.  Some documentation and Cifs
updates, and the rest is random noise.

Please test,

The rip/rdp thing looks very hacky. And *without* the rip/rdp thing, I
think the word-size always matches the TIF32 bit, right?

Why wouldn't the high bits be zero even in 64-bit mode? It would seem
to be a *major* bug if you are in 64-bit mode but (for example) try to
use the low 32-bit of virtual memory (ie something x32-like), and now
your patch decides to use the 32-bit layout.

As far as I can tell, you actually corrupt rid/rdp in that case
(because when you write the fcs thing, it overwrites the high bits of
rip, and fos overwrites the high bits of rdp). So now bits that
*should* be zero are not.

So you're basically trying to save some old state by corrupting new
state instead.

Am I overlooking something?

No, it does *not* preserve "more state".

It preserves *less* state, because the upper 32 bits of rip are now
corrupted. Any 64-bit application that actually looks at the FP
rip/rdp fields now get the WRONG VALUES.

The "upper bits zero" mode may be used just for JIT'ed code, for
example. It doesn't mean that you'd never have full 64-bit addresses,
so writing to the top half of the register *corrupts* that
information, because the top half bits are still relevant in general,
even if perhaps _one_ particular floating point exception happened
with the bits clear.

Now anybody looking at the FP state on the stack gets the wrong results.

More bits set is *not* "more state", when those bits are wrong.

So having looked at this some more, I would *really* prefer a
different solution. The overwriting of the rip/rdp data just really
annoys me.

Is there any reason to not just do it like the following instead:

 - get rid of the "word_size" thing, instead just add separate
"fcs/fos" fields (that do *not* alias with rip/rdp).

 - on 64-bit, always use 64-bit ops, exactly the way we do now

 - if the resulting rip/rpd is zero in the high bits (and we have
reason to believe the values exist at all, so X86_FEATURE_NO_FPU_SEL
and the AMD test and/or checking that they aren't zero in the low bits
too), do an *additional* fnstenv to the sstack, and then just save the
resulting fcs/fos in the non-overlapping things. Use a simple helper
function for this (that just gets called after the xsave/fxsave logic)

 - same on restore.

No games. No "this is the word-size of the thing we've saved in
memory". No overlapping "this field means one thing or another".

For signal handling, save/restore the new fop/fos thing, so that
nobody ever sees the changed format, but FP state gets correctly and
fully restored over signals too, not just kernel FP stuff.

Hmm? That would make me *much* happier, I suspect.Hmm. Why did you do a 2048-bit key? Your old one was 4k. 

Since you haven't gotten anybody to sign the new key yet, may I
suggest throwing this one away and doing a new 4096-bit key?

Yeah, yeah, I know, my *own* key is just 2048 bits, because back when
I created it we were discussing using various hardware tokens, and
some of them (at the time) didn't do more than 2k. So I shouldn't
really complain when somebody else does a 2k key. But that was a few
years ago, and I claim stupidity (I never did use a hardware token).

Anyway, pulled, since it was all signed by your previous key, and I
guess 2k is still supposed to be "good enough".

Quite frankly, these mlx5 updates don't look like regression fixes.
They look like "continued development" to me, and seem to be things
that never worked. At least some of them look like performance
enhancements, in fact, not even resembling bugfixes. Others look like
"not a bug, but let's improve random behavior".

I pulled this once, because hardly anybody really cares. But the other
side of that "nobody cares" is that I might as well not have pulled.
If this kind of "continued development" patches keep happening, I'll
just start ignoring pull requests after the merge window.

So get your act together, and push back on the people you are supposed
to manage. Because this is *not* acceptable for post-rc5, and I'm
giving this single warning. Next time, I'll just ignore the sh*t you
send me.

Comprende?

So we have a number of these quirks, and Dell seems to be one of the
more common factors.

Now, I'd suggest we just trigger it automatically for Dell, but the
thing is, Dell tends to be the *least* differentiating PC maker out
there, so I'm wondering if it is our default reboot order that is just
plain wrong.

We start off trying to reboot using ACPI. Is that really sane? Is
there any reason to not make the default case be to use the PCI
reboot?

The history behind this seems to be that we _used_ to default to
REBOOT_KBD (since that's the traditional method), and fall back to the
triple fault. Then, it was changed to default to ACPI back in 2008 by
Avi Kivity, because those methods apparently could "assert INIT
instead of RESET", and INIT is apparently blocked in Intel VT.  See
commit

But that immediately caused problems, and got reverted by commit
8d00450d296d. But despite the _known_ problems with the ACPI method,
then in 2011 Matthew Garrett defaulted to ACPI again in commit
660e34cebf0a, claiming that that is what Windows does. Which is
clearly NOT TRUE, since I can pretty much guarantee that Windows works
fine on Dell computers.

So Windows clearly does not default to the ACPI reboot model, at least
not unconditionally. There must be something we're missing. I'm
wondering if the way Matthew figured out the Windows defaults was to
run it in a virtual environment? Or uses some other flag to determine
whether the ACPI reboot is reliable or not. Because considering the
known issues with VT, maybe what Windows actually does is to use ACPI
only on VT machines, or something like that.

Because since that whole re-introduction of "default to ACPI", the
majority of patches to the reboot.c file seems to be just "ACPI reboot
doesn't work on this machine, so let's quirk it".

The alternative is that our "acpi_reboot" code is just completely
bogus. If you look at it, it does magic things with
ACPI_ADR_SPACE_PCI_CONFIG: and then calls acpi_reset() if that's not
the case. And the two functions have *inconsistent* validation of the
ACPI registers. acpi_reboot() will happily use a zero address forits
ACPI_ADR_SPACE_PCI_CONFIG poking, for example. And those functions
have gone back and forth on their breakage too (like not checking
ACPI_FADT_RESET_REGISTER which broke things entirely etc etc).

Guys, any ideas/suggestions? Because this constant "let's quirk things
because we're clearly doing something wrong" is wrong.

I have access to one of the affected Dell systems, so I'm willing to
try patches. 

Things seem to be calming down nicely, and rc5 is smaller than previous rc's.

In fact, the most excitement we had this week wasn't even a kernel
bug, it was a compiler bug wrt "asm goto" that was found because of
code that is pending to be merged in 3.13. But the (happily fairly
straightforward) workaround for the bug was merged early, because we
_do_ use asm goto, and it's unclear whether our existing use might
already trigger the bug, just not enough to be as obviously
noticeable.

Aside from that, most of the changes here are the usual architecture
fixes (tile, arm, x86, s390) and drivers (gpu, hid, sound, i2c,
watchdog). With btrfs and the perf tool updates rounding out the rest.
And the usual random noise.

Go forth and test,
               
Hmm. I'm looking at the final version of that patch, and I'm not
seeing anything wrong. It may trigger a compiler bug - there aren't
that many "asm goto" users, and using them for the bitops adds a lot
of new cases.

Your oops makes very little sense, it looks like task_work_run() just
called out to random crap, probably because the work was already
released, so "work->func()" ends up being bad. I'm adding Oleg to the
participants anyway, just in case there is some race. The comment says
that it can race with task_work_cancel() playing with *work. Oleg,
comments?

However, I don't see any actual bit-op code in task_work_run() itself,
so it's something else that got miscompiled and corrupted memory. In
that respect, the oops you have looks more like the oopses you got
with DEBUG_KOBJECT_RELEASE. Are you sure that wasn't set?

That said, Fengguang, can you try two things just to check:

 - add "cc" to the clobbers list for the asm goto (technically it
should be on the non-asm-goto as well, but we never had that, and
maybe the fact that gcc always ends up testing a register afterwards
hides the need for the clobber).

So it would look like this in arch/x86/include/asm/rmwcc.h

(where that "cc" thing is new). I'm not sure if "cc" really matters on
x86 at all (it didn't use to, long long ago), but maybe it does these
days. 

If that makes no difference, please just verify that the non-asm-goto
version works fine, by changing the

into a simple "#if 0" to disable the asm-goto version.

Ok, so it isn't even specific for x86-32, because your test-case shows
the bug for me on 64-bit too. Apparently we just have a harder time
hitting it in practice in the kernel on x86-64./

Too bad. It makes me nervous about all our _traditional_ uses of asm
goto too, never mind the new ones. [ Richard and Jakub added to cc, they can perhaps help or at least
point us to the right gcc person.

  for the patch. The actual inline asm is pretty dang small, and the
non-asm-goto version works fine. Can you take a look? ]

Ok, that was a long shot, I don't think gcc actually ever assumes cc
is live over an asm on x86.

Ok. So it looks very much like "asm goto()" is simply buggered. Too
bad, since it  generated nice clear code.

I suspect it's the memory clobber - maybe it only marks memory as
clobbered for the fallthrough case, and the actual "goto" case might
used old cached values? What do I know, it's just a theory.

We do have "asm goto" with memory clobbers elsewhere (our x86 version
of __mutex_fastpath_lock()), but that use is very limited and only
gets expanded in a single place. The new bitop cases get expanded
*everywhere*, so if there is something subtly wrong wrt code
generation that requires some particular pattern, they'd trigger it
much more easily.

Anybody have any ideas?
We use ADDR for some of the non-barrier ones too, that don't have the
barrier. See clear_bit() and friends. Yeah, except Fengguang is the only one seeing this in his automated tests. It is indeed just an optimization, and we could in theory switch
between the two versions on a case-by-case basis, but we don't have
any sane way to really do that.

I'll try to see if I can reproduce this on my hardware (just applying
that patch on top of my own tip) and see if I can try to narrow things
down. But I looked at the assembly for a couple of files, and it all
looked good, and I know this patch works fine for others (ie all the
normal -tip testing), so I suspect it's something specific to what
Fengguang does.

Yeah, doesn't reproduce here. 

            
We have a memory clobber instead. So the memory is marked as input and
clobbered.

And we'd love to mark it "+m", but "ask goto" cannot have outputs.

For the serializing ones, the memory clobber is ok - they have barrier
semantics anyway. But we'd actually *want* to use "asm goto" for some
cases where the memory clobber is too big of a hammer, so if we ever
get input/output constraints to "asm goto" we'll be happy.

Of course, right now it looks like we shouldn't be in a rush to use
"asm goto" at all. 
Ugh, you've turned off DEBUG_BUGVERBOSE.

We probably shouldn't even allow that. The space savings aren't worth the pain.

I'm not seeing the pattern in your oopses, there's at least three
different cases. At a guess, it's some lock - or a memory allocator -
that is broken by the bitop breakage, and then the oopses are just
fallout from that. 

            

This looks like file->f_inode is NULL, and it's trying to access inode->i_flock.

There's a number of other ones too (that last delayed __fput one, for
example) that might be due to a NULL inode.

And a lot of thre rest are file-handling too, with the BUG_ON() in
__put_cred() when there's something wrong with file->f_cred.

.  but I have no clue how that could happen and why the asm goto
should matter for this.
Nothing there.

I'm *assuming* that you meant to point me at the "for-curr" branch,
which does indeed contain the commit you talk about, but I want these
things to be explicit rather than have to guess from your pull request
and doing "git ls-remote".

Please fix your script.That looks a bit unlikely. The whole acpiphp_init/get/put_context()
logic is entirely internal to drivers/pci/hotplug/acpiphp_glue.c, and
that merge gets absolutely all the changes from one side, except for a
one-liner change to that file that looks entirely unrelated (commit
928bea964827 "PCI: Delay enabling bridges until they're needed").

That said, that one commit did cause other problems (see commit
f41f064cf435: "PCI: Workaround missing pci_set_master in pci
drivers"), so who knows. Some subtle interaction with exactly when the
hotplug functions end up being called?

But it could possibly be timing-related too. Does everything behind
that hp bridge still work?

I think Steven has some buggered email system, he has other emails
being eaten by lkml too, and apparently other mail gateways (because
you were direct-cc'd on the original).

His email sender doesn't quote names with "," in them, and has headers
like this:

  From: Steven Rostedt <rostedt@goodmis.org>
  To: LKML <linux-kernel@vger.kernel.org>
  Cc: Linus Torvalds <torvalds@linux-foundation.org>, Rafael J. Wysocki
   <rafael.j.wysocki@intel.com>, Mika Westerberg
   <mika.westerberg@linux.intel.com>, Bjorn Helgaas <bhelgaas@google.com>,
   Andrew Morton <akpm@linux-foundation.org>

which is apparently against SMTP rules. The magic line is:

  X-Mailer: Claws Mail 3.9.2 (GTK+ 2.24.20; x86_64-pc-linux-gnu)

The tag-line for that mailer is quite appropriate: "The email reader
that bites". That's what they put in the title on their web-page.

Because it sure bites. Except the claws people seemed to think that
was supposed to be a good thing. They clearly don't know the slang
meaning of "that bites".

Or maybe they do, and they are just unusually self-aware.

Steven, I'd suggest just jettisoning that mailer.I can't even begin to say whether this is a good solution or not,
because that if-conditional makes me want to go out and kill some
homeless people to let my aggressions out.

Can we please agree to *never* write code like this? Ever?

Use a well-named inline helper function where the name describes what
the f*ck the code is trying to do, and then comment the separate
issues. Because none of the above line noise makes me go "Ahh, it's
the test for an ejectable function".

What the heck _is_ an "ejectable function" anyway? The only comment
there just makes the code even less sensible.

Please?Thanks, that looks much better.

I do worry that we now seem to add the slot to all the acpiphp lists
even if it is managed by pciehp. That gets rid of the warning Steven
saw (because now it always has that context), but I'm left wondering
how much pcihp and aciphp will fight over the slot.

Yes, the acpiphp_register_hotplug_slot() doesn't get called, but we
still do register_hotplug_dock_device(), for example. How does that
interact with pcihp that thinks it owns the slot?

Or am I misreading the code? It's more readable, and no longer makes
me homicidal, but I don't actually know the code itself.

Well, it is your fault, but only because you use a buggy mailer from hell.There is no "perhaps" about this. It's clearly a Claws bug.

When you send email to Amaury Decrême (to pick a kernel email address
at random with special characters) do you think you should write his
email address as

  =?UTF-8?q?Amaury=20Decr=C3=AAme?= <amaury.decreme@gmail.com>?

No you should not. For similar reasons, any email program that expects
you to quote dots in names is pure and utter garbage. The fact that
SMTP expects dots to be quoted has absolutely zero bearing on
anything, the same way it has zero bearing that SMTP headers should
use even odder quoting rules for other "special" characters.

File a bug on Claws, and if the developers brush it off as your own
problem, just stop using the PoS.

I realize that you seem to have this self-harming habit - first
evolution, now claws - but it's like cutting or anorexia. We're having
an intervention here, and the first step is to realize you have a
problem.
Yeah. I fire up pine every once in a while if I have more than one
patch to send, but touché.

           Your explanation messages tend to channel James Joyce. They may be
literary works of art, but it _does_ occasionally make them somewhat
hard to read.

I have tried to turn them into slightly less stream-of-consciousness.Agreed. Even an unvalidated key is better than no key, since it can be
validated later, and even if never validated shows at least that
multiple pull requests were generated by people who had access to that
key. 

              I like the patch, but I do worry that some user space thing uses this
to check whether the particular filesystem module is loaded.

Those kinds of things can't use /proc/modules (because it might be
built-in), but /proc/filesystems has traditionally contained
everything.

That said, these particular filesystems I really don't think people
would ever possibly check for, so I think it's fine.
.  except you don't.
Please handle 'n == 0' too. Maybe it never happens (ie you get EPIPE
or ENOSPC), but write returning zero is actually possible and a valid
return value and traditional for "end of media". Looping forever is
not a good idea.
Actually POSIX very much allows zero returns. O_NDELAY is mentioned as
a possible cause, in addition to zero-sized writes themselves, of
course.

Also, writing to (but not past) the end of a block device returns 0
for "end of device", iirc.

                Look closer.

  ".  most historical implementations return zero (with the O_NDELAY
flag set, which is the historical predecessor of O_NONBLOCK . "
Hmm. I'm pretty sure I've seen zero returns for EOF somewhere. Ugh.

This interface pretty much guarantees that a compiler can never do
anything clever, like know that "hey, you used a static initializer on
this thing, and the fields are const, so now know statically what the
functions are, and I can just turn the indirect jumps into direct
jumps".

I'm not sure gcc is actually that clever, but by making it this kind
of ops pointer, I *guarantee* that gcc can never do it.

How about you make the rule be:

 - get rid of the stupid "type" enum index thing

 - get rid of the "init" thing that sets pointers in the dynamic data
structures. Get rid of the pointer too.

 - instead, use a "static const" type descriptor for each type (it
approaches being your "rcu_sync_ops" structure). Pass this in as an
argument to all the functions (use a #define per type or something, so
that users don't need to do this by hand)

 - now every single user passes in that type descriptor.

 - together with using a few inline functions, suddenly the "indirect"
jumps through this type descriptor end up actually being nice direct
compile-time constants: iow, they get turned into direct jumps.

Tadaa. You actually get good code generation, and you use *less*
dynamic memory since you don't have to have this pointer to the
descriptor.

Maybe. It just annoys me, because afaik, the function that gets called
is always static per callsite.

                  I doubt it is intentional, but I also cannot really feel that we care
deeply. Afaik we don't really honor the size limit exactly anyway, ie
we tend to check only at page boundaries etc. So do we really care?
No objections, my comment was more of a "probably nobody cares" rather
than much of an objection to you deciding you care enough to fix it. 

            
Hmm. rc4 has more new commits than rc3, which doesn't make me feel all
warm and fuzzy, but nothing major really stands out. More filesystem
upfates than normal at this stage, perhaps, but I suspect that is just
happenstance. We have cifs, xfs, btrfs, fuse and nilfs2 fixes here.

There's the usual driver updates too (mainly net and target this time
around), along with some generic networking changes too. And some arch
updates (arm, power, s+core, avr32).

But nothing scary, so go out and test.

That sounds like the right thing to do.

Or possibly go even further, and say that the default is to return
-EINVAL, and files and filesystems that actually want the
"default_file_splice_write()" semantics have to say so in their d_op
structure.

Hmm?

Trivial. And I agree that that should be a good flag to have.

I really *really* prefer to stay with two names. Miklos had an earlier
three-name version, and it was hugely more complex, and it does not
fit nearly as well in the model.

Two directory entries is also what the current rename() effectively
always does (clearing one, changing another). So doing the
cross-rename model is actually fairly close to a normal rename. A
three-way one is not actually at all similar.

So I was actually very relieved to see this much simpler and cleaner
model, because the alternative really was nasty. This one looks fairly
simple and clean and straightforward. The previous was none of that.

Guys,
 we've been working on scaling path lookup again, and 3.12 will be
pretty kick-ass on many loads.

HOWEVER, there's one really annoying case: we do all the path lookup
in RCU mode, where we can avoid touching any dentry state etc at all,
but then to "finalize" the path lookup in order to use it long-term,
we have to increment reference counts etc. That can be somewhat
expensive for high-scalability cases, since it will force-dirty (and
thus make exclusive in the cache) the core dentry cache entry.

And that's all fine for a lot of the common cases: when you do things
like open a file, you really do have to increment the reference count,
and there's no question that we do the right thing. And most people
who really open files tend to work on them, so getting the dentry
exclusively is fine.

There's one very important exception, though: things like "stat()" and
"access()" do *not* open a file in order to hold on to it. And they
are quite common (stat() in particular), _and_ they are often done on
files that are shared and then passed by rather than worked on. 

Now, interestingly, both stat() and access() actually _already_ do
some kind of retry for special circumstances (LOOKUP_REVAL), and that
really looks like it could be extended to just do the whole lookup in
RCU mode too, and thus avoid ever finalizing the pathname.

However, the LSM interface doesn't really allow for that.

So how do people feel about passing a "mode" value for
security_inode_getattr(), the same way we do for
security_inode_permission()? The only flag would be that MAY_NOT_BLOCK
flag that gets set for RCU lookup, and the semantics would be the same
(return -ECHILD if you need to sleep).

Attached is a patch that adds the interface, and then makes all
security layers just do that ECHILD thing (and nobody actually sets
MAY_NOT_BLOCK yet). So it's purely preparatory. It's also
insufficient, because we'll need the same kind o fflag for the
low-level filesystem "i_op->getattr()" call, but that's an independent
issue.

Al, any comments?

That would get ugly.

However, I don't think we actually really need to do that.  We had a
similar situation with d_revalidate() passing inode pointers etc
totally unnecessarily. Yes, the filesystem needs to use ACCESS_ONCE()
and care about NULL, but it doesn't need anything more than that. And
we really do have that already.

And we already have dentry->d_sb - which is supposed to be valid.
Again, we already use it under RCU for d_revalidate() and for name
hashing. So the super-block had better already be ok with RCU.

Ok, I did a quick test, and it looks ok here, so looking good for 3.13.

However, the new smp_mb() in mntput_no_expire() is quite noticeable in
the path lookup stress-test profiles. And I'm not seeing what that
allegedly protects against, especially if mnt_ns is NULL (ie all the
common important cases).

            Yeah, I misread the profile assembly code. The point being that the
nice fast case now has the smp_mb() in it, and it accounts for about
60% of the cost of that function on my performance profile.

Hmm. The CPU2 mntput can only happen under RCU readlock, right? After
the RCU grace period _and_ if the umount is going ahead, nothing
should have a mnt pointer, right?

So I'm wondering if you couldn't just have a synchronize_rcu() in that
umount path, after clearing mnt_ns. At that point you _know_ you're
the only one that should have access to the mnt.

You'd need to drop the mount-hash lock for that. But I think you can
do it in umount_tree(), right? IOW, you could make the rule be that
umount_tree() must be called with the namespace lock and the
mount-hash lock, and it will drop both. Or does that get too painful
too?Yeah, I think we should be guaranteed that, because the
synchronize_rcu() will guarantee that all other CPU's go through an
idle period. So the "read A" on CPU2 cannot possibly see a 1 _unless_
it happens so early that synchronize_rcu() definitely sees it (ie it's
a "preexisting reader" by definition), in which case synchronize_rcu()
will be waiting for a subsequent idle period, in which case the B=0 on
CPU2 is not only guaranteed to happen but also be visible out, so the
"read B" on CPU1 will see 0. And that's true even if CPU2 doesn't have
an explicit memory barrier, because the "RCU idle" state implies that
it has gone through a barrier.

So I don't see how they could possibly see ones. Modulo terminal bugs
in synchronize_barrier() (which can be very slow, but for umount I
wouldn't worry). Or modulo my brain being fried.Don't think of it in those terms.

The only thing that matters is semantics. The semantics of
synchronize_rcu() is that it needs to wait for all RCU users. It's
that simple. By definition, anything inside a "rcu_read_lock()" is a
RCU user, so if we have a read of memory (memory barrier or not), then
synchronize_rcu() needs to wait for it. Otherwise serialize_rcu() is
clearly totally broken.

Now, the fact that the normal rcu_read_lock() is just a compiler
barrier may make you think "oh, it cannot work", but the thing is, the
way things happens is that synchronize_rcu() ends up relying on the
_scheduler_ data structures, rather than anything else. It requires
seeing an idle scheduler state for each CPU after being called.
We seriosly depend on nothing leaking out. Not just reads. The "U" in
RCU is "update". So it's reads, and it's writes. The fact that it says
"read" in "rcu_read_lock()" doesn't mean that only reads would be
affected.

And no, this still has nothing to do with memory barriers. Every
single RCU user depends on the memory freeing being delayed by RCU,
for example. And again, that's not just reads. It's people taking
spinlocks on things that are RCU-protected etc.

So no, there is no question about this. The only question would be
whether we have some RCU mode that is _buggy_, not whether you need
extra memory barriers. And that is certainly possible.

That's certainly noticeable. Surprisingly so. What makes MK8 so bad in
particular, I wonder?

Just out of interest, have you done any profiles on the kernel cost
here to see what it is that makes such a big difference. Because
normally on a kernel build, I see most of the overhead in path lookup.
But that's only true for otherwise optimized builds that don't have
system call auditing etc debugging that spreads the costs out over
everything. Yes, that would be lovely.
Sounds reasonable to to me.

Side note: I really wish there was some way to avoid having to
finalize the vfsmount entirely for some common things. For example,
"[l]stat[at]()" really doesn't need it for the common cases (network
filesystems may need to revalidate), and is a very critical operation,
and we *could* just look up the inode under RCU and never finalize the
dentry _or_ the vfsmount. However, very annoyingly, the security layer
wants the vfsmount, and we don't know if that is RCU-safe. 
I have to say, that when I was working with the dcache lockref code, I
absolutely _detested_ the magical shrink_dcache_for_umount() code that
violated all the locking rules.

So I actually wouldn't mind at all if that was all forced to follow
all the same rules that the live filesystem code is forced to follow.
Yes, yes, it's going to slow things down, but it's not like umount()
is _that_ performance critical. And I think the whole "let's ignore
locking rules" actually comes from back when we had one global
dcache_lock: we used to have batching in order to not hold the
dcache_lock over long periods, and then it got converted to the
per-dentry locking, and then that got removed entirely with the whole
RCU lookup etc.

So I would be *entirely* ok with having
shrink_dcache_for_umount_subtree() take the d_lock on the dentry as it
shrinks it etc etc.Ugh, yes, that dget/dput(parent) looks wrong in RCU mode.

That said, in RCU mode you simply shouldn't _need_ it at all, you
should be able to just use dentry->d_parent without any refcount
games. Put an ACCESS_ONCE there to be safe. You might want to make
sure that you do the same for the inode, and check for NULL, to be
safe against racing with a cross-directory rename/rmdir. I don't know
if there are then any internal fuse races with the whole
get_fuse_conn() etc, so. 

It does look bad. In practice, of course, it will never hit anything.

That sounds safer, but then the fuse_advise_use_readdirplus() bit
wouldn't get set. But why _is_ that bit set there in the first place?
It sounds stupid. I think the bit should be set in the lookup path (or
the revalidation slow-path when the timeout is over and the thing gets
properly revalidated), why the hell does it do it in the fast-path
revalidation in the first place? That's just odd. Maybe there is some
odd internal fuse logic.

Miklos, please do give that a look. 

I don't think glibc does strlcpy. It's not a standard C function, and
it's somewhat controversial (although I dislike strncpy more with the
crazy zero-padding, ugh).

So with all the lockref work, we now avoid the dentry d_lock for
almost all normal cases.

There is one single remaining common case, though: the final dput()
when the dentry count goes down to zero, and we need to check if we
are supposed to get rid of the dentry (or at least put it on the LRU
lists etc).

And that's something lockref itself cannot really help us with unless
we start adding status bits to it (eg some kind of "enable slow-case"
bit in the lock part of the lockref). Which sounds like a clever but
very fragile approach.

However, I did get myself a i7-4770S exactly because I was
forward-thinking, and wanted to try using transactional memory for
these kinds of things.

Quite frankly, from all I've seen so far, the kernel is not going to
have very good luck with things like lock elision, because we're
really fine-grained already, and at least the Intel lock-elision
(don't know about POWER8) basically requires software to do prediction
on whether the transaction will succeed or not, dynamically based on
aborts etc. And quite frankly, by the time you have to do things like
that, you've already lost. We're better off just using our normal
locks.

So as far as I'm concerned, transactional memory is going to be useful
- *if* it is useful - only for specialized code. Some of that might be
architecture-internal lock implementations, other things might be
exactly the dput() kind of situation.

And the thing is, *normally* dput() doesn't need to do anything at
all, except decrement the dentry reference count. However, for that
normal case to be true, we need to atomically check:

 - that the dentry lock isn't held (same as lockref)
 - that we are already on the LRU list and don't need to add ourselves to it
 - that we already have the DCACHE_REFERENCED bit set and don't need to set it
 - that the dentry isn't unhashed and needs to be killed.

Additionally, we need to check that it's not a dentry that has a
"d_delete()" operation, but that's a static attribute of a dentry, so
that's not something that we need to check atomically wrt the other
things.

ANYWAY. With all that out of the way, the basic point is that this is
really simple, and fits very well with even very limited transactional
memory. We literally need to do just a single write, and something
like three reads from memory. And we already have a trivial fallback,
namely the old code using the lockrefs.  IOW, it's literally ten
straight-line instructions between the xbegin and the xend for me.

So here's a patch that works for me. It requires gcc to know "asm
goto", and it requires binutils that know about xbegin/xabort. And it
requires a CPU that supports the intel RTM extensions.

But I'm cc'ing the POWER people, because I don't know the POWER8
interfaces, and I don't want to necessarily call this "xbegin"/"xend"
when I actually wrap it in some helper functions.

Anyway, profiles with this look beautiful (I'm using "make -j" on a
fully built allmodconfig kernel tree as the source of profile data).
There's no spinlocks from dput at all, and the dput() profile itself
shows basically 1% in anything but the fastpath (probably the _very_
occasional first accesses where we need to add things to the LRU
lists).

And the patch is small, but is obviously totally lacking any test for
CPU support or anything like that. Or portability. But I thought I'd
get the ball rolling, because I doubt the intel TSX patches are going
to be useful (if they were, Intel would be crowing about performance
numbers now that the CPU's are out, and they aren't).

I don't know if the people doing HP performance testing have
TSX-enabled machines, but hey, maybe. So you guys are cc'd too.

I also didn't actually check if performance is affected. I doubt it is
measurable on this machine, especially on "make -j" that spends 90% of
its time in user space. But the profile comparison really does make it
look good. 

Comments?

I think we can pick that up from Andi's patches, he should have that.
Although that did have very x86-specific naming (ie "xbegin"). And I
don't think he used "asm goto" to quite the same advantage as this -
and I think we do want to make sure that the overhead is minimal.

Yes, I haven't thought much about it, but from a quick look it should
be trivial to do fd_install(). We already free the fdtable using rcu
(because we look things up without locking), so we could just get the
rcu read-lock, do fd_install() without any locking at all, and then
verify that the fd-table is still the same. Rinse and repeat if not.

I don't think you even need that. alloc_fd() has already reserved the
spot, so I think you really can just assign without any fancy cmpxchg
at all. If you write to the old fdtable, who cares? Probably just
something like

or similar. Under the RCU lock to guarantee the allocations don't
disappear from under you, but with no other locking.

Maybe I'm missing something, but it really doesn't look that bad.

Now, that only gets rid of fd_install(), but I suspect you could do
something similar for put_unused_fd() (that one does need cmpxchg for
the "next_fd" thing, though). We'd have to replace the non-atomic
bitops on open_fds[] with atomic ones, just to make sure adjacent bit
clearings don't screw up concurrent adjacent bit values, but that
looks fairly straightforward too.

Now, getting rid of the spinlock for alloc_fd() would be more work,
and you'd likely want to keep it for actually resizing (just to keep
that case more straightforward), but you could probably do the
non-resizing cases fairly easily there too without holding the lock.
Scan for a free bit optimistically and without any locking, and then
be somewhat careful with setting that open_fds[] bit - not only using
an atomic test_and_set() for it, but also do the same "let's check
that the fdtable base pointers haven't changed in the meantime and
repeat".

On the whole the fd table looks like if it really is a contention
point, it would be fairly straightforward to fix without any huge
contortions. Sure, lots of care and small details to look out for, but
nothing looks really all that fundamentally difficult.

But is it really a contention point on any real load?
Shouldn't a cmpxchg() in just the dup2 code solve that?

If the old value was NULL, you'd have to repeat and go back and see if
the open_fds[] bit had been cleared in the meantime (ie it's NULL not
because somebody else is busy installing it, but because somebody just
uninstalled it).

But yeah, I do agree that that sounds nasty and a complication I
hadn't even thought about. dup2() does violate our normal "let's
pre-allocate the fd slot" rule. Ugh.
Ugh. It's very possible it's not worth using for the kernel then. The
example I posted is normally fine *without* any transactional support,
since it's a very local per-dentry lock, and since we only take that
lock when the last reference drops (so it's not some common directory
dentry, it's a end-point file dentry). In fact, on ARM they just made
the cmpxchg much faster by making it entirely non-serializing (since
it only updates a reference count, there is no locking involved apart
from checking that the lock state is unlocked)

So there is basically never any contention, and the transaction needs
to basically be pretty much the same cost as a "cmpxchg". It's not
clear if the intel TSX is good enough for that, and if you have to
save a lot of registers in order to use transactions on POWER8, I
doubt it's worthwhile.

We have very few - if any - locks where contention or even cache
bouncing is common or normal. Sure, we have a few particular loads
that can trigger it, but even that is becoming rare. So from a
performance standpoint, the target always needs to be "comparable to
hot spinlock in local cache".Well, just about any kernel transaction will at least read the state
of a lock. Without that, it's generally totally useless. My dput()
example sequence very much verified that the lock was not held, for
example.

I'm not sure how that affects anything. The actual transaction had
better not be visible inside the locked region (ie as far as any lock
users go, transactions better all happen fully before or after the
lock, if they read the lock and see it being unlocked).

That said, I cannot see how POWER8 could possibly violate that rule.
The whole "transactions are atomic" is kind of the whole and only
point of a transaction. So I'm not sure what odd lock restrictions
POWER8 could have.
That's fine, and matches the x86 semantics fairly closely, except
"xbegin" kind of "contains" that "jump to abort address". But we could
definitely use the same models. Call it
"transaction_begin/abort/end()", and it should be architecture-neutral
naming-wise.

Of course, if tbegin then acts basically like some crazy
assembly-level setjmp (I'm guessing it does exactly, and presumably
precisely that kind of compiler support - ie a function with
"__attribute((returns_twice))" in gcc-speak), the overhead of doing it
may kill it.
Btw, I'd really wish people edited things like this when putting them
in the commit logs. I try to do it when I get them (usually though
Andrew's patch-bombs), just because there's just a ton of detail there
that just isn't relevant for the actual issue at hand.

The kernel oops messages try to contain all kinds of possibly-relevant
data, which makes them useful for a wide range of situations ("oh, it
looks like a single-bit flip"), but at the same time means that once
you know what the problem is, 90% of the data printed out is just pure
noise and at that point no longer helpful, but just makes it harder to
see what's actually the issue.

So please, after you've analyzed an oops, don't use the raw oops data
any more. Usually what remains relevant is the actual oops message
itself, and the backtrace.I try to generally edit out the hex
representation of the symbol information, and obviously stale entries
from the backtrace. I'm not consistent, see for example commit
6f6b8951897e (register info remains) vs commit d6394b590029 (mainly
just call trace) vs commit 3e6b11df2451 (where I just truncated it
mercilessly). And no, I don't always clean things up (it can be a
bother), but I generally try, so now I'm just trying to spread the
word. 

Because at some point the excess verbiage really goes from "that's
useful" to being a blob of noise that actually takes away from the
message.
Perhaps because they were never sent to me?

Searching my email archives I see that a pull request went to Al and
the mailing lists, but not actually to me. I do have a pending pull
request from Al (which came in after -rc3) but that doesn't contain
them either. And the "for-linus" branch implies that perhaps somebody
_iindended_ to send them to me, but somehow missed.

               
Ok, this is *much* simpler than adding the new MCS spinlock, so I'm
wondering what the performance difference between the two are.

I'm obviously always in favor of just removing lock contention over
trying to improve the lock scalability, so I really like Waiman's
approach over Tim's new MCS lock. Not because I dislike MCS locks in
general (or you, Tim ;), it's really more fundamental: I just
fundamentally believe more in trying to avoid lock contention than in
trying to improve lock behavior when that contention happens.

As such, I love exactly these kinds of things that Wainman's patch
does, and I'm heavily biased.

But I know I'm heavily biased, so I'd really like to get comparative
numbers for these things. Waiman, can you compare your patch with
Tim's (and Alex's) 6-patch series to make the rwsem's use MCS locks
for the spinlock?

The numbers Tim quotes for the MCS patch series ("high_systime (+7%)")
are lower than the ones you quote (16-20%), but that may be due to
hardware platform differences and just methodology. Tim was also
looking at exim performance.

So Tim/Waiman, mind comparing the two approaches on the setups you have?

Btw, I really hate that thing. I think we should turn it back into a
spinlock. None of what it protects needs a mutex or an rwsem.

Because you guys talk about the regression of turning it into a rwsem,
but nobody talks about the *original* regression.

And it *used* to be a spinlock, and it was changed into a mutex back
in 2011 by commit 2b575eb64f7a. That commit doesn't even have a reason
listed for it, although my dim memory of it is that the reason was
preemption latency.

And that caused big regressions too.

Of course, since then, we may well have screwed things up and now we
sleep under it, but I still really think it was a mistake to do it in
the first place.

So if the primary reason for this is really just that f*cking anon_vma
lock, then I would seriously suggest:

 - turn it back into a spinlock (or rwlock_t, since we subsequently
separated the read and write paths)

 - fix up any breakage (ie new scheduling points) that exposes

 - look at possible other approaches wrt latency on that thing.

Hmm?
Yeah, I'm not happy about or rwlocks. That's one lock that currently
is so broken that I think we could easily argue for making that one
queued.

Waiman had a qrwlock series that looked reasonable, and I think his
later versions were drop-in replacements (ie they automatically just
did the RightThing(tm) wrt interrupts taking a recursive read lock - I
objected to the first versions that required that to be stated
explicitly).

I think Waiman's patches (even the later ones) made the queued rwlocks
be a side-by-side implementation with the old rwlocks, and I think
that was just being unnecessarily careful. It might be useful for
testing to have a config option to switch between the two, but we
might as well go all the way.

The old rwlock's really have been a disappointment - they are slower
than spinlocks, and seldom/never end up scaling any better.  Their
main advantage was literally the irq behavior - allowing readers to
happen without the expense of worrying about irq's.No, I agree. But it's really just that our rwlock implementation isn't
very good. It's neither really high-performing nor fair, and the
premise of a rw-lock is that it should scale better than a spinlock.

And yes, under heavy reader activity *does* work out for that (ok, you
get cacheline bouncing, but at least you don't get the spinning when
you have tons of readers), the extreme unfairness towards writers
under heavy reader activity makes it often simply unacceptable.

So unlike a lot of other "let's try to make our locking fancy" that I
dislike because it tends to hide the fundamental problem of
contention, the rwlock patches make me go "those actually _fix_ a
fundamental problem".

That's just completely bogus, and cannot work.

Maybe just a "write_lock(&anon_vma->root->rwlock)" (which is just
anon_vma_unlock_write(anon_vma)). But I think we might have a lockdep
issue. I'm not quite sure what's up with the nesting there.

That's the wrong way around. It should be

so some more testing definitely needed.Umm. We call them regressions, and we fix them.

As already mentioned, the original switch to a mutex didn't even have
any good explanation for it, much less actual data.

When we make mistakes, we'd better realize it and fix them, rather
than assume that they were fixes just because they were made.

The performance numbers are pretty damn compelling for this having
been a major mistake.

(Of course, the current performance numbers also contain the
conversion to a rwlock_t rather than back to a spinlock, which may
actually help wrt the original situation too)I'll happily break that.

No way will some random crazy RDMA be a reason why we'd drop this kind
of improvement on actual loads that matter way more.

                 
Ok, that's pretty convincing too.

Side note: are you sure that the i_mmap_mutex needs to be a sleeping
lock at all? It's documented to nest outside the anon_vma->rwsem, so
as long as that is a sleeping lock, the i_mmap_mutex needs to be one
too, but looking at the actual users, most of them seem to be *very*
similar to the anon_vma->rwsem users. It is a very close cousin to the
anon_vma->rwsem, after all (just for file-backed pages rather than
anonymous ones). No?

I dunno. Maybe the ranges are too big and it really has latency
issues, the few I looked at looked like fairly trivial interval-tree
operations, though.

And your numbers for Ingo's patch:

Are just overwhelming, in my opinion. The conversion *from* a spinlock
never had this kind of support behind it.

Btw, did anybody run Ingo's patch with lockdep and the spinlock sleep
debugging code to verify that we haven't introduced any problems wrt
sleeping since the lock was converted into a rw-semaphore?

Because quite frankly, considering these kinds of numbers, I really
don't see how we could possibly make excuses for keeping that
rw-semaphore unless there is some absolutely _horrible_ latency issue?Yes it is. And that reminds me of a problem I think we had with this
code: we had a possible case of the preemption counter nesting too
deeply. I forget the details, but it was something people worried
about.

That mm_take_all_locks() thing is really special, and I suspect that
if we go down this way we should just do a single preempt-disable and
then use the arch_write_lock() to avoid both the lockdep splat _and_
the preemption counter overflow.
Yes.

But the problem with anon_vma is that the "usually" may be the 99.9%
case, but then there are some insane loads that do tons of forking
without execve, and they really make some of the rmap code work very
very hard. And then they all not only share that one root vma, but the
mm/rmap.c code ends up having to walk all their VM's because there
could be a page in there somewhere.

These loads aren't necessarily very realistic and very much not
common, but I think AIM7 actually has one of those cases, iirc.

Our anon_vma locking really is some of the more complex parts of the
kernel. Not because of the lock itself, but because of the subtle
rules about the whole anon_vma chain and how we have to lock the root
of the chain etc etc. And under all _normal_ behavior it's not a
problem at all. But I personally dread looking at some of that code,
because if we get anything wrong there (and it's happened), it's too
painful for words.
Btw, I assume the odd binary size reduction is gone now, and code
generation is generally identical?

Btw, unrelated to the patch itselt: you seem to use the old broken
7-character short SHA1 format.

Yes, 7 hex characters is generally still unique. But no, it won't stay
so. The birthday paradox means that you start to get collisions more
quickly than you'd think, and the original 7-character hex string
default was done early in git development, when the BK repository was
considered "big", and that one was approaching having 16-bit commit
numbers.

Oh, how naive. In the eight+ years we've used git, we've flown past
that old 16-bit limit in commit numbers (we're now around 400k
commits), and we have over 3M objects. So we already have lots of
collisions in 7 characters. Now, git is smart enough that when it
picks a short form of the SHA1, it will never pick a collision, but
since the tree keeps growing, they can happen later.

As a result, I currently strongly suggest kernel-developers do

    git config --global core.abbrev 12

to set the default abbreviated SHA1 count to 12 characters, not the
old default of 7. That pushes out the collision worry to the point
where we're talking "later generations" rather than "forseeable
future".
Ack on the whole series.

I guess x86-64 and powerpc no longer care, but for other architectures
the stack usage issue can be a regression even if I think it's only
been reported on Power. So I'm assuming we should pull this into 3.12.
Yes?

And what about earlier stable kernels? Afaik this was originally
introduced by commit facd8b80c67a, merged into 3.9. Or was there some
other trigger? Do we want to just do the "__do_softirq" ->
"do_softirq()" change for stable?

"Help us, Obi-wan Weisbecker. You are our only hope"I'm back to a Sunday release schedule, so here it is, rc3.

Nothing really special stands out. There's a fair amount of churn in
mm, which is unusual at this point, but that's all reverts: during the
merge window Andrew sent some changes that were still being discussed,
and we're reverting them for now.

If you ignore the mm changes, the rest looks very normal: the bulk is
drivers (gpu, dm/bcache, usb, sound. ), with the usual sprinkling of
architecture (powerpc, x86, arm, mips) and filesystem (udf, xfs,
reiserfs) changes. And some perf tooling.

And we had some performance-tweaking of the new lockref support for
ARM and s390.

On the whole, nothing really appears very scary. Go forth and test,

Please, I *really* want signed tags from github. You've done it before. 

Also, please use the "git://" protocol instead of "https://". If for
no other reason than consistency.

I'd actually prefer you do this without the renaming, and we just use
arch_mutex_cpu_relax() in the lockref code.

We have another set of patches kind-of pending that uses
arch_mutex_cpu_relax() (MCS lock for rwsem), and I don't want to screw
them up.

The _one_ thing I'd suggest is that you replace this:

 #ifndef CONFIG_HAVE_ARCH_MUTEX_CPU_RELAX
 #define arch_mutex_cpu_relax() cpu_relax()
 #endif

with just a simple

  #ifndef arch_mutex_cpu_relax
  # define arch_mutex_cpu_relax() cpu_relax()
  #endif

and get rid of that unnecessary CONFIG variable. I'm ok with CONFIG
variables, but they should be for bigger things than "I have this
particular function". The "I have this particular function" is
particularly easily solved with just using the function name as a
preprocessor symbol (either _because_ it is a macro in the first
place, or with a simple "#define fn_name fn_name" to let the generic
code know it exists).

So if you just move the

  #define arch_mutex_cpu_relax()    barrier()

into arch/s390/include/asm/processor.h (so that you always have it
when  you have the regular cpu_relax() one, and people don't need to
include <mutex.h> to get it), and then put that #ifdef into
lib/lockref.c, and we're all ok. We already did the equivalent for the
ARM use of cmpxchg64_relaxed().

And we can maybe rename it later, when there isn't any new use pending. 

I don't like the "let's add dummy operations for everybody who doesn't
care" when it is this specialized.

I'd much rather just add a single

   #ifndef cmpxchg64_relaxed
   # define cmpxchg64_relaxed cmpxchg64
   #endif

to the LOCKREF code, and then ARM (and others) can define it as they wish.

And *if* anybody else ever realizes that they want this outside of the
lockref code, let's look at doing that then. Right now I don't know of
any users, and I'd be leery of people using this willy-nilly, because
very few people really understand memory ordering.

CMOV (and, more generically, any "predicated instruction") tends to
generally a bad idea on an aggressively out-of-order CPU. It doesn't
always have to be horrible, but in practice it is seldom very nice, and
(as usual) on the P4 it can be really quite bad.

On a P4, I think a cmov basically takes 10 cycles.

But even ignoring the usual P4 "I suck at things that aren't totally
normal", cmov is actually not a great idea. You can always replace it by

and assuming the branch is AT ALL predictable (and 95+% of all branches
are), the branch-over will actually be a LOT better for a CPU.

Why? Because branches can be predicted, and when they are predicted they
basically go away. They go away on many levels, too. Not just the branch
itself, but the _conditional_ for the branch goes away as far as the
critical path of code is concerned: the CPU still has to calculate it and
check it, but from a performance angle it "doesn't exist any more",
because it's not holding anything else up (well, you want to do it in
_some_ reasonable time, but the point stands..)

Similarly, whichever side of the branch wasn't taken goes away. Again, in
an out-of-order machine with register renaming, this means that even if
the branch isn't taken above, and you end up executing all the non-branch
instructions, because you now UNCONDITIONALLY over-write the register, the
old data in the register is now DEAD, so now all the OTHER writes to that
register are off the critical path too!

So the end result is that with a conditional branch, on a good CPU, the
_only_ part of the code that is actually performance-sensitive is the
actual calculation of the value that gets used!

In contrast, if you use a predicated instruction, ALL of it is on the
critical path. Calculating the conditional is on the critical path.
Calculating the value that gets used is obviously ALSO on the critical
path, but so is the calculation for the value that DOESN'T get used too.
So the cmov - rather than speeding things up - actually slows things down,
because it makes more code be dependent on each other.

ie the cmov is quite a bit slower. Maybe I did something wrong. But note
how cmov not only is slower, it's fundamnetally more limited too (ie the
branch-over can actually do a lot of things cmov simply cannot do).

So don't use cmov. Except for non-performance-critical code, or if you
really care about code-size, and it helps (which is actually fairly rare:
quite often cmov isn't even smaller than a conditional jump and a regular
move, partly because a regular move can take arguments that a cmov cannot:
move to memory, move from an immediate etc etc, so depending on what
you're moving, cmov simply isn't good even if it's _just_ a move).

(For me, the "cmov" version of the function ends up being three bytes
shorter. So it's actually a good example of everything above)

Interesting. My P4 gets basically exactly the same timings for the cmov
and branch cases.  And my Core 2 is consistently faster (something like
15%) for the branch version.

Btw, the test-case should be the best possible one for cmov, since there
are no data-dependencies except for ALU operations, and everything is
totally independent (the actual values have no data dependencies at all,
since they are constants). So the critical path issue never show up.

No. For the actual CPU memory ordering, we have different macros ("mb()" for
"memory barrier", "rmb()", "wmb()" etc). They _also_ inhibit compiler
reordering, or course (otherwise they'd not be very useful), but they also
emit the instructions to tell the CPU to be careful.

The "barrier()" macro is there entirely to defeat the compiler reordering
that is sometimes deadly.

For example, the compiler doesn't know _squat_ about locking, and that's
just as well, since compiler-generated locking tends to suck dead donkeys
through a straw (ie monitors etc that some languages have, and that are
tied to the data structures and tend to be overly careful since the
compiler doesn't actually understand what's going on).

In particular, the compiler doesn't know that the "local_cpu_lock" thing is
special, and has any relationship to the code inside, so the compiler might
decide to move the assignments around. Maybe it might even decide to remove
the first assignment entirely, since it has "no effect".

Marking the thing volatile doesn't much help - it is not really a well
defined language feature, and it usually just means that the compiler does
really stupid things with it.

So you add a "barrier()" to the locking functions, telling the compiler that
it can't move stuff around it. The barrier doesn't generate any code in
itself.

(No, the above is not a real piece of code, and the locks do _not_ look like
that in Linux, but you get the idea).

There are other cases where local ordering (as opposed to SMP ordering)
really matters. They tend to all be about asynchronous events (ie
interrupts that cause scheduling or similar), and they don't tend to be all
that common, but when they happen you really want to make them clear both
to the programmer _and_ the compiler. The "barrier()" macro is a good way
to do both.

And "volatile" really isn't a useful thing usually. It's not that the data
itself is necessarily something the compiler cannot optimize: it's
literally the flow of _code_ that is volatile, not the data. A particular
piece of memory may well be "stable" as far as most usage is concerned (ie
the variable itself shouldn't be marked volatile, since that would just
needlessly impact the 99% of uses that do not care), but in some cases we
do special things. For example, this is real code from the scheduler.

where the point here is that we must wait until _after_ we clear the
preemption count before we can test the TIF_NEED_RESCHED flag. We want to
make sure that we really are running the highest-priority process, but at
the same time we just disabled asynchronous preemption while we did the
scheduling decision, so we may have had an asynchronous event in the
meantime that made our scheduling decision be the wrong one.

And here the ordering really is a property of the _code_, not of the data
involved. Sure, the thread flag could be marked volatile, but that would
just make the ordering "invisible" . And the fact is, the ordering is
really between those _two_ data structures - it's not just one of them that
is somehow magically "volatile".

If I could make it explicit that there is an ordering constraint on those
data structures, I would. It's actually a bit like the "aliasing" issues
that you find with wild pointers - even though the data doesn't alias in
physical memory, there are ordering constraints on the code that accesses
them. See?

However, trying to describe these aliasing things to the compiler is
basically a huge problem, and it's just not worth it. It's much easier and
clearer to just use a big hammer and say "compiler - you just don't have a
clue, so don't do anything at ALL in this area".

Compiler people tend to think that people want smart compilers. I as a
compiler power-user can emphatically tell you that that isn't true. People
want _reliable_ compilers, and they want compilers that can be told to just
get the hell out of the way when needed. It's not often you need to, but
when you need to, you _really_ need to.

The notion of making the language more complex to be able to tell the
compiler details like the above so that it would just do the RightThing(tm)
is just crazy talk.  Compilers are good at some things (mindless
instruction scheduling, register allocation etc), but that shouldn't make
you think they should be smart.
